{
    "pubmed_qa": {
        "pubmed_qa": {
            "8262881": {
                "source": [
                    "\"Although body dysmorphic disorder (BDD) is classified in DSM-III-R as a nonpsychotic somatoform disorder, controversy exists as to whether BDD can present with psychotic features. If it can, this raises the possibility that its DSM-III-R psychotic counterpart-delusional disorder, somatic type--may not be a separate disorder. The purpose of this study was to determine whether patients with nonpsychotic BDD (defined according to DSM-III-R criteria, i.e., with maintenance of some insight) were different from patients with psychotic BDD (those whose preoccupation was without insight and of delusional intensity).\nFifty consecutive patients meeting DSM-III-R criteria A and C for BDD were assessed with a semistructured interview and the Structured Clinical Interview for DSM-III-R (SCID). Family histories of psychiatric disorders were blindly assessed. The 24 patients with nonpsychotic BDD were compared with the 26 patients with psychotic BDD with respect to demographics, phenomenology, course of illness, associated features, comorbid psychiatric disorders, family history, and treatment response.\nPatients with psychotic BDD displayed a significantly higher rate of lifetime DSM-III-R psychotic disorder diagnoses than patients with nonpsychotic BDD. However, the two groups did not differ significantly on most other variables examined. For instance, both psychotic and nonpsychotic patients displayed significant morbidity; high comorbidity with mood, anxiety, and psychoactive substance use disorders; and apparent preferential response to serotonin reuptake inhibitors rather than to non-serotonin reuptake blocking antidepressants or antipsychotics.\"\nQuestion:\n\"Body dysmorphic disorder: does it have a psychotic subtype?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18603989": {
                "source": [
                    "\"Embalming is the through disinfection and art of preserving bodies after death using chemical substances. It keeps a body life like in appearance during the time it lies in a state prior to funeral.\nThis study was undertaken to investigate the effectiveness of Raksi in sacrificed rats in arresting postmortem changes and establishing scientific fact whether Raksi can be an alternative to standard embalming constituent if it is not available.\n50 albino rats were systematically randomized into control and experiment groups. Raksi and distilled water were injected for embalming purpose intraventricularly in experiment and control groups of rats respectively and kept for 48 to 96 hours for observation for postmortem changes.\nObservations made at 48 and 72 hours of embalming revealed that Raksi can arrest postmortem changes in the rats up to 72 hours (3rd day) successfully in the experimental group whereas moderate to severe postmortem changes were seen in the control group. The experimental group showed mild degree of putrefactive changes, liberation of gases and liquefaction of tissues only at 96 hours (4th day) of embalming.\nThe Raksi used in this experiment contained 34% of alcohol, which was determined by an alcohol hydrometer. Experiment clearly demonstrated from its result that raksi can be utilised temporarily for embalming since it contains alcohol and has preservative, bactericidal and disinfectant properties.\"\nQuestion:\n\"Can homemade alcohol (Raksi) be useful for preserving dead bodies?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15708048": {
                "source": [
                    "\"To determine the effect of prior benign prostate biopsies on the surgical and clinical outcomes of patients treated with radical perineal prostatectomy for prostate cancer.\nA total of 1369 patients with clinically localized prostate cancer underwent radical prostatectomy by a single surgeon between 1991 and 2001. A subset of 203 patients (14.9%), who had undergone at least one prior benign prostate biopsy for a rising prostate-specific antigen and/or abnormal digital rectal examination, constituted our study population. A total of 1115 patients with no prior biopsy represented our control group. After prostatectomy, patients were evaluated at 6-month intervals for biochemical evidence of recurrence, defined as a prostate-specific antigen level of 0.5 ng/mL or greater.\nPatients with a prior benign biopsy had more favorable pathologic features with more organ-confined (74% versus 64%; P<0.001) and less margin-positive (9.8% versus 18%) disease. Only 24 patients (12%) in the study group (versus 20% in control group; P = 0.01) had eventual evidence of biochemical failure. Kaplan-Meier analyses suggested that patients with prior benign biopsies have improved biochemical disease-free survival, especially for those with more aggressive disease (Gleason sum 7 or greater; P<0.01). Overall, patients in the study group had lower probability (odds ratio 0.57, P<0.001) of biochemical failure compared with those in the control group.\"\nQuestion:\n\"Does prior benign prostate biopsy predict outcome for patients treated with radical perineal prostatectomy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15483019": {
                "source": [
                    "\"To assess whether eligibility to an adjuvant chemotherapy protocol in itself represents a good prognostic factor after radical cystectomy for bladder cancer.\nBetween April 1984 and May 1989, our institution entered 35 patients with invasive bladder cancer into the Swiss Group for Clinical and Epidemiological Cancer Research (SAKK) study 09/84. They were randomly assigned to either observation or three postoperative courses of cisplatin monotherapy after cystectomy. This study had a negative result. The outcome of these 35 patients (protocol group) was compared with an age- and tumor-stage-matched cohort (matched group; n = 35) who also underwent cystectomy during the same period, but were not entered into the SAKK study, as well as the remaining 57 patients treated during the study period for the same indication (remaining group).\nMedian overall survival decreased from 76.3 months in the protocol group to 52.1 months in the matched group and to 20.3 months in the remaining group. The respective times of median recurrence-free survival were 67.2, 16.0, and 9.4 months. Tumor progression occurred in 46% of the protocol group compared with 69% in the matched group and 65% in the remaining group (P<.05). Cancer-related death was noted in 40% of the protocol group, 57% in the matched group, and 56% in the remaining group.\"\nQuestion:\n\"Is eligibility for a chemotherapy protocol a good prognostic factor for invasive bladder cancer after radical cystectomy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16097998": {
                "source": [
                    "\"The benefits of serologic screening for coeliac disease in asymptomatic individuals are debatable.AIM: To investigate dietary compliance, quality of life and bone mineral density after long-term treatment in coeliac disease patients found by screening in risk groups.\nThe study comprised 53 consecutive screen-detected coeliac patients diagnosed 14 years (median) ago. Dietary compliance was assessed by interview, 4-day food record and serology. Quality of life was evaluated by the Psychological General Well-Being and SF-36 questionnaires, gastrointestinal symptoms by the Gastrointestinal Symptom Rating Scale and bone mineral density by dual-energy x-ray absorptiometry. Comparisons were made to 44 symptom-detected-treated coeliac patients, 110 non-coeliac subjects and the general population.\nA total of 96% of screen-detected and 93% of symptom-detected coeliac patients adhered to a strict or fairly strict gluten-free diet. In screen-detected patients, quality of life and gastrointestinal symptoms were similar to those in symptom-detected patients or non-coeliac controls and bone mineral density was similar to that in the general population.\"\nQuestion:\n\"Is coeliac disease screening in risk groups justified?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19546588": {
                "source": [
                    "\"Although the mechanism of muscle wasting in end-stage renal disease is not fully understood, there is increasing evidence that acidosis induces muscle protein degradation and could therefore contribute to the loss of muscle protein stores of patients on hemodialysis, a prototypical state of chronic metabolic acidosis (CMA). Because body protein mass is controlled by the balance between synthesis and degradation, protein loss can occur as result of either increased breakdown, impaired synthesis, or both. Correction of acidosis may therefore help to maintain muscle mass and improve the health of patients with CMA. We evaluated whether alkalizing patients on hemodialysis might have a positive effect on protein synthesis and on nutritional parameters.\nEight chronic hemodialysis patients were treated daily with oral sodium bicarbonate (NaHCO(3)) supplementation for 10-14 days, yielding a pre-dialytic plasma bicarbonate concentration of 28.6 +/-1.6 mmol/l. The fractional synthesis rates (FSR) of muscle protein and albumin were obtained by the L-[(2)H(5)ring]phenylalanine flooding technique.\nOral NaHCO(3 )supplementation induced a significant increase in serum bicarbonate (21.5 +/- 3.4 vs. 28.6 +/- 1.6 mmol/l; p = 0.018) and blood pH (7.41 vs. 7.46; p = 0.041). The FSR of muscle protein and the FSR of albumin did not change significantly (muscle protein: 2.1 +/- 0.2 vs. 2.0 +/- 0.5% per day, p = 0.39; albumin: 8.3 +/- 2.2 vs. 8.6 +/- 2.5% per day, p = 0.31). Plasma concentrations of insulin-like growth factor 1 decreased significantly (33.4 +/- 21.3 vs. 25.4 +/- 12.3 nmol/l; p = 0.028), whereas thyroid-stimulating hormone, free thyroxin and free triiodothyronine did not change significantly and nutritional parameters showed no improvement.\"\nQuestion:\n\"Does increasing blood pH stimulate protein synthesis in dialysis patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22440363": {
                "source": [
                    "\"This was a study to compare the results of mitral valve (MV) repair and MV replacement for the treatment of functional mitral regurgitation (MR) in advanced dilated and ischemic cardiomyopathy (DCM).\nOne-hundred and thirty-two patients with severe functional MR and systolic dysfunction (mean ejection fraction 0.32 \u00b1 0.078) underwent mitral surgery in the same time frame. The decision to replace rather than repair the MV was taken when 1 or more echocardiographic predictors of repair failure were identified at the preoperative echocardiogram. Eighty-five patients (64.4%) received MV repair and 47 patients (35.6%) received MV replacement. Preoperative characteristics were comparable between the 2 groups. Only ejection fraction was significantly lower in the MV repair group (0.308 \u00b1 0.077 vs 0.336 \u00b1 0.076, p = 0.04).\nHospital mortality was 2.3% for MV repair and 12.5% for MV replacement (p = 0.03). Actuarial survival at 2.5 years was 92 \u00b1 3.2% for MV repair and 73 \u00b1 7.9% for MV replacement (p = 0.02). At a mean follow-up of 2.3 years (median, 1.6 years), in the MV repair group LVEF significantly increased (from 0.308 \u00b1 0.077 to 0.382 \u00b1 0.095, p<0.0001) and LV dimensions significantly decreased (p = 0.0001). On the other hand, in the MV replacement group LVEF did not significantly change (from 0.336 \u00b1 0.076 to 0.31 \u00b1 0.11, p = 0.56) and the reduction of LV dimensions was not significant. Mitral valve replacement was identified as the only predictor of hospital (odds ratio, 6; 95% confidence interval, 1.1 to 31; p = 0.03) and overall mortality (hazard ratio, 3.1; 95% confidence interval, 1.1 to 8.9; p = 0.02).\"\nQuestion:\n\"Mitral replacement or repair for functional mitral regurgitation in dilated and ischemic cardiomyopathy: is it really the same?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22706226": {
                "source": [
                    "\"To determine current practice and to assess the value of routine follow-up procedures for endometrial cancer surveillance. To discuss whether such procedures are feasible and effective to identify asymptomatic recurrences and describe the pattern of relapse detected by procedures.\nThe records of 282 consecutive women with recurrent endometrial cancer treated from 1986 to 2005 were retrospectively collected in 8 Italian institutions. Primary disease, clinical history, and recurrence features and data were analyzed.\nThirty-five (12.4%) of 282 patients had recurrence in vaginal vault, 51 patients (18.0%) had recurrence in central pelvis, 14 patients (4.9%) had recurrence in pelvic wall, and 39 patients (13.8%) had recurrence in lymph nodes. One-hundred twenty-eight patients (45.3%) showed a distant relapse, whereas 15 patients (5.3%) developed both distant relapse and local relapse. The site of relapse influenced survival because the patients with vaginal vault recurrences lived significantly longer than the patients with recurrences in other sites. Eighty (28.4%) of the 282 patients became symptomatic and anticipated the scheduled visit, 37 (13.1 %) of the patients reported their symptoms during the follow-up meeting, and 165 (58.5 %) of the patients were asymptomatic and the diagnostic path was introduced by a planned visit or examination. Among the asymptomatic patients, the first procedure that led to further examinations was clinical visit alone for 60 (36.4%) of 165 patients, imaging for 103 patients (62.4%), and cytologic examination for 2 patients (1.2%). Symptoms at recurrence can predict survival: patients with an asymptomatic recurrence had a median survival time from relapse of 35 months versus 13 months if they had a symptomatic repetition (P = 0.0001).\"\nQuestion:\n\"Could different follow-up modalities play a role in the diagnosis of asymptomatic endometrial cancer relapses?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18251357": {
                "source": [
                    "\"To evaluate the degree to which histologic chorioamnionitis, a frequent finding in placentas submitted for histopathologic evaluation, correlates with clinical indicators of infection in the mother.\nA retrospective review was performed on 52 cases with a histologic diagnosis of acute chorioamnionitis from 2,051 deliveries at University Hospital, Newark, from January 2003 to July 2003. Third-trimester placentas without histologic chorioamnionitis (n = 52) served as controls. Cases and controls were selected sequentially. Maternal medical records were reviewed for indicators of maternal infection.\nHistologic chorioamnionitis was significantly associated with the usage of antibiotics (p = 0.0095) and a higher mean white blood cell count (p = 0.018). The presence of 1 or more clinical indicators was significantly associated with the presence of histologic chorioamnionitis (p = 0.019).\"\nQuestion:\n\"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19327500": {
                "source": [
                    "\"It remains controversial whether there is a gender difference in survival of patients with resected non-small cell lung cancer.\nWe retrospectively analyzed 2770 patients (1689 men and 1081 women) with non-small cell lung cancer who underwent pulmonary resection between 1995 and 2005 at the National Cancer Center Hospital, Tokyo. A gender difference in survival was studied in all patients, in those divided according to histology or pathologic stage, and in propensity-matched gender pairs.\nThere were no differences in background, such as preoperative pulmonary function, operation procedures, or operative mortality. The proportions of adenocarcinoma and pathologic stage I in women were greater than those in men (93.6% vs 61.7% and 71.4% vs 58.6%, respectively) (P<.001). Overall 5-year survival of women was better than that of men (81% vs 70%, P<.001). In adenocarcinoma, the overall 5-year survival for women was better than that for men in pathologic stage I (95% vs 87%, P<.001) and in pathologic stage II or higher (58% vs 51%, P = .017). In non-adenocarcinoma, there was no significant gender difference in survival in pathologic stage I (P = .313) or pathologic stage II or higher (P = .770). The variables such as age, smoking status, histology, and pathologic stage were used for propensity score matching, and survival analysis of propensity score-matched gender pairs did not show a significant difference (P = .69).\"\nQuestion:\n\"Gender difference in survival of resected non-small cell lung cancer: histology-related phenomenon?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20297950": {
                "source": [
                    "\"To investigate the effect of fenofibrate on sleep apnoea indices.\nProof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period.\nNCT00816829.\nAs this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour.\nFenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate.\nExploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.\"\nQuestion:\n\"Proof of concept study: does fenofibrate have a role in sleep apnoea syndrome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20530150": {
                "source": [
                    "\"Children referred with symptomatic gallstones complicating HS between April 1999 and April 2009 were prospectively identified and reviewed retrospectively. During this period, the policy was to undertake concomitant splenectomy only if indicated for haematological reasons and not simply because of planned cholecystectomy.\nA total of 16 patients (mean age 10.4, range 3.7 to 16 years, 11 women) with HS and symptomatic gallstones underwent cholecystectomy. Three patients subsequently required a splenectomy for haematological reasons 0.8-2.5 years after cholecystectomy; all three splenectomies were performed laparoscopically. There were no postoperative complications in the 16 patients; postoperative hospital stay was 1-3 days after either cholecystectomy or splenectomy. The 13 children with a retained spleen remain under regular review by a haematologist (median follow-up 4.6, range 0.5 to 10.6 years) and are well and transfusion independent.\"\nQuestion:\n\"Is cholecystectomy really an indication for concomitant splenectomy in mild hereditary spherocytosis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19648304": {
                "source": [
                    "\"This randomized controlled study addressed whether sonographic needle guidance affected clinical outcomes of intraarticular (IA) joint injections.\nIn total, 148 painful joints were randomized to IA triamcinolone acetonide injection by conventional palpation-guided anatomic injection or sonographic image-guided injection enhanced with a one-handed control syringe (the reciprocating device). A one-needle, 2-syringe technique was used, where the first syringe was used to introduce the needle, aspirate any effusion, and anesthetize and dilate the IA space with lidocaine. After IA placement and synovial space dilation were confirmed, a syringe exchange was performed, and corticosteroid was injected with the second syringe through the indwelling IA needle. Baseline pain, procedural pain, pain at outcome (2 weeks), and changes in pain scores were measured with a 0-10 cm visual analog pain scale (VAS).\nRelative to conventional palpation-guided methods, sonographic guidance resulted in 43.0% reduction in procedural pain (p<0.001), 58.5% reduction in absolute pain scores at the 2 week outcome (p<0.001), 75% reduction in significant pain (VAS pain score>or = 5 cm; p<0.001), 25.6% increase in the responder rate (reduction in VAS score>or = 50% from baseline; p<0.01), and 62.0% reduction in the nonresponder rate (reduction in VAS score<50% from baseline; p<0.01). Sonography also increased detection of effusion by 200% and volume of aspirated fluid by 337%.\"\nQuestion:\n\"Does sonographic needle guidance affect the clinical outcome of intraarticular injections?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23719685": {
                "source": [
                    "\"The present analysis compares two palliative treatment concepts for lung cancer in terms of overall survival.\nSurvival data from 207\u00a0patients were used in a retrospective analysis. All patients received palliative treatment comprising either 25\u00a0Gy applied in 5\u00a0fractions or 50\u00a0Gy in 20\u00a0fractions. A subgroup analysis was performed to compare patients with a good-fair vs. poor overall condition.\nMedian survival times were 21\u00a0weeks (range\u00a06-26\u00a0weeks) for patients treated with 25\u00a0Gy in 5\u00a0fractions and 23\u00a0weeks (range\u00a014.5-31.5\u00a0weeks) for patients treated with 50\u00a0Gy in 20\u00a0fractions (95\u2009% confidence interval, CI; p\u2009=\u20090.334). For patients with a good-fair overall condition, median survival times were 30\u00a0weeks (21.8-39.2\u00a0weeks) for 25\u00a0Gy in 5\u00a0fractions and 28\u00a0weeks (14.2-41.8\u00a0weeks) for 50\u00a0Gy in 20\u00a0fractions (CI 95\u2009%, p\u2009=\u20090.694). In patients with a poor overall condition, these values were 18\u00a0weeks (14.5-21.5\u00a0weeks) and 21\u00a0weeks (13.0-29.0\u00a0weeks), respectively (CI 95\u2009%, p\u2009=\u20090.248).\"\nQuestion:\n\"Does high-dose radiotherapy benefit palliative lung cancer patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25735444": {
                "source": [
                    "\"A multicentre, retrospective study was conducted of patients with rectal cancer threatening or affecting the prostatic plane, but not the bladder, judged by magnetic resonance imaging (MRI). The use of preoperative chemoradiotherapy and the type of urologic resection were correlated with the status of the pathological circumferential resection margin (CRM) and local recurrence.\nA consecutive series of 126 men with rectal cancer threatening (44) or affecting (82) the prostatic plane on preoperative staging and operated with local curative intent between 1998 and 2010 was analysed. In patients who did not have chemoradiotherapy but had a preoperative threatened anterior margin the CRM-positive rate was 25.0%. In patients who did not have preoperative chemoradiotherapy but did have an affected margin, the CRM-positive rate was 41.7%. When preoperative radiotherapy was given, the respective CRM infiltration rates were 7.1 and 20.7%. In patients having preoperative chemoradiotherapy followed by prostatic resection the rate of CRM positivity was 2.4%. Partial prostatectomy after preoperative chemoradiotherapy resulted in a free anterior CRM in all cases, but intra-operative urethral damage occurred in 36.4% of patients who underwent partial prostatectomy, resulting in a postoperative urinary fistula in 18.2% of patients.\"\nQuestion:\n\"Rectal cancer threatening or affecting the prostatic plane: is partial prostatectomy oncologically adequate?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18799291": {
                "source": [
                    "\": The histidine triad nucleotide-binding protein 1, HINT1, hydrolyzes adenosine 5'-monophosphoramidate substrates such as AMP-morpholidate. The human HINT1 gene is located on chromosome 5q31.2, a region implicated in linkage studies of schizophrenia. HINT1 had been shown to have different expression in postmortem brains between schizophrenia patients and unaffected controls. It was also found to be associated with the dysregulation of postsynaptic dopamine transmission, thus suggesting a potential role in several neuropsychiatric diseases.\n: In this work, we studied 8 SNPs around the HINT1 gene region using the Irish study of high density schizophrenia families (ISHDSF, 1350 subjects and 273 pedigrees) and the Irish case control study of schizophrenia (ICCSS, 655 affected subjects and 626 controls). The expression level of HINT1 was compared between the postmortem brain cDNAs from schizophrenic patients and unaffected controls provided by the Stanley Medical Research Institute.\n: We found nominally significant differences in allele frequencies in several SNPs for both ISHDSF and ICCSS samples in sex-stratified analyses. However, the sex effect differed between the two samples. In expression studies, no significant difference in expression was observed between patients and controls. However, significant interactions amongst sex, diagnosis and rs3864283 genotypes were observed.\"\nQuestion:\n\"Is the histidine triad nucleotide-binding protein 1 (HINT1) gene a candidate for schizophrenia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23348330": {
                "source": [
                    "\"This study aims to evaluate the efficacy of closed reduction and the effects of timing and fracture types on patient satisfaction.\nOnly patients with isolated nasal fractures were included in the study. Patients with additional maxillofacial fractures and patients whose application time to our clinic was more than 10 days after the trauma were excluded. Patients were classified into 5 types according to their fracture. All patients underwent closed reduction and external fixation under local anesthesia. Patients were asked about their satisfaction in a survey at 28th day and sixth month after the surgery. Patients were divided into groups according to fracture type and intervention time, and the results of the survey were evaluated.\nOf the 43 patients included in the study, 38 were male, 5 were female, and the average age was 24.9. The average intervention time of the patients was 5.44 days. Twenty-eight (65%) of 43 patients were satisfied with the result, whereas 15 (35%) patients were not happy with their operation. In a comparison of patient satisfaction rates according to fracture type, the mild fracture group had a higher satisfaction rate compared to the severe fracture group.\"\nQuestion:\n\"Nasal fractures:  is closed reduction satisfying?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20497880": {
                "source": [
                    "\"Bladder catheterisation is a routine part of major abdominal surgery. Transurethral catheterisation is the most common method of bladder drainage but is also notorious for its discomfort and increased risk of urinary tract infection. The present study aimed to establish patient satisfaction with transurethral catheterisation and to assess the incidence of clinically significant urinary tract infections after transurethral catheterisation through survey.\nAll patients who underwent major open abdominal surgery between October 2006 and December 2008 and required standard transurethral bladder catheterisation, were asked to participate in the study. Fifty patients were recruited.\nMale patients were more dissatisfied than their female counterparts with transurethral catheterisation (satisfaction score: 4.18/10 vs. 2.75/10; p = 0.05). Male patients had more than double the score for pain at the urinary meatus with the catheter in situ (p =0.012) and during urine catheter removal (p = 0.013). Half the patients in the study also had symptoms of urinary tract infection after catheter removal.\"\nQuestion:\n\"Is transurethral catheterisation the ideal method of bladder drainage?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9483814": {
                "source": [
                    "\"Uterus-specific synthetic Prostaglandin analogues (gemeprost, sulproston etc.) have been widely employed for termination of pregnancy in the second trimester. Since paracervical anaesthesia may be useful during this procedure, we investigated in this prospective randomised study its impact on the clinical course of abortion and pain especially in the late first and second stage of labour.\n20 women scheduled for elective abortion (fetal reasons) between the 16th and 23rd week of gestation were to be given 1 mg gemeprost vaginally every 6 hours. They were allocated at random: 10 women received only Pethidin intravenously and Butylscopolamine rectally, another 10 women were additionally treated by paracervical anaesthesia (2 x 10 ml 0.5% Bupivacain solution) at a cervical dilatation of 2-3 cm.\nA median of 3 gemeprost applications were administered in both groups. In the group without paracervical anaesthesia the median induction to abortion interval was 20 hours (range: 8-44 hours), 13 hours (range: 8-36 hours, NS) resulting for the paracervical anaesthesia group. The intervals from the last application of prostaglandin until abortion and from 3 cm cervical dilatation to abortion were slightly, but not significantly shorter in the paracervical anaesthesia group. The requirement of Butylscopolamine was higher in the latter group (p<0.05). The requirement of Pethidin and the intensity of pain (measured by pain scale according to Huskisson) especially in the late first stage of labour were not statistically different between both groups. Side effects of paracervical anaesthesia did not occur.\"\nQuestion:\n\"Does para-cervical block offer additional advantages in abortion induction with gemeprost in the 2nd trimester?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26419377": {
                "source": [
                    "\"The purpose of this study was to evaluate safe depth for suture anchor insertion during acetabular labral repair and to determine the neighbouring structures at risk during drilling and anchor insertion.\nTen human cadaveric hips (six males and four females) were obtained. Acetabular labral surface was prepared and marked for right hips as 12, 1 and 3 o'clock positions, for left hips 12, 11 and 9 o'clock positions. Those were defined as anterior, anterior-superior and superior zones, respectively. These labral positions were drilled at defined zones. After measurements, depth of the bone at 10\u00b0 and 20\u00b0 drill angles on zones was compared statistically.\nAcetabular bone widths at investigated labral insertion points did not statistically differ. A total of 14 injuries in 60 penetrations occurred (23.3\u00a0%) with free drill penetrations, and no injuries occurred with stopped drill penetrations. The bone depth was gradually decreasing from 10\u00b0 to 20\u00b0 drill angles and from anterior to superior inserting zones without significant importance. The risk of perforation to the pelvic cavity started with 20\u00a0mm drill depth, and the mean depth for all insertions was calculated as 31.7\u00a0mm (SD 2.6).\"\nQuestion:\n\"Are pelvic anatomical structures in danger during arthroscopic acetabular labral repair?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22813804": {
                "source": [
                    "\"Childhood obesity is pandemic condition. The effect of obesity on trauma outcomes in children has been relatively understudied. We conducted this study to ascertain the effects of obesity on the hospital outcome of injured children.\nA retrospective cohort study of patients aged 2 to 18 years admitted to the King Abdul Aziz Medical City between May 2001 and May 2009 was conducted. Patients were categorized as lean (body mass index<95th percentile) and obese (body mass index \u2265 95th percentile). Groups were compared regarding admission demographics, mechanism of injury, pattern of injury, length of stay, intensive care unit admission, ventilation duration, types of procedures performed, injury severity score, and mortality.\nNine hundred thirty-three patients were included, of those 55 (5.89%) children were obese. The obese children were older than nonobese (P = .001) and had a higher injury severity score (P = .001) and a lower pediatric trauma score (P = .00), heart rate (P = .0081), and respiratory rate (P = .000). There were no differences between groups with regard to sex, mechanism of injury, and surgical procedures. Obese children were more likely to have rib fractures (P = .02) and pelvic injuries (P = .033). There was no significant association between mortality and obesity (P = .42).\"\nQuestion:\n\"Does obesity impact the pattern and outcome of trauma in children?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24191126": {
                "source": [
                    "\"Surgical excision of ovarian endometriomas in patients desiring pregnancy has recently been criticized because of the risk of damage to healthy ovarian tissue and consequent reduction of ovarian reserve. A correct diagnosis in cases not scheduled for surgery is therefore mandatory in order to avoid unexpected ovarian cancer misdiagnosis. Endometriosis is often associated with high levels of CA125. This marker is therefore not useful for discriminating ovarian endometrioma from ovarian malignancy. The aim of this study was to establish if the serum marker CA72-4 could be helpful in the differential diagnosis between ovarian endometriosis and epithelial ovarian cancer.\nSerums CA125 and CA72-4 were measured in 72 patients with ovarian endometriomas and 55 patients with ovarian cancer.\nHigh CA125 concentrations were observed in patients with ovarian endometriosis and in those with ovarian cancer. A marked difference in CA72-4 values was observed between women with ovarian cancer (71.0%) and patients with endometriosis (13.8%) (P<0.0001).\"\nQuestion:\n\"Is CA72-4 a useful biomarker in differential diagnosis between ovarian endometrioma and epithelial ovarian cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19430778": {
                "source": [
                    "\"To correlate magnetic resonance (MR) image findings with pain response by provocation discography in patients with discogenic low back pain, with an emphasis on the combination analysis of a high intensity zone (HIZ) and disc contour abnormalities.\nSixty-two patients (aged 17-68 years) with axial low back pain that was likely to be disc related underwent lumbar discography (178 discs tested). The MR images were evaluated for disc degeneration, disc contour abnormalities, HIZ, and endplate abnormalities. Based on the combination of an HIZ and disc contour abnormalities, four classes were determined: (1) normal or bulging disc without HIZ; (2) normal or bulging disc with HIZ; (3) disc protrusion without HIZ; (4) disc protrusion with HIZ. These MR image findings and a new combined MR classification were analyzed in the base of concordant pain determined by discography.\nDisc protrusion with HIZ [sensitivity 45.5%; specificity 97.8%; positive predictive value (PPV), 87.0%] correlated significantly with concordant pain provocation (P<0.01). A normal or bulging disc with HIZ was not associated with reproduction of pain. Disc degeneration (sensitivity 95.4%; specificity 38.8%; PPV 33.9%), disc protrusion (sensitivity 68.2%; specificity 80.6%; PPV 53.6%), and HIZ (sensitivity 56.8%; specificity 83.6%; PPV 53.2%) were not helpful in the identification of a disc with concordant pain.\"\nQuestion:\n\"Can magnetic resonance imaging accurately predict concordant pain provocation during provocative disc injection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12612531": {
                "source": [
                    "\"Primary eosinophilic esophagitis, a chronic inflammatory disorder of the esophagus, evokes recurrent dysphagia. Endoscopy is often unremarkable, and no consensus exists regarding management of resultant dysphagia. The response of a series of patients with primary eosinophilic esophagitis to dilation is reported together with a description of a possibly pathognomonic sign: fragile esophageal mucosa, for which the term \"cr\u00eape-paper\" mucosa is introduced.\nFive men underwent endoscopy because of dysphagia confirmed (clinically, endoscopically, and histologically) to be caused by primary eosinophilic esophagitis and were treated by bouginage.\nAll patients had extremely fragile, inelastic, and delicate mucosa, which tore easily even with minor trauma. After the procedure, patients remained asymptomatic for 3 to 24 months.\"\nQuestion:\n\"Fragility of the esophageal mucosa: a pathognomonic endoscopic sign of primary eosinophilic esophagitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18435678": {
                "source": [
                    "\"Kell haemolytic disease in pregnancies has been suggested to be associated with decreased fetal platelet counts. The aim of this study was to evaluate the incidence and clinical significance of fetal thrombocytopenia in pregnancies complicated by Kell alloimmunization.\nIn this retrospective cohort study, fetal platelet counts were performed in 42 pregnancies with severe Kell alloimmunization prior to the first intrauterine blood transfusion. Platelet counts from 318 first intrauterine transfusions in RhD alloimmunized pregnancies were used as controls.\nFetal thrombocytopenia (platelet count<150 x 10(9)/l) was found in 4/42 (10%) in the Kell group and in 84/318 (26%) in the RhD group. None of the fetuses in the Kell alloimmunized pregnancies, including 15 with severe hydrops, had a clinically significant thrombocytopenia defined as a platelet count<50 x 10(9)/l. In the RhD alloimmunized pregnancies, 2/230 (1%) of the non-hydropic fetuses and 7/30 (23%) of the severely hydropic fetuses had a clinically significant thrombocytopenia.\"\nQuestion:\n\"Kell alloimmunization in pregnancy: associated with fetal thrombocytopenia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18399830": {
                "source": [
                    "\"To evaluate whether robotically assisted laparoscopic prostatectomy (RALP) is less invasive than radical retropubic prostatectomy (RRP), as experimental studies suggest that the acute phase reaction is proportional to surgery-induced tissue damage.\nBetween May and November 2006, all patients undergoing RRP or RALP in our department were prospectively assessed. Blood samples were collected 24 h before (T0), during surgery (T1), at the end of anaesthesia (T2), and 12 (T3) and 24 h after surgery (T4), and assayed for interleukin(IL)-6 and IL-1 alpha, C-reactive protein (CRP), and lactate. The Mann-Whitney U-, Student's t- and Friedman tests were used to compare continuous variables, and the Pearson chi-square and Fisher test for categorical variables, with a two-sided P<0.05 considered to indicate significance.\nIn all, 35 and 26 patients were assessed for RALP and RRP, respectively; the median (interquartile range) age was 62 (56-68) and 68.5 (59.2-71.2) years, respectively (P<0.009). Baseline levels (T0) of IL-1, IL-6, CRP and lactate were comparable in both arms. IL-6, CRP and lactates levels increased during both kinds of surgery. The mean IL-6 and CPR values were higher for RRP at T1 (P = 0.01 and 0.001), T2 (P = 0.001 and<0.001), T3 (P = 0.002 and<0.001) and T4 (P<0.001 and 0.02), respectively. Lactate was higher for RRP at T2 (P = 0.001), T3 (P = 0.001) and T4 (P = 0.004), although remaining within the normal ranges. IL-1 alpha did not change at the different sample times.\"\nQuestion:\n\"Is robotically assisted laparoscopic radical prostatectomy less invasive than retropubic radical prostatectomy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25443385": {
                "source": [
                    "\"Virtual planning and guided surgery with or without prebent or milled plates are becoming more and more common for mandibular reconstruction with fibular free flaps (FFFs). Although this excellent surgical option is being used more widely, the question of the additional cost of planning and cutting-guide production has to be discussed. In capped payment systems such additional costs have to be offset by other savings if there are no special provisions for extra funding. Our study was designed to determine whether using virtual planning and guided surgery resulted in time saved during surgery and whether this time gain resulted in self-funding of such planning through the time saved.\nAll consecutive cases of FFF surgery were evaluated during a 2-year period. Institutional data were used to determine the price of 1 minute of operative time. The time for fibula molding, plate adaptation, and insetting was recorded.\nDuring the defined period, we performed 20 mandibular reconstructions using FFFs, 9 with virtual planning and guided surgery and 11 freehand cases. One minute of operative time was calculated to cost US $47.50. Multiplying this number by the time saved, we found that the additional cost of virtual planning was reduced from US $5,098 to US $1,231.50 with a prebent plate and from US $6,980 to US $3,113.50 for a milled plate.\"\nQuestion:\n\"Are virtual planning and guided surgery for head and neck reconstruction economically viable?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14631523": {
                "source": [
                    "\"The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading.\nChildren aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems.\nThe overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.\"\nQuestion:\n\"Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17704864": {
                "source": [
                    "\"Laparoscopic adrenalectomy (LA) has become the gold standard treatment for small (less than 6 cm) adrenal masses. However, the role of LA for large-volume (more than 6 cm) masses has not been well defined. Our aim was to evaluate, retrospectively, the outcome of LA for adrenal lesions larger than 7 cm.\n18 consecutive laparoscopic adrenalectomies were performed from 1996 to 2005 on patients with adrenal lesions larger than 7 cm.\nThe mean tumor size was 8.3 cm (range 7-13 cm), the mean operative time was 137 min, the mean blood loss was 182 mL (range 100-550 mL), the rate of intraoperative complications was 16%, and in three cases we switched from laparoscopic procedure to open surgery.\"\nQuestion:\n\"Is laparoscopic adrenalectomy safe and effective for adrenal masses larger than 7 cm?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26113007": {
                "source": [
                    "\"Orthodontic patients show high prevalence of tooth-size discrepancy. This study investigates the possible association between arch form, clinically significant tooth-size discrepancy, and sagittal molar relationship.\nPretreatment orthodontic casts of 230 Saudi patients were classified into one of three arch form types (tapered, ovoid, and square) using digitally scanned images of the mandibular arches. Bolton ratio was calculated, sagittal molar relationship was defined according to Angle classification, and correlations were analyzed using ANOVA, chi-square, and t-tests.\nNo single arch form was significantly more common than the others. Furthermore, no association was observed between the presence of significant Bolton discrepancy and the sagittal molar relationship or arch form. Overall Bolton discrepancy is significantly more prevalent in males.\"\nQuestion:\n\"Is arch form influenced by sagittal molar relationship or Bolton tooth-size discrepancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24958650": {
                "source": [
                    "\"Our aim in this study was to investigate whether mean platelet volume (MPV) value could be used as an early marker to predict pelvic inflammatory disease (PID).\nOverall, 44 patients with PID and 44 healthy women were included in the study. The control group consisted of 44 women who applied to the clinic for a routine gynaecological check-up, without chronic disease or a history of medication use. Owing to the fact that it would affect thrombocyte function, women who have the following conditions were excluded from the study: women who were taking anticoagulant therapy, oral contraceptives, nonsteroid anti-inflammatory medications and who had chronic diseases. The leukocyte count, platelet count, neutrophil ratio and MPV values were collected from PID and the control group. C reactive protein values of patients with PID were also noted.\nMPV values in patients with PID were lower than those in the control group. This reduction in MPV is statistically significant when the PID patient group is compared with the control group (p\u2009<\u20090.001). A negative correlation was discovered between platelet count and MPV values (p\u2009=\u20090.019, r\u2009=\u2009-\u20090.425). Receiver-operating curve analysis pointed out that MPV has greater area under curve value than neutrophil rate, leukocyte and platelet count (0.73, 0.64, 0.72 and 0.49 respectively).\"\nQuestion:\n\"May mean platelet volume levels be a predictor in the diagnosis of pelvic inflammatory disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22205377": {
                "source": [
                    "\"The current study is aimed to assess the relationship between the 'economic/employment' and 'social/welfare' dimensions of social exclusion and suicide mortality in Europe.\nSuicide rates for 26 countries were obtained from the WHO. Data on social expenditure were obtained from the OECD database. Employment rates and GDP were obtained from the Total Economy Database. Questions about citizens' attitudes towards different aspects of social exclusion were taken from the European Social Survey. Structural equation modelling was applied to research the theoretical structure of the variables.\nAll variables are statistically significant in male and female models except of the relationships between 'economic/employment' and 'social/welfare' dimensions and female suicides; and the relationship between 'employment rates' and 'economic/employment' dimension. Suicide mortality rates among both males and females are influenced negatively by 'economic/employment' and 'social/welfare' dimensions. Among females, the influence of 'social/welfare' dimension is stronger compared to the 'economic/employment' dimension. The remaining influence of GDP is positive in both models.\"\nQuestion:\n\"Some aspects of social exclusion: do they influence suicide mortality?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17502203": {
                "source": [
                    "\"Cholestasis occurs frequently in patients with small bowel atresia (SBA) and is often attributed to prolonged parental nutrition. When severe or prolonged, patients may undergo unnecessary intensive or invasive investigation. We characterized cholestasis and analyzed the pertinence of investigating this patient population.\nWith Research Ethics Board approval, patients with SBA between 1996 and 2005 were retrospectively reviewed. Demographics, location of atresia, operative findings, complications, investigations, resumption of feeding, duration of prolonged parental nutrition, and follow-up information were examined. Cholestasis was evaluated for incidence, severity, and evolution.\nFifty-five patients (29 male, 26 female), with a median gestational age and birth weight of 36 weeks and 2025 g, respectively, were reviewed. Care was withdrawn for 2 patients before repair. For the remaining 53 patients, SBA were duodenal atresia in 18, jejunoileal atresia in 32, and multiple atresia in 3. Of 53, 24 (45%) patients developed cholestasis postoperatively (direct/total bilirubin>20%). All patients with short bowel (4) and 60% (6/10) of patients with a delay of enteral feeding more than 14 days postoperatively had cholestasis. Ten patients (36%) proceeded with in-depth evaluations for cholestasis, with 8 (28%) undergoing liver biopsy. No patient had biliary atresia. No deaths were related to isolated cholestasis/cirrhosis. Cholestasis resolved spontaneously in all the survivors.\"\nQuestion:\n\"Cholestasis associated with small bowel atresia: do we always need to investigate?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27040842": {
                "source": [
                    "\"To measure the dimensions of compensatory hypertrophy of the middle turbinate in patients with nasal septal deviation, before and after septoplasty.\nThe mucosal and bony structures of the middle turbinate and the angle of the septum were measured using radiological analysis before septoplasty and at least one year after septoplasty. All pre- and post-operative measurements of the middle turbinate were compared using the paired sample t-test and Wilcoxon rank sum test.\nThe dimensions of bony and mucosal components of the middle turbinate on concave and convex sides of the septum were not significantly changed by septoplasty. There was a significant negative correlation after septoplasty between the angle of the septum and the middle turbinate total area on the deviated side (p = 0.033).\"\nQuestion:\n\"Does septoplasty change the dimensions of compensatory hypertrophy of the middle turbinate?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15528969": {
                "source": [
                    "\"Current guidelines include a recommendation that a pathologist with expertise in breast disease review all ductal carcinoma in situ (DCIS) specimens due to the presence of significant variability in pathologic reporting of DCIS. The objective of this study was to evaluate the completeness and accuracy of pathologic reporting of DCIS over the past decade and to determine the current impact of expert breast pathology assessment on the management of DCIS.\nAll patients with a diagnosis of DCIS referred to a single regional cancer centre between 1982 and 2000 have been reviewed. Inter-observer variability between initial and secondary reports has been evaluated using kappa statistics. For each case, the Van Nuys Prognostic Index (VNPI) using pathologic data obtained from the initial and reviewed pathology reports were compared. The impact of expert breast pathology on risk assessment and treatment was determined.\n481 individuals with DCIS were referred and pathology review was performed on 350 patients (73%). Inter-observer agreement was high for the main pathologic features of DCIS. From 1996 to 2000, secondary pathology assessments lead to a change in the assessment of local recurrence risk in 100 cases (29%) and contributed to a change in treatment recommendation in 93 (43%) cases.\"\nQuestion:\n\"Is expert breast pathology assessment necessary for the management of ductal carcinoma in situ ?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15488260": {
                "source": [
                    "\"Rates of relapse and predictive relapse factors were studied over more than 4 years in a sample of Spanish outpatients with DSM-III-R criteria for unipolar major depressive episode.\nA final sample of 139 outpatient was followed monthly in a naturalistic study. The Structured Clinical Interview for DSM-III-R was used. Phases of evolution were recorded using the Hamilton Depression Rating Scale, applying the Frank criteria. Survival analysis, Kaplan-Meier product limit and proportional hazards models were used.\nA higher rate of relapses was observed in the partial remission group (91.4%) compared to the complete remission one (51.3%). The four factors with predictive relapse value were: \"partial remission versus complete remission\", \"the intensity of clinical symptoms\", \"the age\" and \"the number of previous depressive episodes\". The existence of partial remission was the most powerful predictive factor.\nThe decreasing sample size during the follow-up and the difficulty in warranting the treatment compliance.\"\nQuestion:\n\"Is the type of remission after a major depressive episode an important risk factor to relapses in a 4-year follow up?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26237424": {
                "source": [
                    "\"To evaluate the impact of patient-prosthesis mismatch (PPM) on survival, functional status, and quality of life (QoL) after aortic valve replacement (AVR) with small prosthesis size in elderly patients.\nBetween January 2005 and December 2013, 152 patients with pure aortic stenosis, aged at least 75 years, underwent AVR, with a 19 or 21\u200amm prosthetic heart valve. PPM was defined as an indexed effective orifice area less than 0.85\u200acm/m. Median age was 82 years (range 75-93 years). Mean follow-up was 56 months (range 1-82 months) and was 98% complete. Late survival rate, New York Heart Association functional class, and QoL (RAND SF-36) were assessed.\nOverall, PPM was found in 78 patients (53.8%). Among them, 42 patients (29%) had an indexed effective orifice area less than 0.75\u200acm/m and 17 less than 0.65\u200acm/m (11.7%). Overall survival at 5 years was 78\u200a\u00b1\u200a4.5% and was not influenced by PPM (P\u200a=\u200aNS). The mean New York Heart Association class for long-term survivors with PPM improved from 3.0 to 1.7 (P\u200a<\u200a0.001). QoL (physical functioning 45.18\u200a\u00b1\u200a11.35, energy/fatigue 49.36\u200a\u00b1\u200a8.64, emotional well being 58.84\u200a\u00b1\u200a15.44, social functioning 61.29\u200a\u00b1\u200a6.15) was similar to that of no-PPM patients (P\u200a=\u200aNS).\"\nQuestion:\n\"Does patient-prosthesis mismatch after aortic valve replacement affect survival and quality of life in elderly patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24340838": {
                "source": [
                    "\"Sudden death in athletes can occur during sport activities and is presumably related to ventricular arrhythmias.\nTo investigate the long-term follow-up ofathletes with ventricular arrhythmias during an exercise test.\nFrom a database of 56,462 athletes we identified 192 athletes (35 years old who had ventricular arrhythmias during an exercise test. Ninety athletes had>or =3 ventricular premature beats (VPB) (group A) and 102 athletes had ventricular couplets or non-sustained ventricular tachycardia during an exercise test (group B). A control group of 92 athletesfrom without ventricular arrhythmias was randomly seleclted from the database (group C). Of the 192 athletes 39 returnied for a repeat exercise test after a mean follow-up period of 70 +/- 25 months and they constitute the study population.\nTwelve athletes from group A, 21 fromgroup B and 6 from group C returned for a repeat exercise test. The athletes reached a significantly lower peak heart rate during their follow-up exercise test (P = 0.001). More athletes were engaged in competitive sports during their initialexercise test than in the follow-up test (P = 0.021). Most of theathletes who had VPB and/orventricular couplets and/or NSVT during their initial exercise test had far fewer ventricular arrhythmias in the follow-up exercise test (P = 0.001).\"\nQuestion:\n\"Do ventricular arrhythmias in athletes subside over time?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11926574": {
                "source": [
                    "\"Hepatitis G virus can cause chronic infection in man but the role of this agent in chronic liver disease is poorly understood. Little is known about the relation of another newly discovered agent, the TT virus, with chronic liver disease.AIM: To investigate the rate of infection with hepatitis G virus and TT virus in patients with cryptogenic chronic liver disease.\nA total of 23 subjects with chronically raised alanine transaminase and a liver biopsy in whom all known causes of liver disease had been excluded, and 40 subjects with hepatitis C virus-related chronic liver disease.\nEvaluation of anti-hepatitis G virus by enzyme immunoassay. Hepatitis G virus-RNA by polymerase chain reaction with primers from the 5' NC and NS5a regions. TT virus-DNA by nested polymerase chain reaction with primers from the ORF1 region. Results. Hepatitis G virus-RNA was detected in 4 out of 23 patients with cryptogenic chronic hepatitis and in 6 out of 40 with hepatitis C virus chronic hepatitis (17.4% vs 15% p=ns). At least one marker of hepatitis G virus infection (hepatitis G virus-RNA and/or anti-hepatitis G virus, mostly mutually exclusive) was present in 6 out of 23 patients with cryptogenic hepatitis and 16 out of 40 with hepatitis C virus liver disease (26. 1% vs 40% p=ns). T virus-DNA was present in serum in 3 subjects, 1 with cryptogenic and 2 with hepatitis C virus-related chronic liver disease. Demographic and clinical features, including stage and grade of liver histology, were comparable between hepatitis G virus-infected and uninfected subjects. Severe liver damage [chronic hepatitis with fibrosis or cirrhosis) were significantly more frequent in subjects with hepatitis C virus liver disease.\"\nQuestion:\n\"Are hepatitis G virus and TT virus involved in cryptogenic chronic liver disease?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11481599": {
                "source": [
                    "\"The purpose of this study was to delineate early respiratory predictors of mortality in children with hemato-oncology malignancy who developed acute respiratory distress syndrome (ARDS).\nWe conducted a retrospective chart review of children with malignant and ARDS who needed mechanical ventilation and were admitted to a pediatric intensive care unit from January 1987 to January 1997.\nSeventeen children with ARDS and malignancy aged 10.5 +/- 5.1 years were identified. Six of the 17 children (35.3%) survived. Sepsis syndrome was present in 70.6% of all the children. Peak inspiratory pressure, positive end-expiratory pressure (PEEP), and ventilation index values could distinguish outcome by day 3. A significant relationship between respiratory data and outcome related to efficiency of oxygenation, as determined by PaO(2)/FIO(2) and P(A-a)O(2), was present from day 8 after onset of mechanical ventilation.\"\nQuestion:\n\"Acute respiratory distress syndrome in children with malignancy--can we predict outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25521278": {
                "source": [
                    "\"Identifying eating behaviors which contribute to excess weight gain will inform obesity prevention strategies. A tendency to clear one's plate when eating may be a risk factor for obesity in an environment where food is plentiful. Whether plate clearing is associated with increased body weight in a cohort of US participants was examined.\nNine hundred and ninety-three US adults (60% male, 80% American European, mean age=31 years) completed self-report measures of habitual plate clearing together with behavioral and demographic characteristics known to be associated with obesity.\nPlate clearing tendencies were positively associated with BMI and remained so after accounting for a large number of other demographic and behavioral predictors of BMI in analyses (\u03b2=0.18, 95% CIs=0.07, 0.29, P<0.001); an increased tendency to plate clear was associated with a significantly higher body weight.\"\nQuestion:\n\"Is plate clearing a risk factor for obesity?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24352924": {
                "source": [
                    "\"The purpose of this study was to investigate the efficacy of ultrasonography to confirm Schanz pin placement in a cadaveric model, and the interobserver repeatability of the ultrasound methodology.\nThis investigation is a repeated measures cadaveric study with multiple examiners.\nCadaveric preparation and observations were done by an orthopaedic traumatologist and resident, and two general surgery traumatologists.\nA total of 16 Schanz pins were equally placed in bilateral femora and tibiae. Four examiners took measurements of pin protrusion beyond the distal cortices using first ultrasonography and then by direct measurement after gross dissection.MAIN OUTCOME MEASURE(S): Distal Schanz pin protrusion length measurements from both ultrasonography and direct measurement post dissection.\nSchanz pin protrusion measurements are underestimated by ultrasonography (p<0.01) by an average of 10 percent over the range of 5 to 18 mm, and they display a proportional bias that increases the under reporting as the magnitude of pin protrusion increases. Ultrasound data demonstrate good linear correlation and closely represent actual protrusion values in the 5 to 12 mm range. Interobserver repeatability analysis demonstrated that all examiners were not statistically different in their measurements despite minimal familiarity with the ultrasound methodology (p>0.8).\"\nQuestion:\n\"Is portable ultrasonography accurate in the evaluation of Schanz pin placement during extremity fracture fixation in austere environments?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12848629": {
                "source": [
                    "\"Tuberculosis has increased in parallel with the acquired immunodeficiency syndrome epidemic and the use of immunosuppressive therapy, and the growing incidence of extra-pulmonary tuberculosis, especially with intestinal involvement, reflects this trend. However, the duration of anti-tuberculous therapy has not been clarified in intestinal tuberculosis.AIM: To compare the efficacy of different treatment durations in tuberculous enterocolitis in terms of response and recurrence rates.\nForty patients with tuberculous enterocolitis were randomized prospectively: 22 patients into a 9-month and 18 into a 15-month group. Diagnosis was made either by colonoscopic findings of discrete ulcers and histopathological findings of caseating granuloma and/or acid-fast bacilli, or by clinical improvement after therapeutic trial. Patients were followed up with colonoscopy every other month until complete response or treatment completion, and then every 6 months for 1 year and annually. Complete response was defined as a resolution of symptoms and active tuberculosis by colonoscopy.\nComplete response was obtained in all patients in both groups. Two patients in the 9-month group and one in the 15-month group underwent operation due to intestinal obstruction and perianal fistula, respectively. No recurrence of active intestinal tuberculosis occurred during the follow-up period in either group.\"\nQuestion:\n\"Is a 9-month treatment sufficient in tuberculous enterocolitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27394685": {
                "source": [
                    "\"The prevalence of combined humeral and glenoid defects varies between 79 and 84\u00a0% in case of chronic posttraumatic anterior shoulder instability. The main goal of this study was to evaluate the relationship between humeral and glenoid defects based on quantitative radiological criteria.\nA retrospective study was performed between 2000 and 2011 including patients who underwent primary surgical shoulder stabilization for chronic posttraumatic anterior shoulder instability, with bone defects in both the glenoid and humerus and a healthy contralateral shoulder. The following measurements were taken: D/R ratio (Hill-Sachs lesion depth/humeral head radius) on an AP X-ray in internal rotation and the D1/D2 ratio [diameter of the involved glenoid articular surfaces (D1)/the healthy one (D2)] on a comparative Bernageau glenoid profile view. Measurements were taken by two observers. Correlations were determined by the Spearman correlation coefficients (r), Bland and Altman diagrams, and intra-class correlation coefficients (ICC). A sample size calculation was done.\nThirty patients were included, 25 men/5 women, mean age 29.8\u00a0\u00b1\u00a011.2\u00a0years. The mean D/R was 23\u00a0\u00b1\u00a012\u00a0% for observer 1 and 23\u00a0\u00b1\u00a010\u00a0% for observer 2. The mean D1/D2 was 95\u00a0\u00b1\u00a04\u00a0% for observer 1 and 94\u00a0\u00b1\u00a06\u00a0% for observer 2. No significant correlation was found between humeral and glenoid bone defects by observer 1 (r\u00a0=\u00a00.23, p\u00a0=\u00a00.22) or observer 2 (r\u00a0=\u00a00.05, p\u00a0=\u00a00.78). Agreement of the observers for the D/R ratio was excellent (ICC\u00a0=\u00a00.89\u00a0\u00b1\u00a00.04, p\u00a0<\u00a00.00001) and good for the D1/D2 ratio (ICC\u00a0=\u00a00.54\u00a0\u00b1\u00a00.14, p\u00a0=\u00a00.006).\"\nQuestion:\n\"Bony defects in chronic anterior posttraumatic dislocation of the shoulder: Is there a correlation between humeral and glenoidal lesions?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10490564": {
                "source": [
                    "\"To determine whether anginal episodes might be related to extremes of hypotension in patients with ischaemic heart disease taking drugs to treat angina and heart failure.\nObservational study of patients with ischaemic heart disease attending an urban tertiary referral cardiology centre.\nA selected patient population was enrolled, having: angina on one or more hypotensive cardiovascular medications; hypotension on clinic or ambulatory measurement; and a resting ECG suitable for ambulatory monitoring. Patients had echocardiography, ambulatory blood pressure monitoring, and Holter monitoring. Hypotension induced ischaemic (HII) events were defined as episodes of ST segment ischaemia occurring at least one minute after an ambulatory blood pressure measurement (systolic/diastolic) below 100/65 mm Hg during the day, or 90/50 mm Hg at night.\n25 suitable patients were enrolled, and 107 hypotensive events were documented. 40 ST events occurred in 14 patients, of which a quarter were symptomatic. Fourteen HII events occurred in eight patients, with 13 of the 14 preceded by a fall in diastolic pressure (median diastolic pressure 57.5 mm Hg, interquartile range 11, maximum 72 mm Hg, minimum 45 mm Hg), and six preceded by a fall in systolic pressure (chi(2) = 11.9, p<0.001). ST events were significantly associated with preceding hypotensive events (chi(2) = 40.2, p<0.0001). Patients with HII events were more frequently taking multiple hypotensive drug regimens (8/8 v 9/17, chi(2) = 5.54, p = 0.022).\"\nQuestion:\n\"Hypotension in patients with coronary disease: can profound hypotensive events cause myocardial ischaemic events?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24235894": {
                "source": [
                    "\"Sleep bruxism (SB) is reported to vary in frequency over time. The aim of this study was to assess the first night effect on SB.\nA retrospective polysomnographic (PSG) analysis was performed of data from a sample of SB patients (12 females, 4 males; age range: 17-39 years) recorded in a sleep laboratory over 2 consecutive nights. Sleep parameters and jaw muscle activity variables (i.e., rhythmic masticatory muscle activity [RMMA]) for SB were quantified and compared between the 2 nights. Subjects were classified into groups according to severity of RMMA frequency, such as low frequency (2-4 episodes/h and/or<25 bursts/h) and moderate-high frequency (\u2265 4 episodes/h and \u2265 25 bursts/h).\nOverall, no first night effects were found for most sleep variables. However, total sleep time, sleep efficiency, and stage transitions showed significant time and group interactions (repeated measures ANOVAs, p \u2264 0.05). The RMMA episode index did not differ between the 2 nights, whereas the second night showed significantly higher burst index, bruxism time index, and mean burst duration (repeated measure ANOVAs, p \u2264 0.05). Five patients of 8 in the low frequency group were classified into the moderate-high frequency group on the second night, whereas only one patient in the moderate-high frequency group moved to the low frequency group.\"\nQuestion:\n\"Is there a first night effect on sleep bruxism?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17894828": {
                "source": [
                    "\"An association has been described between elevated serum angiotensin-converting enzyme (ACE) and an increased risk of severe hypoglycaemia (SH). To ascertain whether this reported association could be replicated in a different country, it was re-examined in 300 individuals with Type 1 diabetes.\nPeople with Type 1 diabetes, none of whom was taking renin-angiotensin system blocking drugs, were recruited. Participants recorded the frequency with which they had experienced SH. Glycated haemoglobin (HbA(1c)) and serum ACE were measured. The difference in the incidence of SH between different quartiles of ACE activity and the relationship between serum ACE and SH were examined using non-parametric statistical tests and a negative binomial model.\nData were obtained from 300 patients [158 male; HbA(1c) median (range) 8.2% (5.2-12.8%), median age 36 years (16-88); duration of diabetes 14.5 years (2-49)]. The incidence of SH was 0.93 episodes per patient year. The mean incidence of SH in the top and bottom quartiles of ACE activity was 0.5 and 1.7 episodes per patient year, respectively, but this difference was not statistically significant (P = 0.075). Spearman's test showed a very weak, although statistically significant, association between serum ACE level and SH incidence (r = 0.115, P = 0.047). The binomial model also showed a statistically significant (P = 0.002), but clinically weak, relationship between serum ACE and SH.\"\nQuestion:\n\"Serum angiotensin-converting enzyme and frequency of severe hypoglycaemia in Type 1 diabetes: does a relationship exist?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12769830": {
                "source": [
                    "\"Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\"\nQuestion:\n\"Should tumor depth be included in prognostication of soft tissue sarcoma?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17551944": {
                "source": [
                    "\"To determine whether spectral Doppler measurements obtained from bilateral uterine, arcuate, radial, and spiral arteries in early gestation correlate with adverse pregnancy outcome.\nOne hundred five pregnant women underwent transvaginal Doppler sonographic examination of uteroplacental circulation at 6-12 weeks' gestation. Resistance index (RI) and pulsatility index (PI) of bilateral uterine, arcuate, radial, and spiral arteries were measured. Diameters of gestational sac (GS) and yolk sac, crown-rump length (CRL), GS-CRL difference, and GS/CRL ratio were also recorded. Correlation was made with pregnancy outcome.\nSixteen women developed adverse pregnancy outcome. In these women, right uterine artery PI and RI were significantly higher than in women with normal obstetrical outcome. Spiral artery PI and RI values were also higher, but the difference was not statistically significant. GS-CRL difference, GS/CRL ratio, and yolk sac diameters were significantly lower in this group.\"\nQuestion:\n\"Doppler examination of uteroplacental circulation in early pregnancy: can it predict adverse outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23495128": {
                "source": [
                    "\"The aim of the present study was to explore patients' views on the acceptability and feasibility of using colour to describe osteoarthritis (OA) pain, and whether colour could be used to communicate pain to healthcare professionals.\nSix group interviews were conducted with 17 patients with knee OA. Discussion topics included first impressions about using colour to describe pain, whether participants could associate their pain with colour, how colours related to changes to intensity and different pain qualities, and whether they could envisage using colour to describe pain to healthcare professionals.\nThe group interviews indicated that, although the idea of using colour was generally acceptable, it did not suit all participants as a way of describing their pain. The majority of participants chose red to describe high-intensity pain; the reasons given were because red symbolized inflammation, fire, anger and the stop signal in a traffic light system. Colours used to describe the absence of pain were chosen because of their association with positive emotional feelings, such as purity, calmness and happiness. A range of colours was chosen to represent changes in pain intensity. Aching pain was consistently identified as being associated with colours such as grey or black, whereas sharp pain was described using a wider selection of colours. The majority of participants thought that they would be able to use colour to describe their pain to healthcare professionals, although issues around the interpretability and standardization of colour were raised.\"\nQuestion:\n\"The colour of pain: can patients use colour to describe osteoarthritis pain?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27078715": {
                "source": [
                    "\"Digital tomosynthesis (DT) is a new X-ray-based imaging technique that allows image enhancement with minimal increase in radiation exposure. The purpose of this study was to compare DT with noncontrast computed tomography (NCCT) and to evaluate its potential role for the follow-up of patients with nephrolithiasis in a nonemergent setting.\nA retrospective review of patients with nephrolithiasis at our institution that underwent NCCT and DT from July 2012 to September 2013 was performed. Renal units (RUs) that did not undergo treatment or stone passage were randomly assigned to two blinded readers, who recorded stone count, size area (mm(2)), maximum stone length (mm), and location, for both DT and NCCT. Mean differences per RU were compared. Potential variables affecting stone detection rate, including stone size and body mass index (BMI), were evaluated. Interobserver agreement was determined using the intraclass correlation coefficient to measure the consistency of measurements made by the readers.\nDT and NCCT demonstrated similar stone detection rates in terms of stone counts and stone area mm(2). Of the 79 RUs assessed, 41 RUs showed exact stone counts on DT and NCCT. The mean difference in stone area was 16.5\u2009mm(2) (-4.6 to 38.5), p\u2009=\u20090.121. The mean size of the largest stone on NCCT and DT was 9.27 and 8.87\u2009mm, respectively. Stone size and BMI did not cause a significant difference in stone detection rates. Interobserver agreement showed a strong correlation between readers and adequate reproducibility.\"\nQuestion:\n\"Digital Tomosynthesis: A Viable Alternative to Noncontrast Computed Tomography for the Follow-Up of Nephrolithiasis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22302658": {
                "source": [
                    "\"Patients with aggressive lower extremity musculoskeletal tumors may be candidates for either above-knee amputation or limb-salvage surgery. However, the subjective and objective benefits of limb-salvage surgery compared with amputation are not fully clear.QUESTIONS/\nWe therefore compared functional status and quality of life for patients treated with above-knee amputation versus limb-salvage surgery.\nWe reviewed 20 of 51 patients aged 15 years and older treated with above-knee amputation or limb-salvage surgery for aggressive musculoskeletal tumors around the knee between 1994 and 2004 as a retrospective cohort study. At last followup we obtained the Physiological Cost Index, the Reintegration to Normal Living Index, SF-36, and the Toronto Extremity Salvage Score questionnaires. The minimum followup was 12 months (median, 56 months; range, 12-108 months).\nCompared with patients having above-knee amputation, patients undergoing limb-salvage surgery had superior Physiological Cost Index scores and Reintegration to Normal Living Index. The Toronto Extremity Salvage scores and SF-36 scores were similar in the two groups.\"\nQuestion:\n\"Does limb-salvage surgery offer patients better quality of life and functional capacity than amputation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22954812": {
                "source": [
                    "\"Recent reports indicate that the prevalence of bipolar disorder (BD) in patients with an acute major depressive episode might be higher than previously thought. We aimed to study systematically all patients who sought therapy for major depressive episode (MDE) within the BRIDGE study in Germany, reporting on an increased number (increased from 2 in the international BRIDGE report to 5) of different diagnostic algorithms.\nA total of 252 patients with acute MDE (DSM-IV confirmed) were examined for the existence of BD (a) according to DSM-IV criteria, (b) according to modified DSM-IV criteria (without the exclusion criterion of 'mania not induced by substances/antidepressants'), (c) according to a Bipolarity Specifier Algorithm which expands the DSM-IV criteria, (d) according to HCL-32R (Hypomania-Checklist-32R), and (e) according to a criteria-free physician's diagnosis.\nThe five different diagnostic approaches yielded immensely variable prevalences for BD: (a) 11.6; (b) 24.8%; (c) 40.6%; (d) 58.7; e) 18.4% with only partial overlap between diagnoses according to the physician's diagnosis or HCL-32R with diagnoses according to the three DSM-based algorithms.\"\nQuestion:\n\"Are bipolar disorders underdiagnosed in patients with depressive episodes?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24481006": {
                "source": [
                    "\"79 adjacent proximal surfaces without restorations in permanent teeth were examined. Patients suspected to have carious lesions after a visual clinical and a bitewing examination participated in a CBCT examination (Kodak 9000 3D, 5 \u00d7 3.7 cm field of view, voxel size 0.07 mm). Ethical approval and informed consent were obtained according to the Helsinki Declaration. Radiographic assessment recording lesions with or without cavitation was performed by two observers in bitewings and CBCT sections. Orthodontic separators were placed interdentally between two lesion-suspected surfaces. The separator was removed after 3 days and the surfaces recorded as cavitated (yes/no), i.e. validated clinically. Differences between the two radiographic modalities (sensitivity, specificity and overall accuracy) were estimated by analyzing the binary data in a generalized linear model.\nFor both observers, sensitivity was significantly higher for CBCT than for bitewings (average difference 33%, p<0.001) while specificity was not significantly different between the methods (p = 0.19). The overall accuracy was also significantly higher for CBCT (p<0.001).\"\nQuestion:\n\"Should cavitation in proximal surfaces be reported in cone beam computed tomography examination?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15995461": {
                "source": [
                    "\"This article examines the hypothesis that the six U.S. states with the highest rates of road traffic deaths (group 1 states) also had above-average rates of other forms of injury such as falling, poisoning, drowning, fire, suffocation, homicide, and suicide, and also for the retail trade and construction industries. The converse, second hypothesis, for the six states with the lowest rates of road traffic deaths (group 2 states) is also examined.\nData for these 12 states for the period 1983 to 1995 included nine categories of unintentional and four categories of intentional injury. Seventy-four percent of the group 1 states conformed to the first hypothesis, and 85% of the group 2 states conformed to the second hypothesis.\"\nQuestion:\n\"Do some U.S. states have higher/lower injury mortality rates than others?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21558951": {
                "source": [
                    "\"To ascertain whether level of intrauterine cocaine exposure (IUCE) is associated with early adolescent delinquent behavior, after accounting for prenatal exposures to other psychoactive substances and relevant psychosocial factors.\nNinety-three early adolescents (12.5-14.5 years old) participating since birth in a longitudinal study of IUCE reported delinquent acts via an audio computer-assisted self-interview. Level of IUCE and exposure to cigarettes, alcohol, and marijuana were determined by maternal report, maternal and infant urine assays, and infant meconium assays at birth. Participants reported their exposure to violence on the Violence Exposure Scale for Children-Revised at ages 8.5, 9.5, and 11 years and during early adolescence, and the strictness of supervision by their caregivers during early adolescence.\nOf the 93 participants, 24 (26%) reported \u2265 3 delinquent behaviors during early adolescence. In the final multivariate model (including level of IUCE and cigarette exposure, childhood exposure to violence, and caregiver strictness/supervision) \u2265 3 delinquent behaviors were not significantly associated with level of IUCE but were significantly associated with intrauterine exposure to half a pack or more of cigarettes per day and higher levels of childhood exposure to violence, effects substantially unchanged after control for early adolescent violence exposure.\"\nQuestion:\n\"Are there effects of intrauterine cocaine exposure on delinquency during early adolescence?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19640728": {
                "source": [
                    "\"Bias against operating on patients with prosthetic valve endocarditis (PVE) who have multiple prostheses may preclude the use of life-saving valve replacement. We investigated the accuracy of the preoperative diagnosis of PVE in patients with both mitral and aortic prosthesis and the safety of single-valve replacement when only one valve seemed infected.\nPatients with a diagnosis of active PVE who had mitral and aortic prosthesis in place were assessed. We looked at the methods for diagnosis, causative agents, indication for valve replacement, operative findings and outcome.\nTwenty patients, who had both mitral and aortic prostheses and a diagnosis of PVE, were assessed. Streptococci and staphylococci caused 70% of cases. By means of echocardiography, the valves involved were: mitral (11 patients), aortic (six patients), and in three cases both prosthetic valves seemed infected. Surgery was undertaken in 17 patients (85%). The positive predictive value of transesophageal echocardiogram (TEE) for the preoperative diagnosis of the site of infection was 100%. In 13 patients, only the prosthetic valve that seemed infected was replaced. Four of these patients died within a week after the procedure. Nine patients survived the surgical procedure, completed a course of antimicrobial therapy and were followed up for 15.78 months (95% CI: 12.83-18.72). All were considered cured and relapses were not observed.\"\nQuestion:\n\"Surgical treatment of prosthetic valve endocarditis in patients with double prostheses: is single-valve replacement safe?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15800018": {
                "source": [
                    "\"Impaired fasting glucose (IFG) below the diagnostic threshold for diabetes mellitus (DM) is associated with macrovascular pathology and increased mortality after percutaneous coronary interventions. The study goal was to determine whether pre-operative fasting blood glucose (fB-glu) is associated with an increased mortality after coronary artery bypass grafting (CABG).\nDuring 2001-03, 1895 patients underwent primary CABG [clinical DM (CDM) in 440/1895; complete data on fB-glu for n=1375/1455]. Using pre-operative fB-glu, non-diabetics were categorized as having normal fB-glu (<5.6 mmol/L), IFG (5.6<or =fB-glu<6.1 mmol/L), or suspected DM (SDM) (>or =6.1 mmol/L). fB-glu was normal in 59%. The relative risks of 30 day and 1 year mortality compared with patients with normal fB-glu was 1.7 [95% confidence interval (CI): 0.5-5.5] and 2.9 (CI: 0.8-11.2) with IFG, 2.8 (CI: 1.1-7.2) and 1.9 (CI: 0.5-6.3) with SDM vs. 1.8 (CI: 0.8-4.0) and 1.6 (CI: 0.6-4.3) if CDM, respectively. The receiver operator characteristic area for the continuous variable fB-glu and 1 year mortality was 0.65 (P=0.002).\"\nQuestion:\n\"Are even impaired fasting blood glucose levels preoperatively associated with increased mortality after CABG surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18847643": {
                "source": [
                    "\"Trauma patients who require therapeutic anticoagulation pose a difficult treatment problem. The purpose of this study was to determine: (1) the incidence of complications using therapeutic anticoagulation in trauma patients, and (2) if any patient factors are associated with these complications.\nAn 18-month retrospective review was performed on trauma patients>or= 15 years old who received therapeutic anticoagulation using unfractionated heparin (UH) and/or fractionated heparin (FH). Forty different pre-treatment and treatment patient characteristics were recorded. Complications of anticoagulation were documented and defined as any unanticipated discontinuation of the anticoagulant for bleeding or other adverse events.\nOne-hundred-fourteen trauma patients were initiated on therapeutic anticoagulation. The most common indication for anticoagulation was deep venous thrombosis (46%). Twenty-four patients (21%) had at least 1 anticoagulation complication. The most common complication was a sudden drop in hemoglobin concentration requiring blood transfusion (11 patients). Five patients died (4%), 3 of whom had significant hemorrhage attributed to anticoagulation. Bivariate followed by logistic regression analysis identified chronic obstructive pulmonary disease (OR = 9.2, 95%CI = 1.5-54.7), UH use (OR = 3.8, 95%CI = 1.1-13.0), and lower initial platelet count (OR = 1.004, 95%CI = 1.000-1.008) as being associated with complications. Patients receiving UH vs. FH differed in several characteristics including laboratory values and anticoagulation indications.\"\nQuestion:\n\"Therapeutic anticoagulation in the trauma patient: is it safe?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9488747": {
                "source": [
                    "\"Apparent life-threatening events in infants are a difficult and frequent problem in pediatric practice. The prognosis is uncertain because of risk of sudden infant death syndrome.\nEight infants aged 2 to 15 months were admitted during a period of 6 years; they suffered from similar maladies in the bath: on immersion, they became pale, hypotonic, still and unreactive; recovery took a few seconds after withdrawal from the bath and stimulation. Two diagnoses were initially considered: seizure or gastroesophageal reflux but this was doubtful. The hypothesis of an equivalent of aquagenic urticaria was then considered; as for patients with this disease, each infant's family contained members suffering from dermographism, maladies or eruption after exposure to water or sun. All six infants had dermographism. We found an increase in blood histamine levels after a trial bath in the two infants tested. The evolution of these \"aquagenic maladies\" was favourable after a few weeks without baths. After a 2-7 year follow-up, three out of seven infants continue to suffer from troubles associated with sun or water.\"\nQuestion:\n\"Syncope during bathing in infants, a pediatric form of water-induced urticaria?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11138995": {
                "source": [
                    "\"Alexithymia is presumed to play an important predisposing role in the pathogenesis of medically unexplained physical symptoms. However, no research on alexithymia has been done among general medical outpatients who present with medically unexplained physical symptoms as their main problem and in which anxiety and depression have been considered as possible confounding factors. This study investigated whether patients with medically unexplained physical symptoms are more alexithymic than those with explained symptoms and whether, in patients with unexplained symptoms, alexithymia is associated with subjective health experience and use of medical services.\nWe conducted a cross-sectional study among patients attending an internal medicine outpatient clinic. All patients were given a standardized interview and completed a number of questionnaires.\nAfter complete physical examinations, 169 of 321 patients had unexplained physical symptoms according to two independent raters. Patients with medically unexplained symptoms more often had a mental disorder, but overall they were not more alexithymic. In patients with unexplained physical symptoms, alexithymia was not associated with subjective health experience or use of medical services. However, patients with both unexplained symptoms and a mental disorder who also denied any possible connection between emotional problems and their physical symptoms did have more alexithymic traits.\"\nQuestion:\n\"Is alexithymia a risk factor for unexplained physical symptoms in general medical outpatients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28143468": {
                "source": [
                    "\"Prior literature identified the use of Performance Measurement Systems (PMS) as crucial in addressing improved processes of care. Moreover, a strategic use of PMS has been found to enhance quality, compared to non-strategic use, although a clear understanding of this linkage is still to be achieved. This paper deals with the test of direct and indirect models related to the link between the strategic use of PMS and the level of improved processes in health care organizations. Indirect models were mediated by the degree of perceived managerial discretion.\nA PLS analysis on a survey of 97 Italian managers working for health care organizations in the Lombardy region was conducted. The response rate was 77.6%.\nThe strategic use of PMS in health care organizations directly and significantly (p\u2009<\u20090.001) enhances performance in terms of improved processes. Perceived managerial discretion is positively and significantly (p\u2009<\u20090.001) affected by the strategic use of PMS, whereas the mediation effect is non-significant.\"\nQuestion:\n\"Are performance measurement systems useful?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24901580": {
                "source": [
                    "\"In this study, we aimed to evaluate the potential use of a 3-phase bone scintigraphy method to determine the level of amputation on treatment cost, morbidity and mortality, reamputation rates, and the duration of hospitalization in diabetic foot.\nThirty patients who were admitted to our clinic between September 2008 and July 2009, with diabetic foot were included. All patients were evaluated according to age, gender, diabetes duration, 3-phase bone scintigraphy, Doppler ultrasound, amputation/reamputation levels, and hospitalization periods. Patients underwent 3-phase bone scintigraphy using technetium-99m methylene diphosphonate, and the most distal site of the region displaying perfusion during the perfusion and early blood flow phase was marked as the amputation level. Amputation level was determined by 3-phase bone scintigraphy, Doppler ultrasound, and inspection of the infection-free clear region during surgery.\nThe amputation levels of the patients were as follows: finger in six (20%), ray amputation in five (16.6%), transmetatarsal in one (3.3%), Lisfranc in two (6.6%), Chopart in seven (23.3%), Syme in one (3.3%), below-the-knee in six (20%), above the knee in one (3.3%), knee disarticulation in one (3.3%), and two patients underwent amputation at other centers. After primary amputation, reamputation was performed on seven patients, and one patient was treated with debridement for wound site problems. No mortality was encountered during study.\"\nQuestion:\n\"Is scintigraphy a guideline method in determining amputation levels in diabetic foot?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21398266": {
                "source": [
                    "\"Older adults (OA) with advanced cancer (AC) undergoing phase I clinical trials (PICT) have poor prognosis. There are no studies which describe symptoms experienced by OA.\nRetrospective chart review of PICT participants>60 years. OA were compared by age (>65 vs 60-65) and by number of symptoms (>3 vs \u22643).\nN = 56. Mean age = 67.09; 48.21% female. Median life-expectancy = 5 months (interquartile range = 2-9 months); 80.36% had pain; of those 64% without pain scale. Most did not have interdisciplinary professionals or hospice referrals. Older adults with>3 symptoms had more admissions (37.5% vs 14.29%; P = .0335), complications (46.43% vs 16.07%; P = .0026), and greater decline in functional status (24 participants>3 symptoms vs 8; P = .0173). There were no significant differences comparing OA by age.\"\nQuestion:\n\"Do symptoms matter when considering patients for phase I clinical trials?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25274085": {
                "source": [
                    "\"Several single nucleotide polymorphisms (SNPs) at different loci have been associated with breast cancer susceptibility, accounting for around 10% of the familial component. Recent studies have found direct associations between specific SNPs and breast cancer in BRCA1/2 mutation carriers. Our aim was to determine whether validated susceptibility SNP scores improve the predictive ability of risk models in comparison/conjunction to other clinical/demographic information.\nFemale BRCA1/2 carriers were identified from the Manchester genetic database, and included in the study regardless of breast cancer status or age. DNA was extracted from blood samples provided by these women and used for gene and SNP profiling. Estimates of survival were examined with Kaplan-Meier curves. Multivariable Cox proportional hazards models were fit in the separate BRCA datasets and in menopausal stages screening different combinations of clinical/demographic/genetic variables. Nonlinear random survival forests were also fit to identify relevant interactions. Models were compared using Harrell's concordance index (1 - c-index).\n548 female BRCA1 mutation carriers and 523 BRCA2 carriers were identified from the database. Median Kaplan-Meier estimate of survival was 46.0 years (44.9-48.1) for BRCA1 carriers and 48.9 (47.3-50.4) for BRCA2. By fitting Cox models and random survival forests, including both a genetic SNP score and clinical/demographic variables, average 1 - c-index values were 0.221 (st.dev. 0.019) for BRCA1 carriers and 0.215 (st.dev. 0.018) for BRCA2 carriers.\"\nQuestion:\n\"Can multiple SNP testing in BRCA2 and BRCA1 female carriers be used to improve risk prediction models in conjunction with clinical assessment?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8111516": {
                "source": [
                    "\"To determine whether volunteer family physician reports of the frequency of influenza-like illness (ILI) usefully supplement information from other influenza surveillance systems conducted by the Centers for Disease Control and Prevention.\nEvaluation of physician reports from five influenza surveillance seasons (1987-88 through 1991-92).\nFamily physician office practices in all regions of the United States.\nAn average of 140 physicians during each of five influenza seasons.\nNone.\nAn office visit or hospitalization of a patient for ILI, defined as presence of fever (temperature>or = 37.8 degrees C) and cough, sore throat, or myalgia, along with the physician's clinical judgment of influenza. A subset of physicians collected specimens for confirmation of influenza virus by culture.\nPhysicians attributed 81,408 (5%) of 1,672,542 office visits to ILI; 2754 (3%) patients with ILI were hospitalized. Persons 65 years of age and older accounted for 11% of visits for ILI and 43% of hospitalizations for ILI. In three of five seasons, physicians obtained influenza virus isolates from a greater proportion of specimens compared with those processed by World Health Organization laboratories (36% vs 12%). Influenza virus isolates from sentinel physicians peaked from 1 to 4 weeks earlier than those reported by World Health Organization laboratories. Physicians reported peak morbidity 1 to 4 weeks earlier than state and territorial health departments in four of five seasons and 2 to 5 weeks earlier than peak mortality reported by 121 cities during seasons with excess mortality associated with pneumonia and influenza.\"\nQuestion:\n\"Do family physicians make good sentinels for influenza?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18359123": {
                "source": [
                    "\"Swedish hospital mergers seem to stem from a conviction among policy makers that bigger hospitals lead to lower average costs and improved clinical outcomes. The effects of mergers in the form of multisited hospitals have not been systematically evaluated. The purpose of this article is to contribute to this area of knowledge by exploring responses to the merger of Blekinge Hospital.\nThe evaluation was guided by the philosophy of triangulation. A questionnaire was sent to 597 randomly selected employees, that is 24% of the health care staff. Four hundred ninety-eight employees answered the questionnaire, giving a response rate of 83%. Furthermore, interviews of different groups of stakeholders were conducted.\nA moderate increase of quality was assessed, which, a low proportion of the employees perceived had decisively or largely to do with the merger. The majority perceives economical incentives as the drivers of change, but, at the same time, only 10% of this group believes this target was reached completely or to a large extent.\"\nQuestion:\n\"Is it better to be big?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11729377": {
                "source": [
                    "\"To assess and compare the value of split-liver transplantation (SLT) and living-related liver transplantation (LRT).\nThe concept of SLT results from the development of reduced-size transplantation. A further development of SLT, the in situ split technique, is derived from LRT, which itself marks the optimized outcome in terms of postoperative graft function and survival. The combination of SLT and LRT has abolished deaths on the waiting list, thus raising the question whether living donor liver transplantation is still necessary.\nOutcomes and postoperative liver function of 43 primary LRT patients were compared with those of 49 primary SLT patients (14 ex situ, 35 in situ) with known graft weight performed between April 1996 and December 2000. Survival rates were analyzed using the Kaplan-Meier method.\nAfter a median follow-up of 35 months, actual patient survival rates were 82% in the SLT group and 88% in the LRT group. Actual graft survival rates were 76% and 81%, respectively. The incidence of primary nonfunction was 12% in the SLT group and 2.3% in the LRT group. Liver function parameters (prothrombin time, factor V, bilirubin clearance) and surgical complication rates did not differ significantly. In the SLT group, mean cold ischemic time was longer than in the LRT group. Serum values of alanine aminotransferase during the first postoperative week were significantly higher in the SLT group. In the LRT group, there were more grafts with signs of fatty degeneration than in the SLT group.\"\nQuestion:\n\"Is there still a need for living-related liver transplantation in children?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16046584": {
                "source": [
                    "\"Irregular bleeding affects many users of combined menopausal hormone therapy (HT) and commonly leads to invasive and expensive investigations to exclude underlying malignancy. In most cases no abnormality is found.\nThe main objective of this study was to explore the role of uterine natural killer (uNK) cells and their regulatory cytokine IL-15 in irregular bleeding in HT users.\nThis was a prospective observational study conducted between 2002 and 2004.\nThe study was conducted in a tertiary referral menopause clinic at King Edward Memorial Hospital, Western Australia.\nPatients included 117 postmenopausal women taking combined HT.\nOutpatient endometrial biopsies were taken during and outside bleeding episodes.\nThe relationship between endometrial uNK cells (CD56+) and bleeding patterns was measured. We also addressed the impact of HT exposure on uNK cell populations, the relationship between endometrial IL-15 expression and uNK cell populations, and killer Ig like receptor genotype in subjects with irregular bleeding.\nEndometrial CD56+ uNK cells were significantly increased in biopsies obtained during bleeding episodes (P<0.001), compared with HT users with no bleeding. The highest level of IL-15 expression was also seen in biopsies taken during bleeding. No clear relationship between killer Ig like receptor genotype and bleeding on HT was observed.\"\nQuestion:\n\"Menopausal hormone therapy and irregular endometrial bleeding: a potential role for uterine natural killer cells?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9191526": {
                "source": [
                    "\"In an attempt to improve the care they provide for their patients with breast cancer, the authors' institution developed a multidisciplinary breast cancer clinic (MDBCC) to offer \"one-stop shopping\" consultation and support for newly diagnosed breast cancer patients.\nOne hundred sixty-two patients, the control group for this study, were evaluated at Henry Ford Hospital during the year prior to the opening of the MDBCC. These patients, who were referred in the traditional sequential consultation manner, were compared with the first 177 patients seen during the first year of the clinic's operation. Retrospective chart reviews were conducted to assess treatment timeliness, and anonymous questionnaires were used to assess patient satisfaction.\nThe authors found that the MDBCC increased patient satisfaction by encouraging involvement of patients' families and friends and by helping patients make treatment decisions (P<0.001). The time between diagnosis and the initiation of treatment was also significantly decreased (42.2 days vs. 29.6 days; P<0.0008).\"\nQuestion:\n\"Multidisciplinary breast cancer clinics. Do they work?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27642458": {
                "source": [
                    "\"Polio eradication is now feasible after removal of Nigeria from the list of endemic countries and global reduction of cases of wild polio virus in 2015 by more than 80%. However, all countries must remain focused to achieve eradication. In August 2015, the Catholic bishops in Kenya called for boycott of a polio vaccination campaign citing safety concerns with the polio vaccine. We conducted a survey to establish if the coverage was affected by the boycott.\nA cross sectional survey was conducted in all the 32 counties that participated in the campaign. A total of 90,157 children and 37,732 parents/guardians were sampled to determine the vaccination coverage and reasons for missed vaccination.\nThe national vaccination coverage was 93% compared to 94% in the November 2014 campaign. The proportion of parents/guardians that belonged to Catholic Church was 31% compared to 7% of the children who were missed. Reasons for missed vaccination included house not being visited (44%), children not being at home at time of visit (38%), refusal by parents (12%), children being as leep (1%), and various other reasons (5%). Compared to the November 2014 campaign, the proportion of children who were not vaccinated due to parent's refusal significantly increased from 6% to 12% in August 2015.\"\nQuestion:\n\"Did the call for boycott by the Catholic bishops affect the polio vaccination coverage in Kenya in 2015?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9616411": {
                "source": [
                    "\"To assess whether populations with access to general practitioner hospitals (GP hospitals) utilise general hospitals less than populations without such access.\nObservational study comparing the total rates of admissions and of occupied bed days in general hospitals between populations with and without access to GP hospitals. Comparisons were also made separately for diagnoses commonly encountered in GP hospitals.\nTwo general hospitals serving the population of Finnmark county in north Norway.\n35,435 admissions based on five years' routine recordings from the two hospitals.\nThe total rate of admission to general hospitals was lower in peripheral municipalities with a GP hospital than in central municipalities without this kind of institution, 26% and 28% lower for men and women respectively. The corresponding differences were 38% and 52%, when analysed for occupied bed days. The differences were most pronounced for patients with respiratory diseases, cardiac failure, and cancer who are primarily or intermediately treated or cared for in GP hospitals, and for patients with stroke and fractures, who are regularly transferred from general hospitals to GP hospitals for longer term follow up care.\"\nQuestion:\n\"Do general practitioner hospitals reduce the utilisation of general hospital beds?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12407608": {
                "source": [
                    "\"To investigate whether prepuncture ultrasound evaluation of vascular anatomy facilitates internal jugular vein cannulation compared with landmark-guided puncture.\nProspective randomized study.\nSingle community hospital.\nAdult patients undergoing general anesthesia (n = 240).\nThe right internal jugular vein was cannulated using either anatomic landmarks or prepuncture ultrasound (3.75/7.5 MHz) guidance. In the landmark group, respiratory jugular venodilation was used as the primary landmark for locating the vein. Results of cannulation and the incidence of complications were compared.\nPatients were randomly assigned to the ultrasound or landmark group. Respiratory jugular venodilation was identified in 188 patients (78.3%), in whom results of cannulation did not differ between the 2 techniques with respect to the venous access rate (cannulated at the first attempt: 83.5% in the landmark v 85.7% in the ultrasound group), the success rate (cannulated within 3 attempts: 96.9% v 95.6%), and the incidence of arterial puncture (1.0% v 3.3%). In the remaining 52 respiratory jugular venodilation-unidentified patients, the access rate (30.4% v 86.2%, p<0.001) and the success rate (78.3 v 100%, p<0.05) were significantly better in the ultrasound group, and no arterial puncture was recorded in the ultrasound group, whereas the incidence was 13.0% in the landmark group. The results were similar regardless of the ultrasound frequency used.\"\nQuestion:\n\"Does ultrasound imaging before puncture facilitate internal jugular vein cannulation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22497340": {
                "source": [
                    "\"To clarify whether horizontal canal ocular reflex is influenced by otolith organs input.\nThe subjects were seven healthy humans. The right ear was stimulated using ice-water. Each subject was kept in a left-ear-down position for 20 s and then repositioned to a prone position, a right-ear-down position and a supine position with 20 s intervals. Nystagmus was analysed using three-dimensional video-oculography.\nEye movements in the supine position and the prone position were not in a symmetric fashion. Nystagmus in the left-ear-down position and the right-ear-down position were not symmetric either. These phenomena indicate that the axis of the eyeball rotation was affected by the shift of the direction of gravity exerted on the head.\"\nQuestion:\n\"Is horizontal semicircular canal ocular reflex influenced by otolith organs input?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25787073": {
                "source": [
                    "\"The serum C-reactive protein (CRP) level correlates with the clinical prognosis in patients with kidney, penile and metastatic castration-resistant prostate cancer (PC). We prospectively evaluated the preoperative CRP level as a predictive marker for an advanced tumor stage or high-grade cancer in patients with clinically localized PC.\nThe study evaluated 629 patients with clinically localized PC who underwent radical prostatectomy between 2010 and 2013. Exclusion criteria were signs of systemic infection, symptoms of an autoimmune disease or neoadjuvant androgen deprivation.\nPoorly differentiated PC tends to be more common in patients with elevated CRP levels (15.5 vs. 9.5%, p = 0.08). Analogously, patients with a Gleason score \u22658 PC had significantly higher median CRP levels than those with a Gleason score \u22647 PC (1.9 vs. 1.2 mg/l, p = 0.03). However, neither uni- nor multivariate analysis showed an association between the preoperative CRP level and the presence of a locally advanced tumor stage, lymph node metastases or a positive surgical margin. CRP also failed to correlate with the initial PSA level and the clinical tumor-associated findings. Moreover, multivariate analysis relativized the association between an elevated CRP level and poor tumor differentiation.\"\nQuestion:\n\"Do preoperative serum C-reactive protein levels predict the definitive pathological stage in patients with clinically localized prostate cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25336163": {
                "source": [
                    "\"To describe the interstitial fluid (ISF) and plasma pharmacokinetics of meropenem in patients on continuous venovenous haemodiafiltration (CVVHDF).\nThis was a prospective observational pharmacokinetic study. Meropenem (500 mg) was administered every 8 h. CVVHDF was targeted as a 2-3 L/h exchange using a polyacrylonitrile filter with a surface area of 1.05 m2 and a blood flow rate of 200 mL/min. Serial blood (pre- and post-filter), filtrate/dialysate and ISF concentrations were measured on 2 days of treatment (Profiles A and B). Subcutaneous tissue ISF concentrations were determined using microdialysis.\nA total of 384 samples were collected. During Profile A, the comparative median (IQR) ISF and plasma peak concentrations were 13.6 (12.0-16.8) and 40.7 (36.6-45.6) mg/L and the trough concentrations were 2.6 (2.4-3.4) and 4.9 (3.5-5.0) mg/L, respectively. During Profile B, the ISF trough concentrations increased by \u223c40%. Meropenem ISF penetration was estimated at 63% (60%-69%) and 69% (65%-74%) for Profiles A and B, respectively, using comparative plasma and ISF AUCs. For Profile A, the plasma elimination t1/2 was 3.7 (3.3-4.0) h, the volume of distribution was 0.35 (0.25-0.46) L/kg, the total clearance was 4.1 (4.1-4.8) L/h and the CVVHDF clearance was 2.9 (2.7-3.1) L/h.\"\nQuestion:\n\"Are interstitial fluid concentrations of meropenem equivalent to plasma concentrations in critically ill patients receiving continuous renal replacement therapy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15943725": {
                "source": [
                    "\"Serum pancreatic lipase may improve the diagnosis of pancreatitis compared to serum amylase. Both enzymes have been measured simultaneously at our hospital allowing for a comparison of their diagnostic accuracy.\nSeventeen thousand five hundred and thirty-one measurements of either serum amylase and or serum pancreatic lipase were made on 10 931 patients treated at a metropolitan teaching hospital between January 2001 and May 2003. Of these, 8937 were initially treated in the Emergency Department. These results were collected in a database, which was linked by the patients' medical record number to the radiology and medical records. Patients with either an elevated lipase value or a discharge diagnosis of acute pancreatitis had their radiological diagnosis reviewed along with their biochemistry and histology record. The diagnosis of acute pancreatitis was made if there was radiological evidence of peripancreatic inflammation.\nOne thousand eight hundred and twenty-five patients had either elevated serum amylase and or serum pancreatic lipase. The medical records coded for pancreatitis in a further 55 whose enzymes were not elevated. Three hundred and twenty of these had radiological evidence of acute pancreatitis. Receiver operator characteristic analysis of the initial sample from patients received in the Emergency Department showed improved diagnostic accuracy for serum pancreatic lipase (area under the curve (AUC) 0.948) compared with serum amylase (AUC, 0.906, P<0.05). A clinically useful cut-off point would be at the diagnostic threshold; 208 U/L (normal<190 U/L) for serum pancreatic lipase and 114 U/L (normal 27-100 U/L) for serum amylase where the sensitivity was 90.3 cf., 76.8% and the specificity was 93 cf., 92.6%. 18.8% of the acute pancreatitis patients did not have elevated serum amylase while only 2.9% did not have elevated serum pancreatic lipase on the first emergency department measurement.\"\nQuestion:\n\"Should serum pancreatic lipase replace serum amylase as a biomarker of acute pancreatitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26859535": {
                "source": [
                    "\"Updated guidelines for the screening and management of cervical cancer in the United States recommend starting Papanicolaou (Pap) testing at age 21 and screening less frequently with less aggressive management for abnormalities. We sought to examine updated Pap test screening guidelines and how they may affect the detection of invasive cervical cancer, especially among women<30 years of age.\nPatients diagnosed at Brigham and Women's Hospital with invasive cervical cancer between 2002 and 2012 were retrospectively identified. Prior screening history was obtained and patients were divided into two groups based on age<30 years or age \u226530 years. The two groups were then compared with respect to demographics, pathological findings, and time to diagnosis.\nA total of 288 patients with invasive cervical carcinoma were identified. Among these patients, 109 had adequate information on prior screening history. Invasive adenocarcinoma (IAC) was diagnosed in 37 (33.94%) patients, whereas 64 (58.72%) patients were diagnosed with invasive squamous cell carcinoma (ISCC). The remaining eight patients were diagnosed with other types of cancers of the cervix. A total of 13 patients were younger than 30 while 96 patients were 30 or older. The mean time from normal Pap to diagnosis of IAC was 15 months in patients younger than 30 years of age compared to 56 months in patients aged 30 and older (p\u2009<\u20090.001). The mean time from normal Pap to diagnosis of ISCC was 38 months in patients younger than 30 years of age and 82 months in patients aged 30 and older (p\u2009=\u20090.018).\"\nQuestion:\n\"Screening History Among Women with Invasive Cervical Cancer in an Academic Medical Center: Will We Miss Cancers Following Updated Guidelines?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22453060": {
                "source": [
                    "\"Bystander resuscitation plays an important role in lifesaving cardiopulmonary resuscitation (CPR). A significant reduction in the \"no-flow-time\", quantitatively better chest compressions and an improved quality of ventilation can be demonstrated during CPR using supraglottic airway devices (SADs). Previous studies have demonstrated the ability of inexperienced persons to operate SADs after brief instruction. The aim of this pilot study was to determine whether an instruction manual consisting of four diagrams enables laypersons to operate a Laryngeal Mask Supreme\u00ae (LMAS) in the manikin.\nAn instruction manual of four illustrations with speech bubbles displaying the correct use of the LMAS was designed. Laypersons were handed a bag containing a LMAS, a bag mask valve device (BMV), a syringe prefilled with air and the instruction sheet, and were asked to perform and ventilate the manikin as displayed. Time to ventilation was recorded and degree of success evaluated.\nA total of 150 laypersons took part. Overall 145 participants (96.7%) inserted the LMAS in the manikin in the right direction. The device was inserted inverted or twisted in 13 (8.7%) attempts. Eight (5.3%) individuals recognized this and corrected the position. Within the first 2 minutes 119 (79.3%) applicants were able to insert the LMAS and provide tidal volumes greater than 150 ml (estimated dead space). Time to insertion and first ventilation was 83.2 \u00b1 29 s. No significant difference related to previous BLS training (P = 0.85), technical education (P = 0.07) or gender could be demonstrated (P = 0.25).\"\nQuestion:\n\"Does a 4 diagram manual enable laypersons to operate the Laryngeal Mask Supreme\u00ae?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19268855": {
                "source": [
                    "\"The identification of the most suspect enhancing part of a lesion is regarded as a major diagnostic criterion in dynamic magnetic resonance mammography. Computer-aided diagnosis (CAD) software allows the semi-automatic analysis of the kinetic characteristics of complete enhancing lesions, providing additional information about lesion vasculature. The diagnostic value of this information has not yet been quantified.\nConsecutive patients from routine diagnostic studies (1.5 T, 0.1 mmol gadopentetate dimeglumine, dynamic gradient-echo sequences at 1-minute intervals) were analyzed prospectively using CAD. Dynamic sequences were processed and reduced to a parametric map. Curve types were classified by initial signal increase (not significant, intermediate, and strong) and the delayed time course of signal intensity (continuous, plateau, and washout). Lesion enhancement was measured using CAD. The most suspect curve, the curve-type distribution percentage, and combined dynamic data were compared. Statistical analysis included logistic regression analysis and receiver-operating characteristic analysis.\nFifty-one patients with 46 malignant and 44 benign lesions were enrolled. On receiver-operating characteristic analysis, the most suspect curve showed diagnostic accuracy of 76.7 +/- 5%. In comparison, the curve-type distribution percentage demonstrated accuracy of 80.2 +/- 4.9%. Combined dynamic data had the highest diagnostic accuracy (84.3 +/- 4.2%). These differences did not achieve statistical significance. With appropriate cutoff values, sensitivity and specificity, respectively, were found to be 80.4% and 72.7% for the most suspect curve, 76.1% and 83.6% for the curve-type distribution percentage, and 78.3% and 84.5% for both parameters.\"\nQuestion:\n\"Application of computer-aided diagnosis (CAD) in MR-mammography (MRM): do we really need whole lesion time curve distribution analysis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26606599": {
                "source": [
                    "\"To determine the relationship between injury severity surrogates and other patient factors with the development and severity of heterotopic ossification (HO) following open reduction internal fixation of acetabular fractures treated with a posterior approach.\nRetrospective review.\nAcademic level 1 trauma center.\nTwo hundred forty-one patients who were treated through a posterior approach with a minimum of 6-month radiographic follow-up were identified from an acetabular fracture database.\nNone.\nThe occurrence and severity (Brooker Grade III/IV) of HO 6 months postsurgery.\nLength of stay (LOS) in the intensive care unit (ICU), non-ICU LOS>10 days, and HO prophylaxis with external radiation beam therapy (XRT) were significantly associated with the development of HO in a multivariate model [\n1-2 days, odds ratio (OR) = 4.33, 95% confidence interval (CI): 1.03-18.25; 3-6 days, OR = 4.1, 95% CI, 1.27-13.27;>6 days, OR = 11.7, 95% CI, 3.24-42.22; non-ICU LOS>10 days (vs. 0-6 days): OR = 7.6, 95% CI, 2.6-22.25; XRT HO prophylaxis: OR = 0.29, 95% CI, 0.10-0.85]. Other variables evaluated in multivariate modeling not significantly associated with development and severity of HO included age, gender, mechanism of injury, injury severity score, presence of neurologic injury, Letournel fracture type, occurrence of hip dislocation, interval from injury to surgery, operative time, and estimated blood loss.\"\nQuestion:\n\"Do Surrogates of Injury Severity Influence the Occurrence of Heterotopic Ossification in Fractures of the Acetabulum?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16778275": {
                "source": [
                    "\"Pneumothorax following flexible bronchoscopy (FB) with transbronchial biopsy (TBB) occurs in 1 to 6% of cases. Routine chest radiography (CXR) following TBB is therefore requested by most pulmonologists in an attempt to detect complications, particularly pneumothorax. The objective of this study was to determine if routine CXR after bronchoscopy and TBB is necessary.\nThe study group included 350 consecutive patients who underwent FB with TBB at our institution between December 2001 and January 2004. Routine CXR was performed up to 2 h after the procedure in all cases. Additionally, the following information was recorded in all patients: sex, age, immune status, indication for bronchoscopy, total number of biopsies done, segment sampled, pulse oxygen saturation, and development of symptoms suggestive of pneumothorax.\nPneumothorax was diagnosed radiologically in 10 patients (2.9%). Seven patients had symptoms strongly suggestive of pneumothorax prior to CXR, including four patients with large (>10%) pneumothorax. The other three patients were asymptomatic, with only minimal pneumothorax (</= 10%), which resolved completely 24 to 48 h later.\"\nQuestion:\n\"Is routine chest radiography after transbronchial biopsy necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8847047": {
                "source": [
                    "\"The purpose of this study is to examine whether or not well differentiated (w-d) hepatocellular carcinoma (HCC) is indeed clinically early cancer.\nSeventy six patients with solitary small HCCs up to 3 cm in diameter, who underwent hepatectomy, were observed for at least 2 years for possible recurrence. These patients were divided into two groups: 10 patients with w-d HCCs (Edmondson and Steiner's grade I) and 66 patients with less differentiated (l-d) HCCs (Edmondson and Steiner's grade I-II, II-III, and III).\nThe histological analysis revealed that w-d HCCs had lower incidences of fibrous capsule formation (P<0.01), when compared to l-d HCCs. There were no significant differences in the incidence of intrahepatic metastasis, or portal vein invasion. In a resected specimen of w-d HCC, barium sulfate and gelatin were injected into portal vein and a transparent specimen was made. The transparent specimen showed that the portal vein in the tumor seemed to be intact. Microscopically, cancer cell infiltration into the fibrous frame of the portal tract was present. There were no significant differences in the disease free survival between the two groups. An analysis of tumor volume doubling time in recurrent foci suggested that minute cancerous foci had been present at the time of operation.\"\nQuestion:\n\"Prognosis of well differentiated small hepatocellular carcinoma--is well differentiated hepatocellular carcinoma clinically early cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10749257": {
                "source": [
                    "\"The United States Food and Drug Administration implemented federal regulations governing mammography under the Mammography Quality Standards Act (MQSA) of 1992. During 1995, its first year in implementation, we examined the impact of the MQSA on the quality of mammography in North Carolina.\nAll mammography facilities were inspected during 1993-1994, and again in 1995. Both inspections evaluated mean glandular radiation dose, phantom image evaluation, darkroom fog, and developer temperature. Two mammography health specialists employed by the North Carolina Division of Radiation Protection performed all inspections and collected and codified data.\nThe percentage of facilities that met quality standards increased from the first inspection to the second inspection. Phantom scores passing rate was 31.6% versus 78.2%; darkroom fog passing rate was 74.3% versus 88.5%; and temperature difference passing rate was 62.4% versus 86.9%.\"\nQuestion:\n\"Has the mammography quality standards act affected the mammography quality in North Carolina?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12221908": {
                "source": [
                    "\"The principal causes of morbidity and mortality during pregnancy in Mexico, are preeclampsia/eclampsia, obstetric hemorrhage and puerperium complications; this is, 62% of maternal deaths in last years. HELLP syndrome was observed between 5 to 25% of the mortality in pregnancies of 36 weeks or less.\nTo analyze patients with HELLP syndrome in ICU's (Intensive Care Unit) of a Gynecology and Obstetric Hospital, related to the abnormal hematological, hepatic and renal results with the obstetric case history and the clinical complications.\nA transversal study in patients with HELLP syndrome during 1998 and 1999 were carry out.\nPeripheral blood with Microangiopathic hemolysis, elevated liver enzymes: AST, ALT over 40 UI/L, even when were LDH lower than 600 UI/L. It was evaluated the hepatic and renal function, platelets count, microangiopathic hemolysis, arterial pressure, seizures, icteric skin color, blindness, visual disturbances, nausea, vomiting and upper quadrant right abdominal pain. In newborn we analyzed gestational age, sex, weight and APGAR. We studied for an association between maternal and biochemical variables with Correlation Pearson Test, and dependence between variables with lineal regression model.\n2878 patients with hypertensives disorders in pregnancy (11.64%). The 1.15% (n = 33) had HELLP syndrome with specific maternal mortality of 0.4 per 10,000 live birth, perinatal mortality of 1.62 per 10,000 live birth; and renal damage in 84.5%. Coefficient beta was higher between number of pregnancies to platelets count (-0.33) and creatinine clearance (-0.401).\"\nQuestion:\n\"The HELPP syndrome--evidence of a possible systemic inflammatory response in pre-eclampsia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19854401": {
                "source": [
                    "\"The purpose of this survey was to ascertain the most common surgical practices for attaining negative (tumor-free) surgical margins in patients desiring breast-conservation treatment for breast cancer to see if a consensus exists for optimal treatment of patients.\nWe sent a survey to 1,000 surgeons interested in the treatment of breast cancer. Three hundred eighty-one surgeons responded to this survey and 351 were used for the analysis (response rate of 38%).\nAnswers showed a large variety in clinical practices among breast surgeons across the country. There was little intraoperative margin analysis; only 48% of surgeons examine the margins grossly with a pathologist and even fewer used frozen sections or imprint cytology. Decisions to reexcise specific margins varied greatly. For example, 57% of surgeons would never reexcise for a positive deep margin, but 53% would always reexcise for a positive anterior margin. Most importantly, there was a large range in answers about acceptable margins with ductal carcinoma in situ and invasive carcinoma. Fifteen percent of surgeons would accept any negative margin, 28% would accept a 1-mm negative margin, 50% would accept a 2-mm negative margin, 12% would accept a 5-mm negative margin, and 3% would accept a 10-mm negative margin.\"\nQuestion:\n\"Attaining negative margins in breast-conservation operations: is there a consensus among breast surgeons?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20337202": {
                "source": [
                    "\"To determine the duration of continuing pregnancy after antenatal corticosteroid (AC) administration and to evaluate the potential opportunity for rescue AC.\nRetrospective analysis of women at 24-32 weeks' gestation who received AC at one institution.\nSix hundred ninety-two women received AC. Two hundred forty-seven (35.7%) delivered at>or = 34 weeks' gestation. Three hundred twenty-one (46.4%) delivered within 1 week of AC; 92 of those women (13.3%) delivered within 24 hours. Only 124 (17.9%) remained pregnant 1 week after AC and delivered at<34 weeks. The latter were compared to women delivering>2 week after AC but>or = 34 weeks. More likely to deliver at<34 weeks were those women who received AC for premature preterm rupture of membranes (OR 3.83, 95% CI 2.06-7.17), twins (OR 2.90, 95% CI 1.42-5.95) or before 28 weeks (OR 2.21, 95% CI 1.38-3.52).\"\nQuestion:\n\"Continuation of pregnancy after antenatal corticosteroid administration: opportunity for rescue?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18948835": {
                "source": [
                    "\"Neuromedin U (NmU) is a neuropeptide with anorexigenic activity. Two receptor subtypes (NmUR1 and NmUR2) confer the effects of NmU on target cells. We have recently demonstrated that NmU reduces insulin secretion from isolated pancreatic islets. Aim of our current study is to investigate the role of somatostatin at mediating the effects of NmU on insulin secretion.\nExpression of NmU in the pancreas was detected by immunohistochemistry. Insulin and somatostatin secretion from in situ perfused rat pancreas and isolated pancreatic islets was measured by radioimmunoassay. The paracrine effects of somatostatin within pancreatic islets were blocked by cyclosomatostatin, a somatostatin receptor antagonist.\nReceptor subtype NmUR1, but not NmUR2, was expressed in the endocrine pancreas, predominantly in the periphery. Neuromedin U reduced insulin secretion from in situ perfused rat pancreas and stimulated somatostatin secretion from isolated pancreatic islets. Neuromedin U stimulated somatostatin secretion at both physiological and supraphysiological glucose concentrations. Cyclosomatostatin increased insulin secretion and reduced NmU-induced inhibition of insulin secretion.\"\nQuestion:\n\"Does somatostatin confer insulinostatic effects of neuromedin u in the rat pancreas?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20828836": {
                "source": [
                    "\"To determine the perinatal predictors of discordant screening outcomes based on a two-stage screening protocol with transient-evoked otoacoustic emissions (TEOAE) and automated auditory brainstem response (AABR).\nA cross-sectional study of infants tested with TEOAE and AABR under a hospital-based universal newborn hearing screening program in Lagos, Nigeria. Maternal and infant factors associated with discordant TEOAE and AABR outcomes were determined with multivariable logistic regression analyses adjusting for potential confounding factors.\nOf the 4718 infants enrolled under the program 1745 (36.9%) completed both TEOAE and AABR. Of this group, 1060 (60.7%) passed both TEOAE and AABR (\"true-negatives\"); 92 (5.3%) failed both TEOAE and AABR (\"true-positive\"); 571 (32.7%) failed TEOAE but passed AABR (\"false-positives\") while 22 (1.3%) passed TEOAE but failed AABR (\"false-negatives\"). Infants with false-positives were likely to be admitted into well-baby nursery (p=0.001), belong to mothers who attended antenatal care (p=0.010) or who delivered vaginally (p<0.001) compared to infants with true-negatives while infants with true-positives were also more likely to be delivered vaginally (p=0.002) or admitted into well-baby nursery (p=0.035) compared to infants with false-negatives. Infants with true-positives were significantly more likely to be delivered vaginally (p<0.001) and have severe hyperbilirubinemia (p=0.045) compared with infants with true-negatives. No association was observed between false-negatives and true-negatives. Antenatal care status, mode of delivery and nursery type were useful predictors of discordant outcomes among all infants undergoing screening (c-statistic=0.73).\"\nQuestion:\n\"Is discordance in TEOAE and AABR outcomes predictable in newborns?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23222920": {
                "source": [
                    "\"If pancreas transplantation is a validated alternative for type 1 diabetic patients with end-stage renal disease, the management of patients who have lost their primary graft is poorly defined. This study aims at evaluating pancreas retransplantation outcome.\nBetween 1976 and 2008, 569 pancreas transplantations were performed in Lyon and Geneva, including 37 second transplantations. Second graft survival was compared with primary graft survival of the same patients and the whole population. Predictive factors of second graft survival were sought. Patient survival and impact on kidney graft function and survival were evaluated.\nSecond pancreas survival of the 17 patients transplanted from 1995 was close to primary graft survival of the whole population (71% vs. 79% at 1 year and 59% vs. 69% at 5 years; P=0.5075) and significantly better than their first pancreas survival (71% vs. 29% at 1 year and 59% vs. 7% at 5 years; P=0.0008) regardless of the cause of first pancreas loss. The same results were observed with all 37 retransplantations. Survival of second simultaneous pancreas and kidney transplantations was better than survival of second pancreas after kidney. Patient survival was excellent (89% at 5 years). Pancreas retransplantation had no impact on kidney graft function and survival (100% at 5 years).\"\nQuestion:\n\"Pancreas retransplantation:  a second chance for diabetic patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9100537": {
                "source": [
                    "\"Cytologic criteria reported to be helpful in the distinction of proliferative breast disease without atypia (PBD) from nonproliferative breast disease (NPBD) have not been rigorously tested.\nFifty-one air-dried, Diff-Quik-stained fine-needle aspirates (FNA) of palpable breast lesions with biopsy-proven diagnoses of NPBD (34 cases) or PBD (17 cases) were reviewed. The smears were evaluated for the cellularity, size, and architectural arrangement of the epithelial groups; the presence of single epithelial cells and myoepithelial cells; and nuclear characteristics.\nThe only cytologic feature found to be significantly different between PBD and NPBD was a swirling pattern of epithelial cells. A swirling pattern was noted in 13 of 17 PBD cases (76%) and 12 of 34 NPBD cases (35%) (P = 0.008).\"\nQuestion:\n\"Can nonproliferative breast disease and proliferative breast disease without atypia be distinguished by fine-needle aspiration cytology?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15919266": {
                "source": [
                    "\"The criteria for administration of adjuvant radiation therapy after thymoma resection remains controversial, and it is unclear whether patients with Masaoka stage III thymoma benefit from adjuvant radiation. The goal of this report was to determine whether or not this group benefits from radiation therapy in disease-specific survival and disease-free survival.\nCase records of the Massachusetts General Hospital were retrospectively reviewed from 1972 to 2004. One hundred and seventy-nine patients underwent resection for thymoma, of which 45 had stage III disease.\nForty-five stage III patients underwent resection and in 36 it was complete. Thirty-eight stage III patients received radiation therapy. Baseline prognostic factors between radiated and nonradiated groups were similar. The addition of adjuvant radiotherapy did not alter local or distant recurrence rates in patients with stage III thymoma. Disease-specific survival at 10 years in stage III patients who did not receive radiation was 75% (95% confidence interval, 32% to 100%) and in patients who did receive radiation therapy it was 79% (95% confidence interval, 64% to 94%) (p = 0.21). The most common site of relapse was the pleura.\"\nQuestion:\n\"Adjuvant radiation of stage III thymoma: is it necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25747932": {
                "source": [
                    "\"This paper uses a life-course approach to explore whether the timing and/or duration of urban (vs rural) exposure was associated with risk factors for NCDs.\nA cross-sectional survey was conducted among health care workers in two hospitals in Thailand. Two measures of urbanicity were considered: early-life urban exposure and the proportion of urban life years. We explored four behavioral NCD risk factors, two physiological risk factors and four biological risk factors.\nBoth measures of urbanicity were each independently associated with increases in all behavioral and physiological risk factors. For some biological risk factors, people spending their early life in an urban area may be more susceptible to the effect of increasing proportion of urban life years than those growing up in rural areas.\"\nQuestion:\n\"Living in an urban environment and non-communicable disease risk in Thailand: Does timing matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11978239": {
                "source": [
                    "\"To determine how often primary care physicians prescribe eradication therapy for peptic ulcer disease (PUD) and nonulcer dyspepsia (NUD).\nDuring a 2-year period (1998-2000) we analyzed data concerning patients with PUD or NUD seen by 80 Italian primary care physicians uniformly distributed throughout the country. We classified patients as having a definitive or a presumptive diagnosis on the basis of the completeness of the diagnostic workup and interpreted the prescription of antibiotics for dyspepsia as evidence of attempted eradication of Helicobacter pylori.\nConsecutive ambulatory patients.\nThe frequency with which predefined groups of patients received eradication therapy.\nOf 6866 patients, 690 (10%) received eradication therapy. Of 2162 patients with PUD, 596 (27.6%) received eradication therapy; of 4704 patients with NUD, however, only 94 (2%) received this treatment (P =.0001). A total of 341 (37.7%) of 904 PUD patients with a definitive diagnosis were given eradication therapy and 255 (20.3%) of 1258 PUD patients with a presumptive diagnosis were given therapy (P&lt.0001). In NUD patients, 7 of 743 (0.9%) with a definitive diagnosis received eradication therapy, while 87 (2.2%) of 3961 of those with a presumptive diagnosis were given the same therapy (P =.025).\"\nQuestion:\n\"Do primary care physicians underprescribe antibiotics for peptic ulcer disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19931500": {
                "source": [
                    "\"The aim of this study was to analyze the properties of the immune cell microenvironment of regional lymph nodes (LNs) positive for lung cancer.\nTwenty-four patients operated on for stages T1 and T2 of the NSCLC, were enrolled in the study. Peripheral blood and LN tissue were obtained from different lymph node sites and levels. As a control, LN tissue was taken from patients diagnosed with emphysema or pneumothorax. The cells from randomly chosen LN were tested by multi-color flow cytometry. Separate portions of LN were snap-frozen and examined for the presence of cytokeratin positive cells (CK). Propensity for apoptosis, level of TCR zeta chain expression of T cells and the number and maturation status of dendritic cells were confronted with the presence of CK-positive cells.\nThe presence of metastases correlated with the downregulation of TCR zeta, especially CD8(+) T cells. The most striking feature was the reduction in the number of myeloid CD11c(+) dendritic cells in the LN of patients with LN metastases. This could be a reflection of the immunodeficient state observed in lung cancer patients. Even in the absence of metastases in the regional LN, the same type of changes in the LN microenvironment were observed in those LN located nearer the primary tumor.\"\nQuestion:\n\"Can the condition of the cell microenvironment of mediastinal lymph nodes help predict the risk of metastases in non-small cell lung cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19822586": {
                "source": [
                    "\"Voluntary asphyxiation among children, preteens, and adolescents by hanging or other means of inducing hypoxia/anoxia to enhance sexual excitement is not uncommon and can lead to unintended death. This study addresses autoerotic asphyxiation (AEA) with the intent of increasing pediatricians' knowledge of the syndrome and awareness of its typical onset among young patients. AEA is characteristically a clandestine and elusive practice. Provided with relevant information, pediatricians can identify the syndrome, demonstrate a willingness to discuss concerns about it, ameliorate distress, and possibly prevent a tragedy.\nA retrospective study was undertaken of published cases both fatal and nonfatal and included personal communications, referenced citations, clinical experience, and theoretical formulations as to causation. Characteristic AEA manifestations, prevalence, age range, methods of inducing hypoxia/anoxia, and gender weighting are presented. All sources were used as a basis for additional considerations of etiology and possibilities for intervention.\nAEA can be conceptualized as a personalized, ritualized, and symbolic biopsychosocial drama. It seems to be a reenactment of intense emotional feeling-states involving an identification and sadomasochistic relationship with a female figure. Inept AEA practitioners can miscalculate the peril of the situation that they have contrived and for numerous reasons lose their gamble with death.\"\nQuestion:\n\"Autoerotic asphyxiation: secret pleasure--lethal outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24591144": {
                "source": [
                    "\"To determine if elderly patients with oropharyngeal squamous cell carcinoma (OPSCC) are receiving less treatment and to evaluate the benefit of aggressive therapy in this population.\nRetrospective analysis of a large population database.\nPatients in the Surveillance, Epidemiology, and End Results database with OPSCC diagnosed from 2004 to 2009 were included. The patients were categorized into age groups 45 to 54, 55 to 64, 65 to 74, 75 to 84, and 85 years and older, then further categorized by treatment status. Kaplan-Meier analysis of disease-specific survival (DSS) for late-stage (III and IV) OPSCC was performed for all age and treatment categories, followed by a multivariate cox regression of treatment status, tumor site, race, stage, and sex per age group.\nA total of 14,909 patients with OPSCC were identified. In our demographic data, we observed a significant increase in the number of patients who did not receive treatment (surgery, radiation, or combined therapy) after age 55. Kaplan-Meier analysis showed that age groups 65 to 74 and 75 to 84 had substantial benefits in DSS with surgery, radiation, or combined therapy. Multivariable analysis did not demonstrate any statistically significant difference in the hazard ratios for combined treatment among age groups 45 to 54, 55 to 64, 65 to 74, and 75 to 84.\"\nQuestion:\n\"Are the elderly with oropharyngeal carcinoma undertreated?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24939676": {
                "source": [
                    "\"The aim of this study was to investigate the influence of the pharmacokinetics of s.c. anti-TNF agents on the grade of US-detected synovitis in RA patients.\nFifty RA patients were prospectively recruited from the Biologic Therapy Unit of our hospital. Inclusion criteria were being in treatment with s.c. anti-TNF agents and having had neither changes in therapy nor local corticosteroid injections in the previous 3 months. Patients underwent clinical, laboratory [28-joint DAS (DAS28) and Simplified Disease Activity Index (SDAI)]and US assessment at two time points, i.e. at peak plasma drug concentration and at trough plasma drug concentration. US assessments were performed blindly to the anti-TNF agent, the administration time and the clinical and laboratory data. Twenty-eight joints were investigated for the presence and grade (0-3) of B-mode synovitis and synovial power Doppler signal. Global indices for B-mode synovitis (BSI) and Doppler synovitis (DSI) were calculated for 12 joints and for wrist-hand-ankle-foot joints. B-mode US remission was defined as a BSI<1 and Doppler US remission as a DSI<1.\nThere were no significant differences between the clinical, laboratory and B-mode and Doppler US parameters at peak time and trough time (P = 0.132-0.986). There were no significant differences between the proportion of patients with active disease and those in remission according to DAS28, SDAI, B-mode US and Doppler US at peak time and trough time assessments (P = 0.070-1).\"\nQuestion:\n\"Does ultrasound-scored synovitis depend on the pharmacokinetics of subcutaneous anti-TNF agents in patients with rheumatoid arthritis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21569408": {
                "source": [
                    "\"A growing body of research emphasizes the importance of contextual factors on health outcomes. Using postcode sector data for Scotland (UK), this study tests the hypothesis of spatial heterogeneity in the relationship between area-level deprivation and mortality to determine if contextual differences in the West vs. the rest of Scotland influence this relationship. Research into health inequalities frequently fails to recognise spatial heterogeneity in the deprivation-health relationship, assuming that global relationships apply uniformly across geographical areas. In this study, exploratory spatial data analysis methods are used to assess local patterns in deprivation and mortality. Spatial regression models are then implemented to examine the relationship between deprivation and mortality more formally.\nThe initial exploratory spatial data analysis reveals concentrations of high standardized mortality ratios (SMR) and deprivation (hotspots) in the West of Scotland and concentrations of low values (coldspots) for both variables in the rest of the country. The main spatial regression result is that deprivation is the only variable that is highly significantly correlated with all-cause mortality in all models. However, in contrast to the expected spatial heterogeneity in the deprivation-mortality relationship, this relation does not vary between regions in any of the models. This result is robust to a number of specifications, including weighting for population size, controlling for spatial autocorrelation and heteroskedasticity, assuming a non-linear relationship between mortality and socio-economic deprivation, separating the dependent variable into male and female SMRs, and distinguishing between West, North and Southeast regions. The rejection of the hypothesis of spatial heterogeneity in the relationship between socio-economic deprivation and mortality complements prior research on the stability of the deprivation-mortality relationship over time.\"\nQuestion:\n\"Does context matter for the relationship between deprivation and all-cause mortality?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20401819": {
                "source": [
                    "\"Ultrasound is currently not established for the diagnosis of fractures. The aim of this study was to compare ultrasound and X-ray beyond their use solely for the identification of fractures, i. e., for the detection of fracture type and dislocation for pediatric fracture diagnosis.\nLimb bones of dead young pigs served as a model for pediatric bones. The fractured bones were examined with ultrasound, X-ray, and CT, which served as the gold standard.\n162 of 248 bones were fractured. 130 fractures were identified using ultrasound, and 148 using X-ray. There were some advantages of X-ray over ultrasound in the detection of fracture type (80 correct results using X-ray, 66 correct results using ultrasound). Ultrasound, however, was superior to X-ray for dislocation identification (41 correct results using X-ray, 51 correct results using ultrasound). Both findings were not statistically significant after adjustment for multiple testing.\"\nQuestion:\n\"Is ultrasound equal to X-ray in pediatric fracture diagnosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24625433": {
                "source": [
                    "\"Noise exposure in the neonatal intensive care unit is believed to be a risk factor for hearing loss in preterm neonates. Continuous positive airway pressure (CPAP) devices exceed recommended noise levels. High flow nasal cannulae (HFNC) are an increasingly popular alternative to CPAP for treating preterm infants, but there are no in vivo studies assessing noise production by HFNC.\nTo study whether HFNC are noisier than bubble CPAP (BCPAP) for preterm infants.\nAn observational study of preterm infants receiving HFNC or BCPAP. Noise levels within the external auditory meatus (EAM) were measured using a microphone probe tube connected to a calibrated digital dosimeter. Noise was measured across a range of frequencies and reported as decibels A-weighted (dBA).\nA total of 21 HFNC and 13 BCPAP noise measurements were performed in 21 infants. HFNC gas flows were 2-5 L/min, and BCPAP gas flows were 6-10 L/min with set pressures of 5-7 cm of water. There was no evidence of a difference in average noise levels measured at the EAM: mean difference (95% CI) of -1.6 (-4.0 to 0.9) dBA for HFNC compared to BCPAP. At low frequency (500 Hz), HFNC was mean (95% CI) 3.0 (0.3 to 5.7) dBA quieter than BCPAP. Noise increased with increasing BCPAP gas flow (p=0.007), but not with increasing set pressure. There was a trend to noise increasing with increasing HFNC gas flows.\"\nQuestion:\n\"Are high flow nasal cannulae noisier than bubble CPAP for preterm infants?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17096624": {
                "source": [
                    "\"To examine patterns of knowledge and attitudes among adults aged>65 years unvaccinated for influenza.\nSurveyed Medicare beneficiaries in 5 areas; clustered unvaccinated seniors by their immunization related knowledge and attitudes.\nIdentified 4 clusters: Potentials (45%) would receive influenza vaccine to prevent disease; Fearful Uninformeds (9%) were unsure if influenza vaccine causes illness; Doubters (27%) were unsure if vaccine is efficacious; Misinformeds (19%) believed influenza vaccine causes illness. More Potentials (75%) and Misinformeds (70%) ever received influenza vaccine than did Fearful Uninformeds (18%) and Doubters (29%).\"\nQuestion:\n\"Do patterns of knowledge and attitudes exist among unvaccinated seniors?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22350859": {
                "source": [
                    "\"The objective of this study was to determine the most effective content of pictorial health warning labels (HWLs) and whether educational attainment moderates these effects.\nField experiments were conducted with 529 adult smokers and 530 young adults (258 nonsmokers; 271 smokers). Participants reported responses to different pictorial HWLs printed on cigarette packages. One experiment involved manipulating textual form (testimonial narrative vs. didactic) and the other involved manipulating image type (diseased organs vs. human suffering).\nTests of mean ratings and rankings indicated that pictorial HWLs with didactic textual forms had equivalent or significantly higher credibility, relevance, and impact than pictorial HWLs with testimonial forms. Results from mixed-effects models confirmed these results. However, responses differed by participant educational attainment: didactic forms were consistently rated higher than testimonials among participants with higher education, whereas the difference between didactic and testimonial narrative forms was weaker or not statistically significant among participants with lower education. In the second experiment, with textual content held constant, greater credibility, relevance, and impact was found for graphic imagery of diseased organs than imagery of human suffering.\"\nQuestion:\n\"Can pictorial warning labels on cigarette packages address smoking-related health disparities?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20629769": {
                "source": [
                    "\"The National Infarct Angioplasty Project assessed the feasibility of establishing a comprehensive primary angioplasty service. We aimed to compare satisfaction at intervention hospitals offering angioplasty-based care and control hospitals offering thrombolysis-based care.\nMixed methods, with postal survey of patients and their carers, supported by semi-structured interviews.\nSurvey of 682 patients and 486 carers, and interviews with 33 patients and carers, in eight English hospitals.\nPrimary angioplasty or thrombolysis.\nSatisfaction with treatment.\nResponses were received from 595/682 patients (87%) and 418/486 carers (86%). Satisfaction with overall care was high at both intervention and control sites (78% vs. 71% patients rated their care as 'excellent', P = 0.074). Patient satisfaction was higher at intervention sites for some aspects of care such as speed of treatment (80% vs. 67%'excellent', P = 0.001). Convenience of visiting was rated lower at intervention sites by carers (12% vs. 1%'poor', P = 0.001). During interviews, carers reported that they accepted the added inconvenience of visiting primary angioplasty sites in the context of this life-saving treatment. Patient satisfaction with discharge and aftercare was lower in both treatment groups than for other aspects of care.\"\nQuestion:\n\"Is primary angioplasty an acceptable alternative to thrombolysis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20538207": {
                "source": [
                    "\"It is generally considered that kidney grafts should be preserved at 4 degrees C during cold storage. However, actual temperature conditions are not known. We decided to study the temperature levels during preservation with the Biotainer storage can and Vitalpack transport pack.\nTemperature was monitored using the Thermobouton probe during preservation of pig kidneys, in the same conditions used with human grafts. The probe recorded the temperature level every 10 minutes during four days. We compared the results found with the new storage can with results obtained in the same conditions with the storage can formerly used by our team. We also studied the best position of the probe for temperature monitoring and the influence of the amount of ice within the transport pack on the temperature level. We then monitored the temperature during the conservation of actual human kidney grafts harvested at our institution from August 2007 to May 2008.\nThe temperature levels were the same regardless of the position of the probe within the transport pack. The lowest temperature was maintained during 15 hours, and the temperature level stayed below 5 degrees C for 57 hours with the new storage can. The former storage can maintained the lowest temperature level for 80 minutes, and temperature reached 5 degrees C after 10 hours 40 minutes. Temperature levels were similar when 2 or 4 kg of crushed ice were used. We observed similar results when monitoring the conservation of human grafts.\"\nQuestion:\n\"Should temperature be monitorized during kidney allograft preservation?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18319270": {
                "source": [
                    "\"IVF singletons have poorer perinatal outcomes than singletons from spontaneous conceptions. This may be due to the influence of ovarian stimulation on the chromosomal constitution of the embryos which could be translated into localized chromosomal anomalies in the placenta. The aim of this study was to compare the incidence of confined placental mosaicism (CPM) in IVF/ICSI pregnancies and spontaneous conceptions.\nWe conducted a multi-centre retrospective analysis of karyotype results obtained by chorionic villus sampling (CVS), performed due to advanced maternal age (>or=36 years at 18 weeks of gestation), in the Netherlands between 1995 and 2005.\nFrom a total of 322 246 pregnancies, 20 885 CVS results were analysed: 235 in the IVF/ICSI group and 20 650 in the control group. The mean age of women in both groups was 38.4 years (mean difference -0.08, 95% CI -0.35 to 0.18). Data relating to the fetal karyotype were missing in 143 cases in the control group. When taking into account missing data, the incidence of CPM was lower in the IVF-ICSI group than in the control group, 1.3% versus 2.2% (odds ratio 0.59, 95% CI 0.19-1.85), whereas the incidence of fetal chromosomal anomalies was increased 4.3% versus 2.4% (odds ratio 1.81, 95% CI 0.95-3.42). Neither differences were statistically significant.\"\nQuestion:\n\"Does confined placental mosaicism account for adverse perinatal outcomes in IVF pregnancies?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "7482275": {
                "source": [
                    "\"The accepted treatment protocol for necrotizing fasciitis (NF) consists of extensive surgery and wide spectrum antibiotics. Hyperbaric oxygenation (HBO) has been recommended as adjuvant therapy for NF, improving patient mortality and outcome. However, the beneficial effect of HBO for NF remains controversial.\nA retrospective evaluation of treatment outcome in 37 patients treated for NF between 1984 and 1993 was carried out. The mortality rate, morbidity criteria, and risk factors for grave prognosis were compared between a group of 25 patients who received HBO as part of their treatment protocol and a group of the remaining 12 patients treated by surgical excision and antibiotics alone.\nThe two groups were found to be similar with regard to age, gender, the incidence of individual risk factors for ominous prognosis, and the Acute Physiology and Chronic Health Evaluation (APACHE) II score for disease's severity on presentation. The mortality rate among the HBO-treated patients was 36%, as opposed to 25% in the non-HBO group. The mean number of surgical d\u00e9bridements required per patient was significantly higher in the HBO group: 3.3 compared with 1.5 in the non-HBO-treated patients. Although the average length of hospitalization for survivors was shorter for the HBO group, the difference between the groups did not reach statistical significance.\"\nQuestion:\n\"Necrotizing fasciitis: an indication for hyperbaric oxygenation therapy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24793469": {
                "source": [
                    "\"Multiple sclerosis (MS) is the most common chronic autoimmune demyelinating disease of the central nervous system. The purpose of this study is to determine the relationship between the site of the cervical discopathy and cervical spinal cord plaque in MS patients.\nThis retrospective study included all patients with a definite diagnosis of MS who were treated at an outpatient clinic between September 2004 and September 2011. All patients underwent cervical magnetic resonance imaging (MRI) for primary investigation of the disease. Cervical MRI scans were evaluated for detection of any evidence of cervical discopathy and cervical MS plaques. Any correlation between the site of the MS lesions and discopathy was recorded.\nFrom 536 patients who were involved in the study, 214 patients had both cervical discopathy and cervical cord plaques. In this group 148 (69.1% of patients) had cervical plaque at the same site of cervical discopathy. The number of patients with cervical cord plaque and discopathy at same site was significantly higher than those with plaque and discopathy at different sites (P<0.05).\"\nQuestion:\n\"Is there any relation between cervical cord plaques and discopathy in patients with multiple sclerosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23337545": {
                "source": [
                    "\"Acute fibrinous and organizing pneumonia (AFOP) is a recently described histologic pattern of diffuse pulmonary disease. In children, all cases reported to date have been fatal. In this study, we describe the first nonfatal AFOP in a child and review the literature.\nA 10-year-old boy developed very severe aplastic anemia (VSAA) after being admitted to our hospital with a fulminant hepatic failure of unknown origin. A chest computed tomography scan revealed multiple lung nodules and a biopsy of a pulmonary lesion showed all the signs of AFOP. Infectious workup remained negative. We started immunosuppressive therapy with antithymocyte globulin and cyclosporine to treat VSAA. Subsequent chest computed tomography scans showed a considerable diminution of the lung lesions but the VSAA did not improve until we performed hematopoietic stem cell transplantation 5 months later.\"\nQuestion:\n\"Is acute fibrinous and organizing pneumonia the expression of immune dysregulation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26471488": {
                "source": [
                    "\"Limited and conflicting data exist on an association between mammographic density (MD) and re-excision rates after breast-conserving surgery (BCS). Additionally, the correlation of MD with resection of unnecessary margins during initial BCS is unknown.\nAll women with a diagnosis of breast cancer from 2003 to 2012 and enrolled in a larger study on MD were evaluated. Operative and pathology reports were reviewed to determine margin resection and involvement. Mammographic density was determined both by breast imaging-reporting and data system (BI-RADS) classification and by an automated software program (Volpara Solutions). Additional margins were deemed unnecessary if the lumpectomy specimen margin was free of invasive tumor [\u22652 mm for ductal carcinoma in situ (DCIS)] or if further re-excision was needed.\nOf 655 patients, 398 (60.8%) had BCS, whereas 226 (34.5%) underwent initial mastectomy. The women with denser breasts (BI-RADS 3 or 4) underwent initial mastectomy more frequently than the women with less dense breasts (40.0 vs. 30.5%, respectively; p = 0.0118). Of the patients with BCS, 166 (41.7%) required separate re-excision. Additional margins were taken during BCS in 192 (48.2%) patients, with 151 (78.6%) proving to be unnecessary. In the bivariable analysis, the patients with denser breasts according to BI-RADS classification and volumetric density showed a trend toward requiring more frequent re-excision, but this association was not seen in the multivariable analysis. The rate of unnecessary margins did not differ by breast density. In the multivariate analysis, the re-excision rates increased with DCIS (p<0.0003) and decreased with resection of additional margins (p = 0.0043).\"\nQuestion:\n\"Does Mammographic Density have an Impact on the Margin Re-excision Rate After Breast-Conserving Surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20121683": {
                "source": [
                    "\"Community-based medical education is growing to meet the increased demand for quality clinical education in expanded settings, and its sustainability relies on patient participation. This study investigated patients' views on being used as an educational resource for teaching medical students.\nQuestionnaire-based survey.\nPatients attending six rural and 11 regional general practices in New South Wales over 18 teaching sessions in November 2008, who consented to student involvement in their consultation.\nPatient perceptions, expectations and acceptance of medical student involvement in consultations, assessed by surveys before and after their consultations.\n118 of 122 patients consented to medical student involvement; of these, 117 (99%) completed a survey before the consultation, and 100 (85%) after the consultation. Patients were overwhelmingly positive about their doctor and practice being involved in student teaching and felt they themselves played an important role. Pre-consultation, patients expressed reluctance to allow students to conduct some or all aspects of the consultation independently. However, after the consultation, they reported they would have accepted higher levels of involvement than actually occurred.\"\nQuestion:\n\"Are patients willing participants in the new wave of community-based medical education in regional and rural Australia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24751724": {
                "source": [
                    "\"Ageing is a growing issue for people from UK black, Asian and minority ethnic (BAME) groups. The health experiences of these groups are recognised as a 'tracer' to measure success in end of life patient-preferred outcomes that includes place of death (PoD).AIM: To examine patterns in PoD among BAME groups who died of cancer.\nMortality data for 93,375 cancer deaths of those aged \u226565 years in London from 2001-2010 were obtained from the UK Office for National Statistics (ONS). Decedent's country of birth was used as a proxy for ethnicity. Linear regression examined trends in place of death across the eight ethnic groups and Poisson regression examined the association between country of birth and place of death.\n76% decedents were born in the UK, followed by Ireland (5.9%), Europe(5.4%) and Caribbean(4.3%). Most deaths(52.5%) occurred in hospital, followed by home(18.7%). During the study period, deaths in hospital declined with an increase in home deaths; trend for time analysis for those born in UK(0.50%/yr[0.36-0.64%]p<0.001), Europe (1.00%/yr[0.64-1.30%]p<0.001), Asia(1.09%/yr[0.94-1.20%]p<0.001) and Caribbean(1.03%/yr[0.72-1.30%]p<0.001). However, time consistent gaps across the geographical groups remained. Following adjustment hospital deaths were more likely for those born in Asia(Proportion ratio(PR)1.12[95%CI1.08-1.15]p<0.001) and Africa(PR 1.11[95%CI1.07-1.16]p<0.001). Hospice deaths were less likely for those born in Asia(PR 0.73 [0.68-0.80] p<0.001), Africa (PR 0.83[95%CI0.74-0.93]p<0.001), and 'other' geographical regions (PR0.90[95% 0.82-0.98]p<0.001). Home deaths were less likely for those born in the Caribbean(PR0.91[95%CI 0.85-0.98]p<0.001).\"\nQuestion:\n\"Does ethnicity affect where people with cancer die?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14518645": {
                "source": [
                    "\"Deaths from injury and poisoning (suicide, accidents, undetermined deaths, and homicide) are the major cause of death among young men aged 15-39 years in England and Wales and have been increasing in recent years.AIM: To describe common characteristics among young men who die from injury and poisoning.\nWe employed a retrospective survey methodology to investigate factors associated with deaths by injury and poisoning among young men aged 15-39 years (n = 268) in Merseyside and Cheshire during 1995. Data were collected from Coroner's inquest notes and General Practitioner records.\nThe most common cause of death was poisoning by alcohol and drugs (29.1%, n = 78). A high proportion of cases were unemployed (39.4%, n = 106). Cases were also more likely to be single compared to the general population (74.2% vs 55.5%). Self-destructive behaviour was evident in 77% of deaths (n = 206).\"\nQuestion:\n\"Injury and poisoning mortality among young men--are there any common factors amenable to prevention?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18926458": {
                "source": [
                    "\"Several prospective randomized trials have proved carotid endarterectomy to be safe and effective for both symptomatic and asymptomatic patients younger than 80 years of age. Recently, carotid artery stenting (CAS) has been approved for use in selected high-risk patients. It has been proposed that being an octogenarian places patients in this high-risk category.\nAll patients between the ages of 80 to 89 years undergoing carotid endarterectomy during a 12-year period were included in the study. Information included indications for carotid endarterectomy, associated risk factors, length of stay, and hospital course. Perioperative morbidity and mortality, including neurologic events and myocardial infarction, were recorded.\nA total of 103 carotid endarterectomies were performed in 95 octogenarians. Procedures were performed on 59 men and 36 women. Indications for operation included symptomatic carotid stenosis in 44 patients (43%) and asymptomatic carotid stenosis in 59 (57%). Associated risk factors included diabetes mellitus (17%), hypertension (76%), coronary artery disease (28%), hyperlipidemia (39%), and history of smoking (42%). There were 4 perioperative neurologic complications, which included 1 transient ischemic attack (0.97%), 2 minor strokes (1.94%), and 1 major stroke (0.97%). There were no deaths.\"\nQuestion:\n\"Are octogenarians at high risk for carotid endarterectomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24074624": {
                "source": [
                    "\"Some pediatric patients, typically those that are very young or felt to be especially sick are temporarily admitted to the intensive care unit (ICU) for observation during their first transfusion. If a significant reaction that requires ICU management does not occur, these patients are then transferred to a regular ward where future blood products are administered. The aim of this project was to determine if heightened observation such as temporary ICU admissions for the first transfusion are warranted.\nFrom the blood bank records of a tertiary care pediatric hospital, a list of patients on whom a transfusion reaction was reported between 2007 and 2012, the type of reaction and the patient's transfusion history, were extracted. The hospital location where the transfusion occurred, and whether the patient was evaluated by the ICU team or transferred to the ICU for management of the reaction was determined from the patient's electronic medical record.\nThere were 174 acute reactions in 150 patients. Of these 150 patients, 13 (8.7%) different patients experienced a reaction during their first transfusion; all 13 patients experienced clinically mild reactions (8 febrile non-hemolytic, 4 mild allergic, and 1 patient who simultaneously had a mild allergic and a febrile non-hemolytic), and none required ICU management. Six severe reactions (6 of 174, 3.4%) involving significant hypotension and/or hypoxia that required acute and intensive management occurred during subsequent (i.e. not the first) transfusion in six patients.\"\nQuestion:\n\"Is intensive monitoring during the first transfusion in pediatric patients necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22955530": {
                "source": [
                    "\"The range of injury severity that can be seen within the category of type II supracondylar humerus fractures (SCHFs) raises the question whether some could be treated nonoperatively. However, the clinical difficulty in using this approach lies in determining which type II SCHFs can be managed successfully without a surgical intervention.\nWe reviewed clinical and radiographic information on 259 pediatric type II SCHFs that were enrolled in a prospective registry of elbow fractures. The characteristics of the patients who were treated without surgery were compared with those of patients who were treated surgically. Treatment outcomes, as assessed by the final clinical and radiographic alignment, range of motion of the elbow, and complications, were compared between the groups to define clinical and radiographic features that related to success or failure of nonoperative management.\nDuring the course of treatment, 39 fractures were found to have unsatisfactory alignment with nonoperative management and were taken for surgery. Ultimately, 150 fractures (57.9%) were treated nonoperatively, and 109 fractures (42.1%) were treated surgically. At final follow-up, outcome measures of change in carrying angle, range of motion, and complications did not show clinically significant differences between treatment groups. Fractures without rotational deformity or coronal angulation and with a shaft-condylar angle of>15 degrees were more likely to be associated with successful nonsurgical treatment. A scoring system was developed using these features to stratify the severity of the injury. Patients with isolated extension deformity, but none of the other features, were more likely to complete successful nonoperative management.\"\nQuestion:\n\"Type II supracondylar humerus fractures: can some be treated nonoperatively?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19054501": {
                "source": [
                    "\"Studies have shown that schizophrenia patients have motion perception deficit, which was thought to cause eye-tracking abnormality in schizophrenia. However, eye movement closely interacts with motion perception. The known eye-tracking difficulties in schizophrenia patients may interact with their motion perception.\nTwo speed discrimination experiments were conducted in a within-subject design. In experiment 1, the stimulus duration was 150 msec to minimize the chance of eye-tracking occurrence. In experiment 2, the duration was increased to 300 msec, increasing the possibility of eye movement intrusion. Regular eye-tracking performance was evaluated in a third experiment.\nAt 150 msec, speed discrimination thresholds did not differ between schizophrenia patients (n = 38) and control subjects (n = 33). At 300 msec, patients had significantly higher thresholds than control subjects (p = .03). Furthermore, frequencies of eye tracking during the 300 msec stimulus were significantly correlated with speed discrimination in control subjects (p = .01) but not in patients, suggesting that eye-tracking initiation may benefit control subjects but not patients. The frequency of eye tracking during speed discrimination was not significantly related to regular eye-tracking performance.\"\nQuestion:\n\"Is motion perception deficit in schizophrenia a consequence of eye-tracking abnormality?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19852337": {
                "source": [
                    "\"Peripheral venous thrombophlebitis (PVT) is a common complication of intravenous cannulation, occurring in about 30% of patients. We evaluated the effect of elective re-siting of intravenous cannulae every 48 hours on the incidence and severity of PVT in patients receiving intravenous fluids/drugs.\nWe randomized 42 patients who were admitted for major abdominal surgery to either the control or study group (n = 21 in either group). Informed consent was obtained from all of them. Cannulae in the control group were removed only if the site became painful, the cannula got dislodged or there were signs and symptoms suggestive of PVT, namely pain, erythema, swelling, excessive warmth or a palpable venous cord. Cannulae in the study group were changed and re-sited electively every 48 hours. All the patients were examined every 24 hours for signs and symptoms of PVT at the current and previous sites of infusion.\nThe incidence of PVT was 100% (21/21) in the control group and only 9.5% (2/21) in the study group (p<0.0001). The severity of PVT was also less in the study group compared with that in the control group. Day-wise correlation of the incidence of PVT showed that 82.6% of the episodes of PVT occurred on day 3.\"\nQuestion:\n\"Does elective re-siting of intravenous cannulae decrease peripheral thrombophlebitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21756515": {
                "source": [
                    "\"Medical units at an academic tertiary referral hospital in Southern India.\nTo investigate the impact of solid culture on L\u00f6wenstein-Jensen medium on clinical decision making.\nIn a retrospective review of 150 culture-positive and 150 culture-negative consecutively sampled tuberculosis (TB) suspects, treatment decisions were analysed at presentation, after the availability of culture detection results and after the availability of drug susceptibility testing (DST) culture results.\nA total of 124 (82.7%) culture-positive patients and 35 (23.3%) culture-negative patients started anti-tuberculosis treatment prior to receiving their culture results; 101 patients (33.7%) returned for their results; two (1.3%) initiated treatment based on positive culture and no culture-negative patients discontinued treatment. DST was performed on 119 (79.3%) positive cultures: 30 (25.2%) showed any resistance, eight (6.7%) showed multidrug resistance and one (0.84%) showed extensively drug-resistant TB. Twenty-eight patients (23.5%) returned for their DST results. Based on DST, treatment was modified in four patients (3.4%).\"\nQuestion:\n\"Does solid culture for tuberculosis influence clinical decision making in India?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11438275": {
                "source": [
                    "\"It is generally believed that positioning of the patient in a head-down tilt (Trendelenberg position) decreases the likelihood of a venous air embolism during liver resection.\nThe physiological effect of variation in horizontal attitude on central and hepatic venous pressure was measured in 10 patients during liver surgery. Hemodynamic indices were recorded with the operating table in the horizontal, 20 degrees head-up and 20 degrees head-down positions.\nThere was no demonstrable pressure gradient between the hepatic and central venous levels in any of the positions. The absolute pressures did, however, vary in a predictable way, being highest in the head-down and lowest during head-up tilt. However, on no occasion was a negative intraluminal pressure recorded.\"\nQuestion:\n\"Does patient position during liver surgery influence the risk of venous air embolism?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9569972": {
                "source": [
                    "\"To investigate whether the S + G2/M fraction (proliferative index) is a prognostic determinant in breast cancers classified as Auer IV.\nPrognostic evaluation of Auer IV DNA histograms with respect to the high versus low S + G2/M fraction, obtained by image cytometry on consecutive breast cancer imprint preparations.\nWhen studying recurrence-free survival (n = 136), the prognostic value of S + G2/M was found to vary with time: it was negligible before the median time to relapse (1.5 years) but thereafter statistically significant, in both univariate and multivariate analysis. The same pattern was found when overall survival was used as the end point; the effect was delayed to about the median time until death (three years). Tumors with a low S + G2/M fraction were smaller and more often estrogen receptor- and progesterone receptor-positive than those with a high S + G2/M fraction.\"\nQuestion:\n\"Proliferative index obtained by DNA image cytometry. Does it add prognostic information in Auer IV breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17462393": {
                "source": [
                    "\"Beating-heart valve surgery appears to be a promising technique for protection of hypertrophied hearts. Normothermic normokalemic simultaneous antegrade/retrograde perfusion (NNSP) may improve myocardial perfusion. However, its effects on myocardial oxygenation and energy metabolism remain unclear. The present study was to determine whether NNSP improved myocardial oxygenation and energy metabolism of hypertrophied hearts relative to normothermic normokalemic antegrade perfusion (NNAP).\nTwelve hypertrophied pig hearts underwent a protocol consisting of three 20-minute perfusion episodes (10 minutes NNAP and 10 minutes NNSP in a random order) with each conducted at a different blood flow in the left anterior descending coronary artery (LAD [100%, 50%, and 20% of its initial control]). Myocardial oxygenation was assessed using near-infrared spectroscopic imaging. Myocardial energy metabolism was monitored using localized phosphorus-31 magnetic resonance spectroscopy.\nWith 100% LAD flow, both NNAP and NNSP maintained myocardial oxygenation, adenosine triphosphate, phosphocreatine, and inorganic phosphate at normal levels. When LAD flow was reduced to 50% of its control level, NNSP resulted in a small but significant decrease in myocardial oxygenation and phosphocreatine, whereas those measurements did not change significantly during NNAP. With LAD flow further reduced to 20% of its control level, both NNAP and NNSP caused a substantial decrease in myocardial oxygenation, adenosine triphosphate, and phosphocreatine with an increase in inorganic phosphate. However, the changes were significantly greater during NNSP than during NNAP.\"\nQuestion:\n\"Does normothermic normokalemic simultaneous antegrade/retrograde perfusion improve myocardial oxygenation and energy metabolism for hypertrophied hearts?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23497210": {
                "source": [
                    "\"Although record linkage of routinely collected health datasets is a valuable research resource, most datasets are established for administrative purposes and not for health outcomes research. In order for meaningful results to be extrapolated to specific populations, the limitations of the data and linkage methodology need to be investigated and clarified. It is the objective of this study to investigate the differences in ascertainment which may arise between a hospital admission dataset and a dispensing claims dataset, using major depression in pregnancy as an example. The safe use of antidepressants in pregnancy is an ongoing issue for clinicians with around 10% of pregnant women suffer from depression. As the birth admission will be the first admission to hospital during their pregnancy for most women, their use of antidepressants, or their depressive condition, may not be revealed to the attending hospital clinicians. This may result in adverse outcomes for the mother and infant.\nPopulation-based de-identified data were provided from the Western Australian Data Linkage System linking the administrative health records of women with a delivery to related records from the Midwives' Notification System, the Hospital Morbidity Data System and the national Pharmaceutical Benefits Scheme dataset. The women with depression during their pregnancy were ascertained in two ways: women with dispensing records relating to dispensed antidepressant medicines with an WHO ATC code to the 3rd level, pharmacological subgroup, 'N06A Antidepressants'; and, women with any hospital admission during pregnancy, including the birth admission, if a comorbidity was recorded relating to depression.\nFrom 2002 to 2005, there were 96698 births in WA. At least one antidepressant was dispensed to 4485 (4.6%) pregnant women. There were 3010 (3.1%) women with a comorbidity related to depression recorded on their delivery admission, or other admission to hospital during pregnancy. There were a total of 7495 pregnancies identified by either set of records. Using data linkage, we determined that these records represented 6596 individual pregnancies. Only 899 pregnancies were found in both groups (13.6% of all cases). 80% of women dispensed an antidepressant did not have depression recorded as a comorbidity on their hospital records. A simple capture-recapture calculation suggests the prevalence of depression in this population of pregnant women to be around 16%.\"\nQuestion:\n\"Are women with major depression in pregnancy identifiable in population health data?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12913878": {
                "source": [
                    "\"Nd:YAG laser-induced thermo therapy (LITT) of rat brains is associated with blood-brain barrier (BBB) permeability changes. We address the question of whether LITT-induced locoregional disruption of the BBB could possibly allow a locoregional passage of chemotherapeutic agents into brain tissue to treat malignant glioma.STUDY DESIGN/\nCD Fischer rats were subject to LITT of the left forebrain. Disruption of the BBB was analyzed using Evans blue and immunohistochemistry (IH). Animals were perfused with paclitaxel, and high-pressure liquid chromatography (HPLC) was employed to analyze the content of paclitaxel in brain and plasma samples.\nLITT induces an opening of the BBB as demonstrated by locoregional extravasation of Evans blue, C3C, fibrinogen, and IgM. HPLC proved the passage of paclitaxel across the disrupted BBB.\"\nQuestion:\n\"Locoregional opening of the rodent blood-brain barrier for paclitaxel using Nd:YAG laser-induced thermo therapy: a new concept of adjuvant glioma therapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18928979": {
                "source": [
                    "\"The objective of the study was to determine whether myometrial electrical activity can differentiate false from true preterm labor.\nElectrical uterine myography (EUM) was measured prospectively on 87 women, gestational age less than 35 weeks. The period between contractions, power of contraction peaks and movement of center of electrical activity (RMS), was used to develop an index score (1-5) for prediction of preterm delivery (PTD) within 14 days of the test. The score was compared with fetal fibronectin (fFN) and cervical length (CL).\nPatients delivering within 14 days from testing showed a higher index and mean RMS (P = .000). No patients with EUM index scores of 1-2 delivered in this time frame. Combining EUM with CL or fFN increased predictability. Logistic regression revealed that history of PTD and EUM index had 4- to 5-fold increased risk for PTD. Gestational age at testing, body mass index, fFN, and CL were nonsignificant contributors to PTD risk.\"\nQuestion:\n\"Can myometrial electrical activity identify patients in preterm labor?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15000338": {
                "source": [
                    "\"To determine the cost of 46 commonly used investigations and therapies and to assess British Columbia family doctors' awareness of these costs.\nMailed survey asking about costs of 23 investigations and 23 therapies relevant to family practice. A random sample of 600 doctors was asked to report their awareness of costs and to estimate costs of the 46 items.\nBritish Columbia.\nSix hundred family physicians.\nEstimates within 25% of actual cost were considered correct. Associations between cost awareness and respondents'characteristics (eg, sex, practice location) were sought. Degree of error in estimates was also assessed.\nOverall, 283 (47.2%) surveys were returned and 259 analyzed. Few respondents estimated costs within 25% of true cost, and estimates were highly variable. Physicians underestimated costs of expensive drugs and laboratory investigations and overestimated costs of inexpensive drugs. Cost awareness did not correlate with sex, practice location, College certification, faculty appointment, or years in practice.\"\nQuestion:\n\"Do family physicians know the costs of medical care?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23735520": {
                "source": [
                    "\"To determine the potential prognostic value of using functional magnetic resonance imaging (fMRI) to identify patients with disorders of consciousness, who show potential for recovery.\nObservational study.\nUnit for acute rehabilitation care.\nPatients (N=22) in a vegetative state (VS; n=10) and minimally conscious state (MCS; n=12) during the first 200 days after the initial incident.\nNot applicable.\nFurther course on the Coma Recovery Scale-Revised.\nParticipants performed a mental imagery fMRI paradigm. They were asked to alternately imagine playing tennis and navigating through their home. In 14 of the 22 examined patients (VS, n=5; MCS, n=9), a significant activation of the regions of interest (ROIs) of the mental imagery paradigm could be found. All 5 patients with activation of a significant blood oxygen level dependent signal, who were in a VS at the time of the fMRI examination, reached at least an MCS at the end of the observation period. In contrast, 5 participants in a VS who failed to show activation in ROIs, did not (sensitivity 100%, specificity 100%). Six of 9 patients in an MCS with activation in ROIs emerged from an MCS. Of 3 patients in an MCS who did not show activation, 2 patients stayed in an MCS and 1 patient emerged from the MCS (sensitivity 85%, specificity 40%).\"\nQuestion:\n\"Can mental imagery functional magnetic resonance imaging predict recovery in patients with disorders of consciousness?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26044262": {
                "source": [
                    "\"Rates of active travel vary by socio-economic position, with higher rates generally observed among less affluent populations. Aspects of both social and built environments have been shown to affect active travel, but little research has explored the influence of physical environmental characteristics, and less has examined whether physical environment affects socio-economic inequality in active travel. This study explored income-related differences in active travel in relation to multiple physical environmental characteristics including air pollution, climate and levels of green space, in urban areas across England. We hypothesised that any gradient in the relationship between income and active travel would be least pronounced in the least physically environmentally-deprived areas where higher income populations may be more likely to choose active transport as a means of travel.\nAdults aged 16+ living in urban areas (n\u2009=\u200920,146) were selected from the 2002 and 2003 waves of the UK National Travel Survey. The mode of all short non-recreational trips undertaken by the sample was identified (n\u2009=\u2009205,673). Three-level binary logistic regression models were used to explore how associations between the trip being active (by bike/walking) and three income groups, varied by level of multiple physical environmental deprivation.\nLikelihood of making an active trip among the lowest income group appeared unaffected by physical environmental deprivation; 15.4% of their non-recreational trips were active in both the least and most environmentally-deprived areas. The income-related gradient in making active trips remained steep in the least environmentally-deprived areas because those in the highest income groups were markedly less likely to choose active travel when physical environment was 'good', compared to those on the lowest incomes (OR\u2009=\u20090.44, 95% CI\u2009=\u20090.22 to 0.89).\"\nQuestion:\n\"Are income-related differences in active travel associated with physical environmental characteristics?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25725704": {
                "source": [
                    "\"Clinical supervision is widely recognised as a mechanism for providing professional support, professional development and clinical governance for healthcare workers. There have been limited studies about the effectiveness of clinical supervision for allied health and minimal studies conducted within the Australian health context. The aim of the present study was to identify whether clinical supervision was perceived to be effective by allied health professionals and to identify components that contributed to effectiveness. Participants completed an anonymous online questionnaire, administered through the health service's intranet.\nA cross-sectional study was conducted with community allied health workers (n = 82) 8 months after implementation of structured clinical supervision. Demographic data (age, gender), work-related history (profession employment level, years of experience), and supervision practice (number and length of supervision sessions) were collected through an online survey. The outcome measure, clinical supervision effectiveness, was operationalised using the Manchester Clinical Supervision Scale-26 (MCSS-26). Data were analysed with Pearson correlation (r) and independent sample t-tests (t) with significance set at 0.05 (ie the probability of significant difference set at P<0.05).\nThe length of the supervision sessions (r(s) \u2265 0.44), the number of sessions (r(s) \u2265 0.35) and the total period supervision had been received (r(s) \u2265 0.42) were all significantly positively correlated with the MCSS-26 domains of clinical supervision effectiveness. Three individual variables, namely 'receiving clinical supervision', 'having some choice in the allocation of clinical supervisor' and 'having a completed clinical supervision agreement', were also significantly associated with higher total MCSS-26 scores (P(s)<0.014).\"\nQuestion:\n\"Can clinical supervision sustain our workforce in the current healthcare landscape?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21979183": {
                "source": [
                    "\"Nobody has analyzed the sequelae of desmoids according to the type of surgery that precipitated them.\nThis study aims to determine whether the clinical effects of abdominal desmoids would be worse in patients with restorative proctocolectomy than in patients with ileorectal anastomosis.\nThis is a retrospective, database study.\nIncluded were patients with familial adenomatous polyposis who had undergone proctocolectomy with IPAA or colectomy and ileorectal anastomosis, and subsequently developed an intra-abdominal desmoid tumor.\nThe primary outcome measures were the clinical course of the desmoids; morbidity, and the requirement for stoma.\nThere were 86 patients: 49 had restorative proctocolectomy and 37 had ileorectal anastomosis. Patient demographics were similar. Average follow-up was 9.8 years (range, 2.7-23.8) and 16.3 years (range, 2.3 - 42.9). Treatment of the desmoids included surgery (64.4% vs 65.6%), medical therapy (69.4% vs 59.5%), chemotherapy (36.2% vs 30.0%), and radiotherapy (4.5% vs 10.0%), and was the same for each group. The overall complication rate of desmoids was similar, approaching 70%. The risk of individual complications was also similar (bleeding (2.0% vs 0.0%), fistula (10.2% vs 13.5%), bowel obstruction (32.7% vs 48.6%), pain (34.7% vs 21.6%), and death related to desmoid tumors (2.0% vs 10.8%)); 38.8% of the restorative proctocolectomy group and 51.4% the ileorectal group had surgery for desmoid tumor complications (P = .21), and 22.4% and 22.2% of patients ultimately had permanent stomas.\nThis study was limited by the relatively small numbers of patients.\"\nQuestion:\n\"Does intra-abdominal desmoid disease affect patients with an ileal pouch differently than those with an ileorectal anastomosis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26505821": {
                "source": [
                    "\"The levels of bone formation and resorption can be assessed at the tissue level by bone histomorphometry on transiliac bone biopsies. Systemic biochemical markers of bone turnover reflect the overall bone formation and resorption at the level of the entire skeleton but cannot discriminate the different skeletal compartments.\nOur aim was to investigate the correlations between the serum biochemical markers of formation and resorption with histomorphometric parameters.\nWe performed post hoc analysis of a previous clinical study.\nPatients were selected from the general population.\nA total of 371 untreated postmenopausal osteoporotic women aged 50 to 84 years with a lumbar T-score \u2264 -2.5 SD or \u2264 -1 SD with at least one osteoporotic fracture.\nTransiliac bone biopsies were obtained after a double tetracycline labeling, and blood samples were collected.\nThe static and dynamic parameters of formation and bone resorption were measured by histomorphometry. Serum biochemical markers of formation (bone alkaline phosphatase [ALP]; procollagen type I N-terminal propeptide [PINP]) and resorption (C-terminal crosslinking telopeptide of collagen type 1 [sCTX]) were assessed.\nThe mean values of biochemical markers were: bone ALP, 15.0 \u00b1 5.2 ng/mL; PINP, 56.2 \u00b1 21.9 \u03bcg/mL; and sCTX, 0.58 \u00b1 0.26 ng/mL. Bone ALP and PINP were significantly correlated with both the static and dynamic parameters of formation (0.21 \u2264 r' \u2264 0.36; 0.01 \u2265 P \u2265 .0001). sCTX was significantly correlated with all resorption parameters (0.18 \u2264 r' \u2264 0.24; 0.02 \u2265 P \u2265 .0001).\"\nQuestion:\n\"Are Biochemical Markers of Bone Turnover Representative of Bone Histomorphometry in 370 Postmenopausal Women?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16678696": {
                "source": [
                    "\"It is widely accepted that exemplary surgical care involves a surgeon's involvement in the preoperative, perioperative, and postoperative periods. In an era of ever-expanding therapeutic modalities available to the vascular surgeon, it is important that trainees gain experience in preoperative decision-making and how this affects a patient's operative and postoperative course. The purpose of this study was to define the current experience of residents on a vascular surgery service regarding the continuity of care they are able to provide for patients and the factors affecting this experience.\nThis prospective cohort study was approved by the Institutional Review Board and conducted at the University of British Columbia during January 2005. All patients who underwent a vascular procedure at either of the two teaching hospitals were included. In addition to type of case (emergent, outpatient, inpatient), resident demographic data and involvement in each patient's care (preoperative assessment, postoperative daily assessment, and follow-up clinic assessment) were recorded. Categoric data were analyzed with the chi2 test.\nThe study included 159 cases, of which 65% were elective same-day admission patients, 20% were elective previously admitted patients; and 15% were emergent. The overall rate of preoperative assessment was 67%, involvement in the decision to operate, 17%; postoperative assessment on the ward, 79%; and patient follow-up in clinic, 3%. The rate of complete in-hospital continuity of care (assessing patient pre-op and post-op) was 57%. Emergent cases were associated with a significantly higher rate of preoperative assessment (92% vs 63%, P<.05). For elective cases admitted before the day of surgery compared with same-day admission patients, the rates of preoperative assessment (78% vs 58%, P<.05) and involvement in the decision to operate (16% vs 4%, P<.05) were significantly higher.\"\nQuestion:\n\"Continuity of care experience of residents in an academic vascular department: are trainees learning complete surgical care?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21789019": {
                "source": [
                    "\"The increasingly older population confronts oncologists with an imposing challenge: older cancer patients have specific healthcare needs both independent of and associated with the diagnosis of cancer. The aim of the present study is to examine whether elderly versus younger cancer patients have different needs with respect to attendance, treatment and information.\nThis is an observational and cross-sectional study. Cancer patients aged 35 to 82 years were consecutively interviewed. The group was divided into two subgroups aged \u226465 and \u226566 years old. The Needs Evaluation Questionnaire (NEQ) was used to assess patients' needs and demographic variables were collected. Data analysis was carried out by means of cross-tabulation analyses and the chi-square test.\nThe requests most frequently expressed by the older group concerned financial-insurance information (73.9%), the need to talk to people with the same illness (71.7%), the need to receive more comprehensible information from doctors and nurses (71.7%), and the need for a better dialogue with clinicians (69.6%). Few significant differences between the two age subgroups were found, with the exception of issues such as the need for intimacy and support.\"\nQuestion:\n\"Do elderly cancer patients have different care needs compared with younger ones?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24684514": {
                "source": [
                    "\"Optimization of the preoperative hemoglobin (Hb) level is an effective way to reduce allogeneic transfusion in total knee arthroplasty (TKA) though the procedure is expensive, requires close monitoring and is often inconvenient for patients with reduced mobility. Our aim was to investigate the value of preoperative Hb levels to predict transfusion and thereby tailoring Hb optimization to patient characteristics.\nAll consecutive patients who undergone primary TKA in our center over 2\u00a0years, and received tranexamic acid intraoperatively, were reviewed. The adjusted association between preoperative Hb levels and transfusion was assessed by multivariate logistic regression, and the estimated probability of transfusion for individual patients was derived from the logistic model.\nOut of the 784 patients who meet the inclusion criteria, risk of transfusion was associated with poorer performance status, as measured by the America Association of Anestesiology (ASA) score III/IV (OR: 3\u00b73, P\u00a0<\u00a00\u00b7001) and lower preoperative Hb level (OR 3\u00b78 for each g/dl below 13\u00a0g/dl; P\u00a0<\u00a00\u00b7001). According to the Hb level, the estimated probability of transfusion was 0\u00b703 (range: 0\u00b703-0\u00b764) for ASA I/II patients and 0\u00b710 (range: 0\u00b710-0\u00b784) for ASA III/IV.\"\nQuestion:\n\"Should all patients be optimized to the same preoperative hemoglobin level to avoid transfusion in primary knee arthroplasty?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25406780": {
                "source": [
                    "\"Incontinence-associated dermatitis (IAD) is a potentially serious skin injury that can lead to pressure ulcers (PUs). Multiple studies have indicated the need for evidence to find the most effective skin care protocol to reduce the incidence and severity of IAD in critically ill patients.\nTo compare the incidence and severity of IAD in two groups on a progressive care unit (PCU) using a defined skin care protocol: cleaning with a gentle cleanser and moisturizer, then applying a skin protectant/barrier. The control group received the skin care protocol every 12 hours and the interventional group received the protocol every 6 hours; both groups also received it as needed.\nA 9-month randomized prospective study was conducted on 99 patients (N = 55 in the intervention group and N = 44 in the control group) who were incontinent of urine, stool, or both, or had a fecal diversion device or urinary catheter for more than 2 days.\nThe dermatitis score in the intervention group on discharge was significantly less (7.1%; P \u2264 0.001) in the moderate IAD group than in the control group (10.9%). The dermatitis score means and P values of each group were compared using a paired t test.\"\nQuestion:\n\"Does skin care frequency affect the severity of incontinence-associated dermatitis in critically ill patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15918864": {
                "source": [
                    "\"Little is known about how information needs change over time in the early postpartum period or about how these needs might differ given socioeconomic circumstances. This study's aim was to examine women's concerns at the time of hospital discharge and unmet learning needs as self-identified at 4 weeks after discharge.\nData were collected as part of a cross-sectional survey of postpartum health outcomes, service use, and costs of care in the first 4 weeks after postpartum hospital discharge. Recruitment of 250 women was conducted from each of 5 hospitals in Ontario, Canada (n = 1,250). Women who had given vaginal birth to a single live infant, and who were being discharged at the same time as their infant, assuming care of their infant, competent to give consent, and able to communicate in one of the study languages were eligible. Participants completed a self-report questionnaire in hospital; 890 (71.2%) took part in a structured telephone interview 4 weeks after hospital discharge.\nApproximately 17 percent of participants were of low socioeconomic status. Breastfeeding and signs of infant illness were the most frequently identified concerns by women, regardless of their socioeconomic status. Signs of infant illness and infant care/behavior were the main unmet learning needs. Although few differences in identified concerns were evident, women of low socioeconomic status were significantly more likely to report unmet learning needs related to 9 of 10 topics compared with women of higher socioeconomic status. For most topics, significantly more women of both groups identified learning needs 4 weeks after discharge compared with the number who identified corresponding concerns while in hospital.\"\nQuestion:\n\"Learning needs of postpartum women: does socioeconomic status matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16100194": {
                "source": [
                    "\"Angiotensin-converting enzyme inhibitors (ACE-I) are considered safe, but they are associated with characteristic side effects, namely cough and angioedema, usually requiring discontinuation. We perceived that referrals for these side effects have become more and more frequent; therefore, we evaluated the degree of knowledge on the safety of ACE-I in different medical categories.\nA questionnaire (13 questions) on side effects of ACE-I was posted to physicians.\nEveryday clinical practice.\nCardiologists, allergists, and general practitioners (GPs) from the National Healthcare System.\nThree hundred twelve physicians were contacted, and 154 returned questionnaires that could be analyzed. Of the 154 physicians (mean age, 45 years) 48 were cardiologists, 52 were GPs, and 54 were allergists. The percentage of correct answers was low: 31.9% for cardiologists, 40% for GPs, and 33% for allergists. Thus, GPs provided a significantly higher percentage of correct answers with respect to the remaining categories (p = 0.05). The lower rate of correct answers (0 to 15.9%) concerned the time of onset of cough and the action to take. Cardiologists seemed to be less aware of the fact that angiotensin receptor blockers (sartans) can cross-react with ACE-I.\"\nQuestion:\n\"Are physicians aware of the side effects of angiotensin-converting enzyme inhibitors?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23453079": {
                "source": [
                    "\"To determine whether prostate morphology or technique used has any effect on postoperative outcomes after holmium laser enucleation of the prostate.\nA retrospective review of prospectively collected data was completed for all patients undergoing a holmium laser enucleation of the prostate at our institution. Prostate morphology was classified as either \"bilobar\" or \"trilobar\" according to the cystoscopic appearance. The baseline characteristics, complications, and postoperative outcomes were collected.\nA total of 304 patients with either \"bilobar\" (n\u00a0= 142) or \"trilobar\" (n\u00a0= 162) prostate morphology were included. The trilobar group was more likely to have longer operative times (112 vs 100 minutes, P\u00a0= .04), although this difference was not significant on multivariate analysis. The postoperative outcomes were similar between the 2 groups for American Urological Association symptom score, change in American Urological Association symptom score, bother score, maximal flow rate, change in maximal flow rate, postvoid residual urine volume, and complication rate. However, the trilobar group had a significantly greater decrease in their PVR urine volume (296 vs 176 mL, P\u00a0= .01), a difference that persisted on multivariate analysis. A subset analysis of the trilobar prostates revealed that performing a 2-lobe technique achieved shorter operative and enucleation times, although the difference was not significant.\"\nQuestion:\n\"Does prostate morphology affect outcomes after holmium laser enucleation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12094116": {
                "source": [
                    "\"The purpose of this study was to identify the relationships between leg muscle power and sprinting speed with changes of direction.\nthe study was designed to describe relationships between physical qualities and a component of sports performance.\ntesting was conducted in an indoor sports hall and a biomechanics laboratory.\n15 male participants were required to be free of injury and have recent experience competing in sports involving sprints with changes of direction.\nsubjects were timed in 8 m sprints in a straight line and with various changes of direction. They were also tested for bilateral and unilateral leg extensor muscle concentric power output by an isokinetic squat and reactive strength by a drop jump.\nThe correlations between concentric power and straight sprinting speed were non-significant whereas the relationships between reactive strength and straight speed were statistically significant. Correlations between muscle power and speed while changing direction were generally low and non-significant for concentric leg power with some moderate and significant (p<0.05) coefficients found for reactive strength. The participants who turned faster to one side tended to have a reactive strength dominance in the leg responsible for the push-off action.\"\nQuestion:\n\"Is muscle power related to running speed with changes of direction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15539888": {
                "source": [
                    "\"The atopy patch test (APT), namely the patch test with aeroallergens, is regarded as specific for patients with atopic dermatitis (AD), but small numbers of positive APT were reported in the past also in atopic subjects without dermatitis and in healthy persons.\nThe aim of this study was to evaluate the response to the APT with house dust mites (HDM) in subjects nonaffected by AD and to compare the outcomes observed in these cases with those pointed out in AD patients, evaluating also the differences between two allergen extracts manufactured at different purifications and concentrations.\nForty-seven atopic subjects without eczema (AWE), 33 nonatopic (NA) subjects and 77 adult AD patients were patch tested with an extract of purified bodies of HDM at 20% and with another extract of whole bodies of HDM at 30%, the latter corresponding to 300 microg/g of Der p 1. The reproducibility of APT was also tested in 8 AD patients, in 37 AWE subjects and in 19 NA subjects.\nPositive responses with extract at 20% were observed in 29 (37.7%) AD, in 5 (10.6%) AWE and in 4 (12.1%) NA subjects. The APT with HDM at 30% was positive in 32 (41.6%) AD, 9 (19.1%) AWE and 4 (12.1%) NA persons. The rates of positivity and the intensity scores of responses were significantly different between AD and non-AD subjects (p<0.01). The reproducibility of the APT in the three groups was satisfactory.\"\nQuestion:\n\"Is the atopy patch test with house dust mites specific for atopic dermatitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15095519": {
                "source": [
                    "\"The purpose of this study was to determine if registered dietitian (RD) and registered nurse (RN) certified diabetes educators (CDEs) provide similar recommendations regarding carbohydrates and dietary supplements to individuals with diabetes.\nA survey was mailed to CDEs in the southern United States. Participants were asked to indicate their recommendations for use of carbohydrates, fiber, artificial sweeteners, and 12 selected dietary and herbal supplements when counseling individuals with diabetes.\nThe survey sample consisted of 366 CDEs: 207 were RNs and 159 were RDs. No statistically significant differences were found between RNs and RDs in typical carbohydrate recommendations for treatment of diabetes. However, RDs were more likely than RNs to make recommendations for fiber intake or use of the glycemic index. A significant difference also was found in the treatment of hypoglycemia: RNs were more likely than RDs to recommend consuming a carbohydrate source with protein to treat hypoglycemia.\"\nQuestion:\n\"Are patients with diabetes receiving the same message from dietitians and nurses?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16816043": {
                "source": [
                    "\"To determine under what conditions lay people and health professionals find it acceptable for a physician to breach confidentiality to protect the wife of a patient with a sexually transmitted disease (STD).\nIn a study in France, breaching confidentiality in 48 scenarios were accepted by 144 lay people, 10 psychologists and 7 physicians. The scenarios were all possible combinations of five factors: severity of the disease (severe, lethal); time taken to discuss this with (little time, much time); intent to inform the spouse about the disease (none, one of these days, immediately); intent to adopt protective behaviours (no intent, intent); and decision to consult an expert in STDs (yes, no), 2 x 2 x 3 x 2 x 2. The importance and interactions of each factor were determined, at the group level, by performing analyses of variance and constructing graphs.\nThe concept of breaching confidentiality to protect a wife from her husband's STD was favoured much more by lay people and psychologists than by physicians (mean ratings 11.76, 9.28 and 2.90, respectively, on a scale of 0-22). The patient's stated intentions to protect his wife and to inform her of the disease had the greatest impact on acceptability. A cluster analysis showed groups of lay participants who found breaching confidentiality \"always acceptable\" (n = 14), \"depending on the many circumstances\" (n = 87), requiring \"consultation with an expert\" (n = 30) and \"never acceptable (n = 13)\".\"\nQuestion:\n\"Do French lay people and health professionals find it acceptable to breach confidentiality to protect a patient's wife from a sexually transmitted disease?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22867778": {
                "source": [
                    "\"Health services often spend more on safety interventions than seems cost-effective. This study investigates whether the public value safety-related health care improvements more highly than the same improvements in contexts where the health care system is not responsible.\nAn online survey was conducted to elicit the relative importance placed on preventing harms caused by 1) health care (hospital-acquired infections, drug administration errors, injuries to health care staff), 2) individuals (personal lifestyle choices, sports-related injuries), and 3) nature (genetic disorders). Direct valuations were obtained from members of the public by using a person trade-off or \"matching\" method. Participants were asked to choose between two preventative interventions of equal cost and equal health benefit per person for the same number of people, but differing in causation. If participants indicated a preference, their strength of preference was measured by using person trade-off.\nResponses were obtained from 1030 people, reflecting the sociodemographic mix of the UK population. Participants valued interventions preventing hospital-acquired infections (1.31) more highly than genetic disorders (1.0), although drug errors were valued similarly to genetic disorders (1.07), and interventions to prevent injury to health care staff were given less weight than genetic disorders (0.71). Less weight was also given to interventions related to lifestyle (0.65) and sports injuries (0.41).\"\nQuestion:\n\"Does responsibility affect the public's valuation of health care interventions?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21593045": {
                "source": [
                    "\"Women with ovaries of polycystic morphology (PCO), without any other features of polycystic ovary syndrome (PCOS), respond similarly to women with PCOS when stimulated with exogenous gonadotrophins, and both groups share various endocrinological disturbances underlying their pathology. In women with PCOS, metformin co-treatment during IVF has been shown to increase pregnancy rates and reduce the risk of ovarian hyperstimulation syndrome (OHSS). The aim of this study was to investigate whether metformin co-treatment before and during IVF can also increase the live birth rate (LBR) and lower severe OHSS rates for women with PCO, but no other manifestations of PCOS.\nThis study was a double-blind, multi-centre, randomized, placebo-controlled trial. The study population included 134 women with ovulatory PCO (and no evidence of clinical or biochemical hyperandrogenism) undergoing IVF treatment at three tertiary referral IVF units. The primary outcome was LBR.\nIn total, 134 women were randomized, 69 to metformin and 65 to placebo. There were no statistically significant differences between the two groups in baseline characteristics. With regard to IVF outcome, no significant improvements were found in the metformin group when compared with the placebo group. In particular, there was no difference between the groups in rates of live birth [metformin n = 27 (39.1%), placebo n = 30 (46.2), (95% confidence interval 0.38, 1.49, odds ratio = 0.75)], clinical pregnancy [metformin n = 29 (42.0%), placebo n = 33 (50.8%)]or severe OHSS [metformin n = 6 (8.7%), placebo n = 5 (7.7%)].\"\nQuestion:\n\"Do women with ovaries of polycystic morphology without any other features of PCOS benefit from short-term metformin co-treatment during IVF?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11481172": {
                "source": [
                    "\"The authors sought to determine whether the manic/mixed episode distinction in patients with bipolar disorder runs true over time.\nOver an 11-year period, the observed distribution of manic and mixed episodes (N=1,224) for patients with three or more entries in the management information system of a community mental health center (N=241) was compared to the expected distribution determined by averaging 1,000 randomly generated simulations.\nEpisodes were consistent (all manic or all mixed) in significantly more patients than would be expected by chance.\"\nQuestion:\n\"Does the manic/mixed episode distinction in bipolar disorder patients run true over time?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17312514": {
                "source": [
                    "\"Seroma is the most frequent complication in abdominoplasty. Some patients are more prone to develop this complication. Ultrasound is a well-known method with which to diagnose seroma in the abdominal wall. The purpose of this study was to verify the efficacy of the use of quilting suture to prevent seroma.\nTwenty-one female patients who presented with abdominal deformity type III/A according to the authors' classification of abdominal skin and myoaponeurotic deformity had undergone abdominoplasty. The selected patients should have had at least one of the following characteristics: body mass index greater than 25 kg/m; weight loss greater than 10 kg; previous incision in the supraumbilical region; or present thinning of the subcutaneous in the area above the umbilicus. Ultrasound was performed for every patient from 15 to 18 days after the operation to search for fluid collection in the abdominal wall.\nThe average fluid collection found was 8.2 cc per patient. Only two patients underwent aspiration because ultrasound showed greater than 20 cc collected above the fascial layer. These patients did not present with recurrence of seroma after aspiration.\"\nQuestion:\n\"Does quilting suture prevent seroma in abdominoplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19100463": {
                "source": [
                    "\"Tacrolimus is a potent immunosuppressive drug used in organ transplantation. Because of its substantial toxic effects, narrow therapeutic index, and interindividual pharmacokinetic variability, therapeutic drug monitoring of whole-blood tacrolimus concentrations has been recommended. We investigated the comparability of the results of 2 immunoassay systems, affinity column-mediated immunoassay (ACMIA) and microparticle enzyme immunoassay (MEIA), comparing differences in the tacrolimus concentrations measured by the 2 methods in relation to the hematologic and biochemical values of hepatic and renal functions.\nA total of 154 samples from kidney or liver transplant recipients were subjected to Dimension RxL HM with a tacrolimus Flex reagent cartilage for the ACMIA method and IMx tacrolimus II for the MEIA method.\nTacrolimus concentrations measured by the ACMIA method (n = 154) closely correlated with those measured by the MEIA method (r = 0.84). The Bland-Altman plot using concentration differences between the 2 methods and the average of the 2 methods showed no specific trends. The tacrolimus levels determined by both the MEIA method and the ACMIA method were not influenced by hematocrit levels, but the difference between the 2 methods (ACMIA - MEIA) tended to be larger in low hematocrit samples (P<.001).\"\nQuestion:\n\"Is the affinity column-mediated immunoassay method suitable as an alternative to the microparticle enzyme immunoassay method as a blood tacrolimus assay?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21342862": {
                "source": [
                    "\"To evaluate the construct validity of the Turkish version of the EQ-5D in patients with acute coronary syndrome.\nThe study was conducted as a cross-sectional study at the Trakya University Hospital between February and May 2008. All patients completed the Turkish version of the EQ-5D and MacNew heart-related quality of life scale. Construct validity of the EQ-5D was assessed according to relationships with MacNew subscales by using Spearman rank correlation and multiple linear regression analyses.\nOne hundred and twenty-two patients responded to the instruments. Mean age was 62.9\u00b19.3 years and male gender (88 or 72.1%) was dominant. Mean score of the EQ-5D index was 0.79\u00b10.32, while the global score of MacNew was 5.01\u00b11.16. The correlation coefficients of the EQ-5D index score with the MacNew subscales ranged from 0.557 to 0.721, with EQ-5D VAS score ranging from 0.297 to 0.484 (p<0.001 for all of them). According to the stepwise regression model MacNew global score was found to be significantly effective factor on EQ-5D index score (\u03b2 =0.188; 95% CI: 0.152-0.224; p<0.001).\"\nQuestion:\n\"Is EQ-5D a valid quality of life instrument in patients with acute coronary syndrome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26518378": {
                "source": [
                    "\"Academic medical researchers are judged by how often their publications are cited in the literature. When serving as journal reviewers, they may be more favorably disposed to manuscripts that cite their work. We investigate whether manuscripts that contain a citation to the reviewer's work receive higher evaluations than those that do not and\u00a0whether peer reviewers encourage authors to cite that reviewer's work.\nWe analyzed all research manuscripts submitted in 2012 to Annals of Emergency Medicine to determine whether they contained citations to each reviewer's work. To determine whether citation affected reviewer scores, we obtained each reviewer's score of the manuscript's overall desirability (1=worst to 5=best) and used descriptive statistics and regression modeling to compare scores of cited and noncited reviewers. We also enumerated how often reviewers suggested that authors add citations to the reviewer's work or other work.\nThere were 395 manuscripts and 999 corresponding reviews with an manuscript desirability score. The 83 reviews by cited reviewers (8.3%) had a mean score of 2.8 (SD 1.4); the 916 reviews by noncited reviewers (91.7%), 2.5 (1.2; \u0394=0.3; 95% confidence interval [CI] 0 to 0.6). The mean score in the 117 reviews of the noncited reviewers of\u00a0the\u00a057\u00a0manuscripts that had both cited and noncited reviewers was 2.9 (SD 1.2) compared with 2.9 (SD 1.1) for the 68 reviews by cited reviewers (\u0394=0; 95% CI -0.3 to 0.4). In the final ordinal regression model, the unadjusted OR for\u00a0the manuscript desirability score was 1.6 (95% CI 1.0 to 2.7); when adjusting for the manuscripts' mean desirability score, it was 1.4 (95% CI 0.8 to\u00a02.2), demonstrating that manuscript quality was a confounder. Authors were asked to\u00a0add a citation to the reviewer's work in 28 reviews (3%) but to others' work in 98 (10%).\"\nQuestion:\n\"Are Reviewers' Scores Influenced by Citations to Their Own Work?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24851767": {
                "source": [
                    "\"The optimal age at which to perform orchiopexy for cryptorchidism has long been debated. The aim of this study was to determine if age at orchiopexy affected testicular atrophy.\nA retrospective review of patients undergoing orchiopexy from 2000 to 2010 was conducted. An individual testis, rather than patient, was used as the dependent variable. A total of 349 testicles from 1126 charts (ICD-9=752.51) were identified. Primary study outcome was testicular survival without atrophy.\nMean follow up for the study was 25 months. There was postoperative atrophy in 27 testes (7.7%). Intraabdominal testicle was independently associated with increased postsurgical atrophy (p<0.0001). The odds of postsurgical atrophy were 15.66 times higher for an abdominal vs. inguinal location (95% CI: 5.5-44.6). Testicular atrophy was highest for orchiopexy at ages 13-24 months (n=16 of 133, 12%) vs. those less than 13 months (n=3 of 64, 5%), and those greater than 24 months (n=8 of 152, 5%) (p=0.0024). After adjusting for location, age was not statistically significant with postsurgical atrophy (p=0.055).\"\nQuestion:\n\"Undescended testes: does age at orchiopexy affect survival of the testis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26708803": {
                "source": [
                    "\"Treatment of neonatal testicular torsion has two objectives: salvage of the involved testicle (which is rarely achieved) and preservation of the contralateral gonad. The second goal universally involves contralateral testicular scrotal fixation to prevent the future occurrence of contralateral torsion. However, there is controversy with regards to management of a synchronous contralateral hydrocele. It has been our policy not to address the contralateral hydrocele through an inguinal incision to minimize potential injury to the spermatic cord. Our objective in this study was to determine whether the decision to manage a contralateral hydrocele in cases of neonatal testicular torsion solely through a scrotal approach is safe and effective.\nWe reviewed all cases of neonatal testicular torsion occurring at our institution between the years 1999 and 2006. Age at presentation, physical examination, ultrasonographic and intraoperative findings were recorded. Patients were followed after initial surgical intervention to determine the likelihood of developing a subsequent hydrocele or hernia.\nThirty-seven patients were identified as presenting with neonatal torsion. Age of presentation averaged 3.5 days (range 1-14 days). Left-sided pathology was seen more commonly than the right, with a 25:12 distribution. All torsed testicles were nonviable. Twenty-two patients were noted to have a contralateral hydrocele at presentation. All hydroceles were opened through a scrotal approach at the time of contralateral scrotal fixation. No patient underwent an inguinal exploration to examine for a patent process vaginalis. None of the patients who presented with a hydrocele have developed a clinical hydrocele or hernia after an average 7.5 years (range 4.3-11.2) follow-up.\"\nQuestion:\n\"Treatment of contralateral hydrocele in neonatal testicular torsion: Is less more?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26194560": {
                "source": [
                    "\"The incidence of acetabular fractures in osteoporotic patients is increasing. Immediate total hip arthroplasty (THA) has potential advantages, but achieving acetabular component stability is challenging and, at early followup, reported revision rates for loosening are high.QUESTIONS/\nThis study measured acetabular component stability and the initial surface contact achieved between the acetabular component and unfractured region of the pelvis after THA using an oversized acetabular component and cup-cage reconstruction.\nBetween November 2011 and November 2013, we treated 40 acute acetabular fractures in patients older than 70 years of age. Of these, 12 (30%) underwent immediate THA using an oversized acetabular component with screws inserted only into the ilium and a cup-cage construct. Postoperatively all patients were mobilized without weightbearing restrictions. Indications for immediate THA after acetabular fractures were displaced articular comminution deemed unreducible. Eleven of the 12 were prospectively studied to evaluate the initial stability of the reconstructions using radiostereometric analysis. One of the patients died of a pulmonary embolism after surgery, and the remaining 10 (median age, 81 years; range, 72-86 years) were studied. Of these, five were analyzed at 1 year and five were analyzed at 2 years. Acetabular component migration was defined as acceptable if less than the limits for primary THA that predict later loosening (1.76 mm of proximal migration and 2.53\u00b0 of sagittal rotation). The contact surface between the acetabular component and ilium in direct continuity with the sacroiliac joint, and the ischium and pubis in direct continuity with the symphysis pubis, was measured on postoperative CT scans.\nAt 1 year the median proximal migration was 0.83 mm (range, 0.09-5.13 mm) and sagittal rotation was 1.3\u00b0 (range, 0.1\u00b0-7.4\u00b0). Three of the 10 components had migration above the suggested limits for primary THA at 1 year postoperatively. The contact surface achieved at surgery between the acetabular component and pelvis ranged from 11 to 17 cm(2) (15%-27% of each component).\"\nQuestion:\n\"Does cup-cage reconstruction with oversized cups provide initial stability in THA for osteoporotic acetabular fractures?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20608141": {
                "source": [
                    "\"Prostate-specific antigen (PSA) levels can show wide fluctuations when repeatedly measured. Here we investigatewd if: (a) biopsy timing influences the prostate cancer (PC) detection rate in patients with fluctuating PSA (flu-PSA) in comparison with patients with steadily increasing PSA (si-PSA); (b) PSA slope estimated in patients with flu-PSA predicts a different risk of cancer detection; (c) flu-PSA and si-PSA patients develop PC in topographically different sites; (d) the behaviour of pre-operative PSA is an expression of a disease with defferent characteristics to the following radical prostatectomy.\nThe study involved 211 patients who underwent at least a second biopsy after a first negative prostate biopsy. PSA Slope, PSA velocity (PSAV) and PSA doubling time (PSADT) were estimated. Flu-PSA level was defined as a PSA series with at least one PSA value lower than the one immediately preceding it.\n82 patients had flu-PSA levels and 129 si-PSA levels. There were no significant differences between the two groups in terms of cancer detection, clinical or pathological stage, but the si-PSA group with cancer had a higher Gleason score. No difference was found for PSA Slope between flu-PSA patients with cancer and those without.\"\nQuestion:\n\"PSA repeatedly fluctuating levels are reassuring enough to avoid biopsy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20602784": {
                "source": [
                    "\"This paper investigates the impact of geographic scale (census tract, zip code, and county) on the detection of disparities in breast cancer mortality among three ethnic groups in Texas (period 1995-2005). Racial disparities were quantified using both relative (RR) and absolute (RD) statistics that account for the population size and correct for unreliable rates typically observed for minority groups and smaller geographic units. Results were then correlated with socio-economic status measured by the percentage of habitants living below the poverty level.\nAfrican-American and Hispanic women generally experience higher mortality than White non-Hispanics, and these differences are especially significant in the southeast metropolitan areas and southwest border of Texas. The proportion and location of significant racial disparities however changed depending on the type of statistic (RR versus RD) and the geographic level. The largest proportion of significant results was observed for the RD statistic and census tract data. Geographic regions with significant racial disparities for African-Americans and Hispanics frequently had a poverty rate above 10.00%.\"\nQuestion:\n\"Identification of racial disparities in breast cancer mortality: does scale matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10456814": {
                "source": [
                    "\"Although desflurane is commonly used to control surgically induced hypertension, its effects on left ventricular (LV) function have not been investigated in this clinical situation. The purpose of the present study was to evaluate the LV function response to desflurane, when used to control intraoperative hypertension.\nIn 50 patients, scheduled for vascular surgery, anesthesia was induced with sufentanil 0.5 microg/kg, midazolam 0.3 mg/kg and atracurium 0.5 mg/kg. After tracheal intubation, anesthesia was maintained with increments of drugs with controlled ventilation (N2O/O2=60/40%) until the start of surgery. A 5 Mhz transesophageal echocardiography (TEE) probe was inserted after intubation. Pulmonary artery catheter and TEE measurements were obtained after induction (to)(control value), at surgical incision (t1) if it was associated with an increase in systolic arterial pressure (SAP) greater than 140 mmHg (hypertension) and after control of hemodynamic parameters by administration of desflurane (return of systolic arterial pressure to within 20% of the control value) (t2) in a fresh gas flow of 31/ min.\nSixteen patients developed hypertension at surgical incision. SAP was controlled by desflurane in all 16 patients. Afterload assessed by systemic vascular resistance index (SVRI), end-systolic wall-stress (ESWS) and left-ventricular stroke work index (LVSWI) increased with incision until the hypertension returned to post-induction values with mean end-tidal concentration of 5.1+/-0.7% desflurane. No change in heart rate, cardiac index, mean pulmonary arterial pressure, stroke volume, end-diastolic and end-systolic cross-sectional areas, fractional area change and left ventricular circumferential fiber shortening was noted when desflurane was added to restore blood pressure.\"\nQuestion:\n\"Does desflurane alter left ventricular function when used to control surgical stimulation during aortic surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9107172": {
                "source": [
                    "\"If long-term use of left ventricular assist devices (LVADs) as bridges to transplantation is successful, the issue of permanent device implantation in lieu of transplantation could be addressed through the creation of appropriately designed trials. Our medium-term experience with both pneumatically and electrically powered ThermoCardiosystems LVADs is presented to outline the benefits and limitations of device support in lieu of transplantation.\nDetailed records were kept prospectively for all patients undergoing LVAD insertion. Fifty-eight LVADs were inserted over 5 years, with a survival rate of 74%. Mean patient age was 50 years, and duration of support averaged 98 days. Although common, both preexisting infection and infection during LVAD support were not associated with increased mortality or decreased rate of successful transplantation. Thromboembolic complications were rare, occurring in only three patients (5%) despite the absence of anticoagulation. Ventricular arrhythmias were well tolerated in all patients except in cases of early perioperative right ventricular failure, with no deaths. Right ventricular failure occurred in one third of patients and was managed in a small percentage by right ventricular assist device (RVAD) support and/or inhaled nitric oxide therapy. There were no serious device malfunctions, but five graft-related hemorrhages resulted in two deaths. Finally, a variety of noncardiac surgical procedures were performed in LVAD recipients, with no major morbidity and mortality.\"\nQuestion:\n\"Bridge experience with long-term implantable left ventricular assist devices. Are they an alternative to transplantation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25691513": {
                "source": [
                    "\"Effective musical communication requires conveyance of the intended message in a manner perceptible to the receiver. Communication disorders that impair transmitting or decoding of structural features of music (e.g., pitch, timbre) and/or symbolic representation may result in atypical musical communication, which can have a negative impact on music therapy interventions.\nThis study compared recognition of symbolic representation of emotions or movements in music by two groups of children with different communicative characteristics: severe to profound hearing loss (using cochlear implants [CI]) and autism spectrum disorder (ASD). Their responses were compared to those of children with typical-development and normal hearing (TD-NH). Accuracy was examined as a function of communicative status, emotional or movement category, and individual characteristics.\nParticipants listened to recorded musical excerpts conveying emotions or movements and matched them with labels. Measures relevant to auditory and/or language function were also gathered.\nThere was no significant difference between the ASD and TD-NH groups in identification of musical emotions or movements. However, the CI group was significantly less accurate than the other two groups in identification of both emotions and movements. Mixed effects logistic regression revealed different patterns of accuracy for specific emotions as a function of group.\"\nQuestion:\n\"Do communication disorders extend to musical messages?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27928673": {
                "source": [
                    "\"The mode of delivery depends on multiple parameters. After assisted reproductive technology (ART), previous studies have shown elevated C-section rates but few studies differentiated between elective and emergency operations and different protocols of cryopreservation. Because these studies did not use multiparity as exclusion criteria which reduces confounding with previous pregnancies, aim of this study is to compare mode of delivery of different techniques of ART using data of primiparae only [1, 2].\nRetrospective analysis of patient data treated at the university hospital of Luebeck in a period of 12 years. Patients were divided in different groups according to their way of conception: spontaneous conception and conception after\u00a0ART. The group of ART was further divided into: (a) a group of fresh transferred embryos (IVF/ICSI), (b) vitrification and (c) slow freezing. Exclusion criteria were defined as: multiparity, delivery<24.\u00a0+\u00a00\u00a0p.m., incomplete data and treatment outside university of Luebeck. Main parameter of this study was mode of delivery which was divided into spontaneous delivery or C-section. C-sections were further differentiated into elective or emergency C-sections.\nThe group of fresh transferred embryos and slow freezing showed higher risks for elective and emergency C-sections (elective C-sections odds ratio 2.0, CI 95% 1.6-2.6, emergency C-sections odds ratio 1.4, CI 95% 1.1-1.9). Moreover, all groups of ART show enhanced risk of significant perinatal bleeding.\"\nQuestion:\n\"Do ART patients face higher C-section rates during their stage of delivery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17054994": {
                "source": [
                    "\"Frozen section (FS) evaluation during thyroid surgery is often used to guide intraoperative management. We sought to determine the utility of FS in patients undergoing thyroidectomy for multinodular thyroid disease.\nFrom May 1994 through November 2004, 236 patients with multinodular goiter underwent thyroidectomy at our institution. Patient data were retrospectively analyzed to see if a frozen section was performed during the procedure and whether it changed the patient's outcome.\nOf the 236 patients, 135 (57%) had intra-operative FS. There were no differences between patients who had FS analysis and those who did not with regard to age, gender, and the incidence of malignancy. Of the patients who had FS, 4/135 (3%) were subsequently diagnosed with thyroid cancer on permanent histology. Three of these FS were misread as benign. Therefore, the sensitivity of FS for the diagnosis of thyroid cancer was only 25%. Importantly, in none of the 135 patients did FS alter the intraoperative management.\"\nQuestion:\n\"Does frozen section alter surgical management of multinodular thyroid disease?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21550158": {
                "source": [
                    "\"This investigation assesses the effect of platelet-rich plasma (PRP) gel on postoperative pain, swelling, and trismus as well as healing and bone regeneration potential on mandibular third molar extraction sockets.\nA prospective randomized comparative clinical study was undertaken over a 2-year period. Patients requiring surgical extraction of a single impacted third molar and who fell within the inclusion criteria and indicated willingness to return for recall visits were recruited. The predictor variable was application of PRP gel to the socket of the third molar in the test group, whereas the control group had no PRP. The outcome variables were pain, swelling, and maximum mouth opening, which were measured using a 10-point visual analog scale, tape, and millimeter caliper, respectively. Socket healing was assessed radiographically by allocating scores for lamina dura, overall density, and trabecular pattern. Quantitative data were presented as mean. Mann-Whitney test was used to compare means between groups for continuous variables, whereas Fischer exact test was used for categorical variables. Statistical significance was inferred at P<.05.\nSixty patients aged 19 to 35 years (mean: 24.7 \u00b1 3.6 years) were divided into both test and control groups of 30 patients each. The mean postoperative pain score (visual analog scale) was lower for the PRP group at all time points and this was statistically significant (P<.05). Although the figures for swelling and interincisal mouth opening were lower in the test group, this difference was not statistically significant. Similarly, the scores for lamina dura, trabecular pattern, and bone density were better among patients in the PRP group. This difference was also not statistically significant.\"\nQuestion:\n\"Can autologous platelet-rich plasma gel enhance healing after surgical extraction of mandibular third molars?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11296674": {
                "source": [
                    "\"To report an uncommon association of prostate and lung cancer.\nThe characteristics of both tumors, their association with tumors in other sites and the time of presentation are analyzed.\nBoth tumors were in the advanced stages. Metastatic carcinoma of the prostate was discarded due to the form of presentation.\"\nQuestion:\n\"Prostatic syndrome and pleural effusion: are they different diseases?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24172579": {
                "source": [
                    "\"Women are more likely to have a worse outcome after an acute stroke than men. Some studies have suggested that women also benefit less from intravenous thrombolysis after an acute ischemic stroke, but others found no sex differences in safety and efficacy. We aimed to evaluate differences in 3-month outcome between sexes in intravenous tissue-type plasminogen activator-treated patients registered in the Safe Implementation of Treatments in Stroke-International Stroke Thrombolysis Register.\nA total of 45 079 patients treated with intravenous alteplase were recorded from 2002 to 2011. Main outcome measures were symptomatic intracerebral hemorrhage, functional independence (modified Rankin Scale score, 0-2), and mortality at 3 months.\nAmong 25 777 (57.2%) men and 19 302 (42.8%) women, we found no difference in the rate of symptomatic intracerebral hemorrhage (P=0.13), a significantly higher likelihood of functional independence at 3 months in men (P<0.0001) and a higher mortality in women when compared with men (P<0.00001). After adjustment for confounding variables, we did not observe any difference between sexes in functional outcome (odds ratio, 1.03; 95% confidence interval, 0.97-1.09; P=0.39), whereas male sex was related to a higher risk of mortality (odds ratio, 1.19; 95% confidence interval, 1.10-1.29; P=0.00003) and symptomatic intracerebral hemorrhage (odds ratio, 1.25, 95% confidence interval, 1.04-1.51; P=0.02).\"\nQuestion:\n\"Does sex influence the response to intravenous thrombolysis in ischemic stroke?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9347843": {
                "source": [
                    "\"Because of the inflammatory nature of Crohn's disease, ileocolic resections are often difficult to perform, especially if an abscess, phlegmon, or recurrent disease at a previous ileocolic anastomosis is present. Our goal was to determine whether the above factors are contraindications to a successful laparoscopic-assisted ileocolic resection.\nBetween 1992 and 1996, 46 laparoscopic-assisted ileocolic resections were attempted. Fourteen patients had an abscess or phlegmon treated with bowel rest before operation (group I), 10 patients had recurrent Crohn's disease at the previous ileocolic anastomosis (group II), and 22 patients had no previous operation and no phlegmon or abscess associated with their disease (group III). These groups were compared with each other and with 70 consecutive open ileocolic resections for Crohn's disease during the same time period (group IV).\nOperative blood loss and time were greater in group IV than in groups I, II, and III (245 versus 151, 131, and 195 ml, respectively, and 202 versus 152, 144, and 139 minutes, respectively). Conversion to open procedure occurred in 5 patients (group I, 1 [7%]; group II, 2 [20%]; group III, 2 [9%]). Morbidity was highest in group IV (21% versus 0%, 10%, and 10%, respectively). Only one patient died (group IV, 1%). Length of hospital stay was longest in group IV (7.9 versus 4.8, 3.9, and 4.5 days, respectively).\"\nQuestion:\n\"Laparoscopic-assisted ileocolic resections in patients with Crohn's disease: are abscesses, phlegmons, or recurrent disease contraindications?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16195477": {
                "source": [
                    "\"Obesity is associated with an increased risk for cardiovascular disease. Although it is known that white adipose tissue (WAT) produces numerous proinflammatory and proatherogenic cytokines and chemokines, it is unclear whether adipose-derived chemotactic signals affect the chronic inflammation in atherosclerosis.\nHistological examination showed that perivascular WAT (pWAT) is in close proximity to vascular walls, particularly at sites that have a tendency to develop atherosclerosis. In rodents, the amount of pWAT is markedly increased by a high-fat diet. At a functional level, supernatant from subcutaneous and pWAT strongly induced the chemotaxis of peripheral blood leukocytes. The migration of granulocytes and monocytes was mostly mediated by interleukin-8 and monocyte chemoattractant protein-1, respectively, whereas both chemokines contributed to the migration of activated T cells. Moreover, pWAT produces these chemokines, as shown by immunohistochemistry and by explant culture. The accumulation of macrophages and T cells at the interface between pWAT and the adventitia of human atherosclerotic aortas may reflect this prochemotactic activity of pWAT.\"\nQuestion:\n\"Production of chemokines by perivascular adipose tissue: a role in the pathogenesis of atherosclerosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17565137": {
                "source": [
                    "\"To evaluate the effect of an antismoking advertisement on young people's perceptions of smoking in movies and their intention to smoke.SUBJECTS/\n3091 cinema patrons aged 12-24 years in three Australian states; 18.6% of the sample (n = 575) were current smokers.DESIGN/\nQuasi-experimental study of patrons, surveyed after having viewed a movie. The control group was surveyed in week 1, and the intervention group in weeks 2 and 3. Before seeing the movie in weeks 2 and 3, a 30 s antismoking advertisement was shown, shot in the style of a movie trailer that warned patrons not to be sucked in by the smoking in the movie they were about to see.\nAttitude of current smokers and non-smokers to smoking in the movies; intention of current smokers and non-smokers to smoke in 12 months.\nAmong non-smokers, 47.8% of the intervention subjects thought that the smoking in the viewed movie was not OK compared with 43.8% of the control subjects (p = 0.04). However, there was no significant difference among smokers in the intervention (16.5%) and control (14.5%) groups (p = 0.4). A higher percentage of smokers in the intervention group indicated that they were likely to be smoking in 12 months time (38.6%) than smokers in the control group (25.6%; p<0.001). For non-smokers, there was no significant difference in smoking intentions between groups, with 1.2% of intervention subjects and 1.6% of controls saying that they would probably be smoking in 12 months time (p = 0.54).\"\nQuestion:\n\"Out of the smokescreen II: will an advertisement targeting the tobacco industry affect young people's perception of smoking in movies and their intention to smoke?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25481573": {
                "source": [
                    "\"To assess the extent to which the title and font of participant information sheets (PISs) can influence pregnant women's and trainee midwives' perceptions of an antenatal intervention.\nPregnant women (n=35) and trainee midwives (n=36) were randomly presented with one of four PISs where the title and font of the PIS had been manipulated to create four experimental conditions (i.e., Double Fluent; Double Awkward; Fluent Title-Awkward Font; Awkward Title-Fluent Font). After reading the PIS, participants rated their perceptions of the intervention (i.e., Attractiveness, Complexity, Expected Risk, Required Effort) using five-point Likert scales.\nA 4\u00d72 factorial multivariate analysis of variance revealed that pregnant women rated the Double Awkward condition as significantly more complex than the Double Fluent (p=.024) and Awkward Title-Fluent Font (p=.021) conditions.\"\nQuestion:\n\"Processing fluency effects: can the content and presentation of participant information sheets influence recruitment and participation for an antenatal intervention?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24336869": {
                "source": [
                    "\"The 'law of spatiotemporal concentrations of events' introduced major preventative shifts in policing communities. 'Hotspots' are at the forefront of these developments yet somewhat understudied in emergency medicine. Furthermore, little is known about interagency 'data-crossover', despite some developments through the Cardiff Model. Can police-ED interagency data-sharing be used to reduce community-violence using a hotspots methodology?\n12-month (2012) descriptive study and analysis of spatiotemporal clusters of police and emergency calls for service using hotspots methodology and assessing the degree of incident overlap. 3775 violent crime incidents and 775 assault incidents analysed using spatiotemporal clustering with k-means++ algorithm and Spearman's rho.\nSpatiotemporal location of calls for services to the police and the ambulance service are equally highly concentrated in a small number of geographical areas, primarily within intra-agency hotspots (33% and 53%, respectively) but across agencies' hotspots as well (25% and 15%, respectively). Datasets are statistically correlated with one another at the 0.57 and 0.34 levels, with 50% overlap when adjusted for the number of hotspots. At least one in every two police hotspots does not have an ambulance hotspot overlapping with it, suggesting half of assault spatiotemporal concentrations are unknown to the police. Data further suggest that more severely injured patients, as estimated by transfer to hospital, tend to be injured in the places with the highest number of police-recorded crimes.\"\nQuestion:\n\"Can routinely collected ambulance data about assaults contribute to reduction in community violence?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25475395": {
                "source": [
                    "\"For women, the correlation between circulating androgens and sexual desire is inconclusive. Substitution with androgens at physiological levels improves sexual function in women who experience decreased sexual desire and androgen deficiency from surgical menopause, pituitary disease, and age-related decline in androgen production in the ovaries. Measuring bioactive testosterone is difficult and new methods have been proposed, including measuring the primary androgen metabolite androsterone glucuronide (ADT-G).AIM: The aim of this study was to investigate a possible correlation between serum levels of androgens and sexual desire in women and whether the level of ADT-G is better correlated than the level of circulating androgens with sexual desire.\nThis was a cross-sectional study including 560 healthy women aged 19-65 years divided into three age groups. Correlations were considered to be statistically significant at P<0.05.\nSexual desire was determined as the total score of the sexual desire domain of the Female Sexual Function Index. Total testosterone (TT), calculated free testosterone (FT), androstenedione, dehydroepiandrosterone sulfate (DHEAS), and ADT-G were analyzed using mass spectrometry.\nSexual desire correlated overall with FT and androstenedione in the total cohort of women. In a subgroup of women aged 25-44 years with no use of systemic hormonal contraception, sexual desire correlated with TT, FT, androstenedione, and DHEAS. In women aged 45-65 years, androstenedione correlated with sexual desire. No correlations between ADT-G and sexual desire were identified.\"\nQuestion:\n\"Is there a correlation between androgens and sexual desire in women?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19482903": {
                "source": [
                    "\"Earlier studies have demonstrated low peak oxygen uptake ((.)Vo(2)peak) in children with spina bifida. Low peak heart rate and low peak respiratory exchange ratio in these studies raised questions regarding the true maximal character of (.)Vo(2)peak values obtained with treadmill testing.\nThe aim of this study was to determine whether the Vo(2)peak measured during an incremental treadmill test is a true reflection of the maximum oxygen uptake ((.)Vo(2)max) in children who have spina bifida and are ambulatory.\nA cross-sectional design was used for this study.\nTwenty children who had spina bifida and were ambulatory participated. The (.)Vo(2)peak was measured during a graded treadmill exercise test. The validity of (.)Vo(2)peak measurements was evaluated by use of previously described guidelines for maximum exercise testing in children who are healthy, as well as differences between Vo(2)peak and (.)Vo(2) during a supramaximal protocol ((.)Vo(2)supramaximal).\nThe average values for (.)Vo(2)peak and normalized (.)Vo(2)peak were, respectively, 1.23 L/min (SD=0.6) and 34.1 mL/kg/min (SD=8.3). Fifteen children met at least 2 of the 3 previously described criteria; one child failed to meet any criteria. Although there were no significant differences between (.)Vo(2)peak and Vo(2)supramaximal, 5 children did show improvement during supramaximal testing.\nThese results apply to children who have spina bifida and are at least community ambulatory.\"\nQuestion:\n\"Treadmill testing of children who have spina bifida and are ambulatory: does peak oxygen uptake reflect maximum oxygen uptake?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11458136": {
                "source": [
                    "\"By requiring or encouraging enrollees to obtain a usual source of care, managed care programs hope to improve access to care without incurring higher costs.\n(1) To examine the effects of managed care on the likelihood of low-income persons having a usual source of care and a usual physician, and; (2) To examine the association between usual source of care and access.\nCross-sectional survey of households conducted during 1996 and 1997.\nA nationally representative sample of 14,271 low-income persons.\nUsual source of care, usual physician, managed care enrollment, managed care penetration.\nHigh managed care penetration in the community is associated with a lower likelihood of having a usual source of care for uninsured persons (54.8% vs. 62.2% in low penetration areas) as well as a lower likelihood of having a usual physician (60% vs. 72.8%). Managed care has only marginal effects on the likelihood of having a usual source of care for privately insured and Medicaid beneficiaries. Having a usual physician substantially reduces unmet medical needs for the insured but less so for the uninsured.\"\nQuestion:\n\"Does managed care enable more low income persons to identify a usual source of care?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15137012": {
                "source": [
                    "\"Most pediatric surgeons perform oophorectomy in girls presenting with ovarian torsion in which the ovary appears necrotic. However, the adult gynecology literature suggests that many ovaries can be treated by detorsion alone.\nAll children with ovarian torsion on the gynecology and general surgery services between 1988 and 2002 were reviewed.\nThere were 36 torsions in 34 children. Seventeen underwent detorsion with or without ovarian cystectomy, and 19 had oophorectomy (mean age 10 years in both groups). Torsion was suspected preoperatively in 94% of the detorsion cases and in 47% of the oophorectomy patients. Median time from presentation to surgery was significantly lower in the detorsion than the oophorectomy group (median 14 v 27 hours; P =.04). Postoperative complications and length of stay were similar between the 2 groups. Despite the ovary being judged intraoperatively as moderately to severely ischemic in 53% of the detorsion cases, follow-up sonogram or ovarian biopsy available in 14 of the 17 cases showed normal ovary with follicular development in each case.\"\nQuestion:\n\"Ovarian torsion in children: is oophorectomy necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18049437": {
                "source": [
                    "\"Multiple sclerosis (MS) is an immune-mediated inflammatory demyelinating disease of uncertain etiology. Although the mechanisms of inducting autoimmunity by some of the infectious agents have been investigated, there is not yet enough research on streptococcal infections.MATERIAL/\nTo understand the effect of past group A streptococcal infection on MS, antistreptolysin O (ASO) and antideoxyribonuclease B (ADNase B) were measured in 21 patients with relapsing-remitting MS and 21 healthy blood donors by nephelometric assay.\nADNase B levels in the patients with MS were found to be significantly higher than in the controls (p<0.001); however, ASO levels were similar in both groups.\"\nQuestion:\n\"Is there any relationship between streptococcal infection and multiple sclerosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18307476": {
                "source": [
                    "\"The robust relationship between socioeconomic factors and health suggests that social and economic policies might substantially affect health, while other evidence suggests that medical care, the main focus of current health policy, may not be the primary determinant of population health. Income support policies are one promising avenue to improve population health. This study examines whether the federal cash transfer program to poor elderly, the Supplemental Security Income (SSI) program, affects old-age disability.\nThis study uses the 1990 and 2000 censuses, employing state and year fixed-effect models, to test whether within-state changes in maximum SSI benefits over time lead to changes in disability among people aged sixty-five and older.\nHigher benefits are linked to lower disability rates. Among all single elderly individuals, 30 percent have mobility limitations, and an increase of $100 per month in the maximum SSI benefit caused the rate of mobility limitations to fall by 0.46 percentage points. The findings were robust to sensitivity analyses. First, analyses limited to those most likely to receive SSI produced larger effects, but analyses limited to those least likely to receive SSI produced no measurable effect. Second, varying the disability measure did not meaningfully alter the findings. Third, excluding the institutionalized, immigrants, individuals living in states with exceptionally large benefit changes, and individuals living in states with no SSI supplements did not change the substantive conclusions. Fourth, Medicaid did not confound the effects. Finally, these results were robust for married individuals.\"\nQuestion:\n\"Upstream solutions: does the supplemental security income program reduce disability in the elderly?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26200172": {
                "source": [
                    "\"In recent years, biofeedback has become increasingly popular for its proven success in peak performance training - the psychophysiological preparation of athletes for high-stakes sport competitions, such as the Olympic games. The aim of this research was to test whether an 8-week period of exposure to biofeedback training could improve the psychophysiological control over competitive anxiety and enhance athletic performance in participating subjects.\nParticipants of this study were highly competent athletes, each training in different sport disciplines. The experimental group consisted of 18 athletes (4 women, 14 men), whereas the Control group had 21 athletes (4 women, 17 men). All athletes were between 16 and 34 years old. The biofeedback device, Nexus 10, was used to detect and measure the psychophysiological responses of athletes. Athletes from both groups (control and experimental) were subjected to stress tests at the beginning of the study and once again at its conclusion. In between, the experimental group received training in biofeedback techniques. We then calculated the overall percentage of athletes in the experimental group compared with those in the control group who were able to control respiration, skin conductance, heart rate, blood flow amplitude, heart rate variability, and heart respiration coherence. One year following completion of the initial study, we questioned athletes from the experimental group, to determine whether they continued to use these skills and if they could detect any subsequent enhancement in their athletic performance.\nWe demonstrated that a greater number of participants in the experimental group were able to successfully control their psychophysiological parameters, in comparison to their peers in the control group. Significant results (p<0.05) were noted in regulation of GSR following short stress test conditions (p = 0.037), in regulation of HR after exposure to STROOP stressor (p = 0.037), in regulation of GSR following the Math and GSR stressors (p = 0.033, p = 0.409) and in achieving HR - breathing coherence following the math stressor (p = 0.042).\"\nQuestion:\n\"Can biofeedback training of psychophysiological responses enhance athletes' sport performance?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16241924": {
                "source": [
                    "\"To study the effect of parity on impairment of insulin sensitivity during pregnancy and on the risk of gestational diabetes (GDM).\nWe studied the relationship between parity and peripheral insulin sensitivity index (ISI(OGTT)) or GDM in 1880 caucasian women, who underwent a 100-g, 3-h oral glucose tolerance test (OGTT) between the 24th and 28th gestational week and in 75 women who underwent an OGTT in two consecutive pregnancies. A proxy for beta-cell function (basal plasma C peptide/fasting plasma glucose; CP/FPG) was also measured.\nBy univariate analysis parity was related to decreased ISI(OGTT) and to increased CP/FPG in those with parity>3 and likewise GDM, diagnosed in 124 women (6.58%), was linearly related to parity (P = 0.0034) and strongly age dependent. The relationships between parity and ISI(OGTT), CP/FPG and GDM were no longer significant after adjustment for age, pregestational body mass index (BMI), and weight gain. GDM was significantly related to age and pregestational weight, while ISI(OGTT) and CP/FPG were inversely related to prepregnancy BMI or weight gain. In comparison with the index pregnancy, the subsequent pregnancy was characterized by an increase in actual and prepregnancy BMI, in 2 h area under curve (AUC) glucose and by a decrease in ISI(OGTT) (P = 0.0001). The longer the time interval between pregnancies and the higher the increment in pregestational BMI or in weight gain during the pregnancy, the greater were the ISI(OGTT) decrease and 2-h AUC glucose increase.\"\nQuestion:\n\"Does parity increase insulin resistance during pregnancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9446993": {
                "source": [
                    "\"To determine the ability of dentists to recognize digitally manipulated radiographs.\nA poster was presented at the Annual Meeting of the German Society for Periodontology displaying the intra-oral radiographs of 12 different patients. Half of the radiographs were subjected to digital manipulation to add or remove specific features. Dentists were asked to identify these radiographs by means of a questionnaire.\nThirty-nine dentists submitted usable questionnaires. Statistical evaluation revealed a distribution of hits similar to the random distribution. None of the dentists detected all the six manipulated radiographs; three dentists had five correct, but there were five with only one. An authentic radiograph scored highest as a manipulation.\"\nQuestion:\n\"Can dentists recognize manipulated digital radiographs?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25079920": {
                "source": [
                    "\"As parents of young children are often unaware their child is overweight, screening provides the opportunity to inform parents and provide the impetus for behaviour change. We aimed to determine if parents could recall and understand the information they received about their overweight child after weight screening.\nRandomised controlled trial of different methods of feedback.\nParticipants were recruited through primary and secondary care but appointments took place at a University research clinic.\n1093 children aged 4-8\u2005years were screened. Only overweight children (n=271, 24.7%) are included in this study. Parents of overweight children were randomised to receive feedback regarding their child's weight using best practice care (BPC) or motivational interviewing (MI) at face-to-face interviews typically lasting 20-40\u2005min. 244 (90%) parents participated in a follow-up interview 2\u2005weeks later to assess recall and understanding of information from the feedback session.\nInterviews were audio-taped and transcribed verbatim before coding for amount and accuracy of recall. Scores were calculated for total recall and sub-categories of interest.\nOverall, 39% of the information was recalled (mean score 6.3 from possible score of 16). Parents given feedback via BPC recalled more than those in the MI group (difference in total score 0.48; 95% CI 0.05 to 0.92). Although 94% of parents were able to correctly recall their child's weight status, fewer than 10 parents could accurately describe what the measurements meant. Maternal education (0.81; 0.25 to 1.37) and parental ratings of how useful they found the information (0.19; 0.04 to 0.35) were significant predictors of recall score in multivariate analyses.\"\nQuestion:\n\"Do parents recall and understand children's weight status information after BMI screening?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9722752": {
                "source": [
                    "\"To evaluate the outcome of a new modification of percutaneous needle suspension, using a bone anchor system for fixing the suture at the public bone, and to compare the results with those published previously.\nFrom March 1996, 37 patients with stress urinary incontinence (>2 years) were treated using a bone anchor system. On each side the suture was attached to the pubocervical fascia and the vaginal wall via a broad 'Z'-stitch. A urodynamic investigation performed preoperatively in all patients confirmed stress incontinence and excluded detrusor instability. The outcome was assessed by either by a clinical follow-up investigation or using a standardized questionnaire, over a mean follow-up of 11 months (range 6-18).\nIn the 37 patients, the procedure was successful in 25 (68%), with 16 (43%) of the patients completely dry and nine (24%) significantly improved. Removal of the bone anchor and suture was necessary in two patients, because of unilateral bacterial infection in one and a bilateral soft tissue granuloma in the other. One bone anchor became dislocated in a third patient. In two cases where the treatment failed, new detrusor instability was documented urodynamically. Minor complications were prolonged wound pain in 10 (26%) and transient urinary retention or residual urine in 12 patients (32%).\"\nQuestion:\n\"Does bone anchor fixation improve the outcome of percutaneous bladder neck suspension in female stress urinary incontinence?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18594195": {
                "source": [
                    "\"Refusal of patients to participate in intervention programs is an important problem in clinical trials but, in general, researchers devote relatively little attention to it. In this article, a comparison is made between patients who, after having been invited, agreed to participate in a self-management intervention (participants) and those who refused (refusers). Compared with other studies of refusers, relatively more information could be gathered with regard to both their characteristics and reasons for refusing, because all potential participants were invited personally.\nOlder patients from a Dutch outpatient clinic were invited to participate in a self-management intervention, and their characteristics were assessed. Demographic data were collected, as well as data on physical functioning and lack of emotional support. People who refused to participate were asked to give their reasons for refusing.\nOf the 361 patients invited, 267 (74%) refused participation. These refusers were more restricted in their mobility, lived further away from the location of the intervention, and had a partner more often than did the participants. No differences were found in level of education, age or gender. The main reasons given by respondents for refusing to participate were lack of time, travel distance, and transport problems.\"\nQuestion:\n\"Do older patients who refuse to participate in a self-management intervention in the Netherlands differ from older patients who agree to participate?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10757151": {
                "source": [
                    "\"Ischemic preconditioning (IP) is initiated through one or several short bouts of ischemia and reperfusion which precede a prolonged ischemia. To test whether a reperfusion must precede the prolonged index ischemia, a series without reperfusion (intraischemic preconditioning: IIP) and a series with gradual onset of ischemia, i.e. ramp ischemia (RI), which is possibly related to the development of hibernation, was compared to conventional IP (CIP).\nExperiments were performed an 27 blood-perfused rabbit hearts (Langendorff apparatus) that were randomized into one of four series: (1) control (n = 7): 60 min normal flow - 60 min low flow (10%) ischemia - 60 min reperfusion. (2) CIP (n = 7): 4 times 5 min zero flow with 10 min reperfusion each - 60 min low flow (10%) - ischemia 60 min reperfusion. (3) IIP (n = 7): 50 min normal flow - 10 min no flow - 60min low flow (10%) ischemia -4 60min reperfusion. (4) RI (n=6): gradual reduction to 10% flow during 60min - 60min low flow (10%) ischemia - 60min reperfusion. At the end of each protocol, the infarcted area was assessed.\nThe infarct area in control hearts was 6.7+/-1.4% (means+/-SEM) of LV total area, in CIP hearts 2.6+/-0.8%, in IIP hearts 3.1+/-0.5%, and in RI hearts 3.0+/-0.3% (all p<0.05 vs. control). The differences between the three protection protocols were statistically not significant, and no protective protocol reduced post-ischemic myocardial dysfunction.\"\nQuestion:\n\"Does ischemic preconditioning require reperfusion before index ischemia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27581329": {
                "source": [
                    "\"Measurement of basal metabolic rate (BMR) is suggested as a tool to estimate energy requirements. Therefore, BMR prediction equations have been developed in multiple populations because indirect calorimetry is not always feasible. However, there is a paucity of data on BMR measured in overweight and obese adults living in Asia and equations developed for this group of interest. The aim of this study was to develop a new BMR prediction equation for Chinese adults applicable for a large BMI range and compare it with commonly used prediction equations.\nSubjects were 121 men and 111 women (age: 21-67 years, BMI: 16-41\u00a0kg/m(2)). Height, weight, and BMR were measured. Continuous open-circuit indirect calorimetry using a ventilated hood system for 30\u00a0min was used to measure BMR. A regression equation was derived using stepwise regression and accuracy was compared to 6 existing equations (Harris-Benedict, Henry, Liu, Yang, Owen and Mifflin). Additionally, the newly derived equation was cross-validated in a separate group of 70 Chinese subjects (26 men and 44 women, age: 21-69 years, BMI: 17-39\u00a0kg/m(2)).\nThe equation developed from our data was: BMR (kJ/d)\u2009=\u200952.6 x weight (kg)\u2009+\u2009828 x gender\u2009+\u20091960 (women\u2009=\u20090, men\u2009=\u20091; R(2)\u2009=\u20090.81). The accuracy rate (within 10\u00a0% accurate) was 78\u00a0% which compared well to Owen (70\u00a0%), Henry (67\u00a0%), Mifflin (67\u00a0%), Liu (58\u00a0%), Harris-Benedict (45\u00a0%) and Yang (37\u00a0%) for the whole range of BMI. For a BMI greater than 23, the Singapore equation reached an accuracy rate of 76\u00a0%. Cross-validation proved an accuracy rate of 80\u00a0%.\"\nQuestion:\n\"Estimation of basal metabolic rate in Chinese: are the current prediction equations applicable?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23774337": {
                "source": [
                    "\"Despite a previous meta-analysis that concluded that central venous pressure should not be used to make clinical decisions regarding fluid management, central venous pressure continues to be recommended for this purpose.AIM: To perform an updated meta-analysis incorporating recent studies that investigated indices predictive of fluid responsiveness. A priori subgroup analysis was planned according to the location where the study was performed (ICU or operating room).\nMEDLINE, EMBASE, Cochrane Register of Controlled Trials, and citation review of relevant primary and review articles.\nClinical trials that reported the correlation coefficient or area under the receiver operating characteristic curve (AUC) between the central venous pressure and change in cardiac performance following an intervention that altered cardiac preload. From 191 articles screened, 43 studies met our inclusion criteria and were included for data extraction. The studies included human adult subjects, and included healthy controls (n = 1) and ICU (n = 22) and operating room (n = 20) patients.\nData were abstracted on study characteristics, patient population, baseline central venous pressure, the correlation coefficient, and/or the AUC between central venous pressure and change in stroke volume index/cardiac index and the percentage of fluid responders. Meta-analytic techniques were used to summarize the data.\nOverall 57% \u00b1 13% of patients were fluid responders. The summary AUC was 0.56 (95% CI, 0.54-0.58) with no heterogenicity between studies. The summary AUC was 0.56 (95% CI, 0.52-0.60) for those studies done in the ICU and 0.56 (95% CI, 0.54-0.58) for those done in the operating room. The summary correlation coefficient between the baseline central venous pressure and change in stroke volume index/cardiac index was 0.18 (95% CI, 0.1-0.25), being 0.28 (95% CI, 0.16-0.40) in the ICU patients, and 0.11 (95% CI, 0.02-0.21) in the operating room patients.\"\nQuestion:\n\"Does the central venous pressure predict fluid responsiveness?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21801416": {
                "source": [
                    "\"It is now widely accepted that AMP-activated protein kinase (AMPK) is a critical regulator of energy homeostasis. Recently, it has been shown to regulate circadian clocks. In seasonal breeding species such as sheep, the circadian clock controls the secretion of an endogenous rhythm of melatonin and, as a consequence, is probably involved in the generation of seasonal rhythms of reproduction. Considering this, we identified the presence of the subunits of AMPK in different hypothalamic nuclei involved in the pre- and post-pineal pathways that control seasonality of reproduction in the ewe and we investigated if the intracerebroventricular (i.c.v.) injection of two activators of AMPK, metformin and AICAR, affected the circadian rhythm of melatonin in ewes that were housed in constant darkness. In parallel the secretion of insulin was monitored as a peripheral metabolic marker. We also investigated the effects of i.c.v. AICAR on the phosphorylation of AMPK and acetyl-CoA carboxylase (ACC), a downstream target of AMPK, in brain structures along the photoneuroendocrine pathway to the pineal gland.\nAll the subunits of AMPK that we studied were identified in all brain areas that were dissected but with some differences in their level of expression among structures. Metformin and AICAR both reduced (p<0.001 and p<0.01 respectively) the amplitude of the circadian rhythm of melatonin secretion independently of insulin secretion. The i.c.v. injection of AICAR only tended (p = 0.1) to increase the levels of phosphorylated AMPK in the paraventricular nucleus but significantly increased the levels of phosphorylated ACC in the paraventricular nucleus (p<0.001) and in the pineal gland (p<0.05).\"\nQuestion:\n\"The effect of an intracerebroventricular injection of metformin or AICAR on the plasma concentrations of melatonin in the ewe: potential involvement of AMPK?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19481382": {
                "source": [
                    "\"Androgen serum levels significantly decrease in older men, causing quality of life impairment and increasing the risk of chronic disease. This disorder is defined as PADAM (Partial Androgen Deficiency of Aging Men).\nTo evaluate a PADAM screening tool and determine the prevalence of this disorder in healthy adult men.\nThis was a cross-sectional study in which 96 men aged 40 or more of the South Metropolitan Region of Santiago de Chile were surveyed with the Androgen Deficiency of Aging Men (ADAM) questionnaire of the Saint Louis University and sampled for the serum determination of total testosterone, sexual hormone binding globulin (SHBG) and albumin. Also free and bioavailable testosterone were calculated. PADAM was considered present if items 1 or 7 or any 3 other questions of the ADAM questionnaire were positive. An available testosterone of<198.4 ng/dL was used as a gold standard for the diagnosis of PADAM.\nA total of 78 men (81.3%) were identified as possible PADAM according to the ADAM questionnaire. Total testosterone levels fell from 503.6+/-180.1 ng/dL in men aged 40 to 54 years to 382.1+/-247.3 in those>70 years; however this was not statistically significant (ANOVA, p=0.06). In the same age groups, SHBG significantly increased (31.0+/-15.0 to 47.5+/-15.0 nmol/L, p<0.001) whereas free and available testosterone significantly decreased (10.6+/-3.2 to 6.4+/-3.6 ng/dL and 266.6+/-81.2 to 152.2+/-97.6 ng/dL, respectively, p<0.0001). Overall (n=96), available testosterone confirmed PADAM diagnosis in 27 cases (28.1%). The ADAM tool rendered a 83.3% sensitivity and 19.7% specificity in the detection of PADAM. Item 1 (decreased sexual desire) was a better predictor of hypogonadism than the complete questionnaire (63.3% sensitivity and 66.7% specificity).\"\nQuestion:\n\"Is the Androgen Deficiency of Aging Men (ADAM) questionnaire useful for the screening of partial androgenic deficiency of aging men?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11035130": {
                "source": [
                    "\"It is postulated that some aspects of methotrexate toxicity may be related to its action as an anti-folate. Folic acid (FA) is often given as an adjunct to methotrexate therapy, but there is no conclusive proof that it decreases the toxicity of methotrexate and there is a theoretical risk that it may decrease the efficacy of methotrexate.\nTo look at the effect of stopping FA supplementation in UK rheumatoid arthritis (RA) patients established on methotrexate<20 mg weekly and FA 5 mg daily, to report all toxicity (including absolute changes in haematological and liver enzyme indices) and to report changes in the efficacy of methotrexate.\nIn a prospective, randomized, double-blind, placebo-controlled study, 75 patients who were established on methotrexate<20 mg weekly and FA 5 mg daily were asked to stop their FA and were randomized to one of two groups: placebo or FA 5 mg daily. Patients were evaluated for treatment toxicity and efficacy before entry and then at intervals of 3 months for 1 yr.\nOverall, 25 (33%) patients concluded the study early, eight (21%) in the group remaining on FA and 17 (46%) in the placebo group (P = 0.02). Two patients in the placebo group discontinued because of neutropenia. At 9 months there was an increased incidence of nausea in the placebo group (45 vs. 7%, P = 0.001). The placebo group had significantly lower disease activity on a few of the variables measured, but these were probably not of clinical significance.\"\nQuestion:\n\"Do patients with rheumatoid arthritis established on methotrexate and folic acid 5 mg daily need to continue folic acid supplements long term?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24245816": {
                "source": [
                    "\"To compare the primary stability of miniscrews inserted into bone blocks of different bone mineral densities (BMDs) with and without cortical bone, and investigate whether some trabecular properties could influence primary stability.\nFifty-two bone blocks were extracted from fresh bovine pelvic bone. Four groups were created based on bone type (iliac or pubic region) and presence or absence of cortical bone. Specimens were micro-computed tomography imaged to evaluate trabecular thickness, trabecular number, trabecular separation, bone volume density (BV/TV), BMD, and cortical thickness. Miniscrews 1.4 mm in diameter and 6 mm long were inserted into the bone blocks, and primary stability was evaluated by insertion torque (IT), mini-implant mobility (PTV), and pull-out strength (PS).\nIntergroup comparison showed lower levels of primary stability when the BMD of trabecular bone was lower and in the absence of cortical bone (P\u2264.05). The Pearson correlation test showed correlation between trabecular number, trabecular thickness, BV/TV, trabecular BMD, total BMD, and IT, PTV, and PS. There was correlation between cortical thickness and IT and PS (P\u2264.05).\"\nQuestion:\n\"Is trabecular bone related to primary stability of miniscrews?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26485091": {
                "source": [
                    "\"Several atypical antipsychotics (AAPs) are used as second-line agents for treatment resistant depression. AAPs can be expensive compared to other treatment options and can cause several side effects.\nTo estimate healthcare costs and utilization of AAPs compared to other second-line agents.\nObservational study using Medicaid claims data (2006-2011). Subjects were depression-diagnosed adult members with at least two prescriptions of antidepressant medications followed by a second-line agent. Gamma generalized linear models (GLM) produced estimates of the difference in mean expenditures among treatment groups after adjusting for individual baseline characteristics using propensity scores. Negative binomial models produced estimates of the difference in number of hospitalizations and emergency department (ED) visits.\nA total of 3910 members received second-line treatment. Treatment groups were AAPs (n\u2009=\u20092211), augmentation agents other than AAPs (n\u2009=\u20091008), and antidepressant switching (n\u2009=\u2009691). AAPs resulted in higher mean adjusted pharmacy costs and higher mean adjusted total mental health-related costs. Mean adjusted total healthcare costs and number of inpatient and ED visits were not different among treatments.\"\nQuestion:\n\"Does the use of atypical antipsychotics as adjunctive therapy in depression result in cost savings?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21946341": {
                "source": [
                    "\"Ablation of persistent atrial fibrillation (AF) may require adjunctive methods of substrate modification. Both ablation-targeting complex fractionated atrial electrograms (CFAEs) recorded during AF and fractionated electrograms recorded during sinus rhythm (sinus rhythm fractionation [SRF]) have been described. However, the relationship of CFAEs with SRF is unclear.\nTwenty patients (age 62 \u00b1 9 years, 13 males) with persistent AF and 9 control subjects without organic heart disease or AF (age 36 \u00b1 6 years, 4 males) underwent detailed CFAE and SRF left atrial electroanatomic maps. The overlap in left atrial regions with CFAEs and SRF was compared in the AF population, and the distribution of SRF was compared among patients with AF and normal controls. Propagation maps were analyzed to identify the activation patterns associated with SR fractionation.\nSRF (338 \u00b1 150 points) and CFAE (418 \u00b1 135 points) regions comprised 29% \u00b1 14% and 25% \u00b1 15% of the left atrial surface area, respectively. There was no significant correlation between SRF and CFAE maps (r = .2; P = NS). On comparing patients with AF and controls, no significant difference was found in the distribution of SRF between groups (P = .74). Regions of SRF overlapped areas of wave-front collision 75% \u00b1 13% of the time.\"\nQuestion:\n\"Is there a relationship between complex fractionated atrial electrograms recorded during atrial fibrillation and sinus rhythm fractionation?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9003088": {
                "source": [
                    "\"A prerequisite for a hormonal influence on anal continence in women is the presence of hormone receptors in the tissues of the anal canal. Using immunohistochemical techniques, we demonstrated and localized estrogen and progesterone receptors (ER, PR) in tissue sections of the anal canal.\nThirty-five specimens of the anorectal region from 21 patients (14 women, seven men) were examined histologically for smooth muscle (present in specimens from ten females and in seven males), striated muscle (present in three females and five males), and perimuscular connective tissue (present in 12 females and seven males). Immunostaining for ER and PR was then performed by incubating with primary anti-ER and anti-PR antibody and visualization of specific antibody binding by the ABC technique with DAB as chromogen.\nPositive staining for ER and PR was seen exclusively over cell nuclei. Estrogen receptors were found in the smooth muscle cells of the internal sphincter of all females (10/10) and in four of the seven males. Staining for ER was detected in the perimuscular connective tissue of all females (12/12) and in four of the seven males. No specific staining for ER or PR was found in the nuclei of striated muscle cells of the external sphincter in males or females (n = 8). In females, about 50% of the ER-positive tissues were also found to be positive for PR. Amongst the male patients, only one exhibited staining for PR, and this was confined to the smooth muscle.\"\nQuestion:\n\"Immunohistochemical assessment of steroid hormone receptors in tissues of the anal canal. Implications for anal incontinence?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11867487": {
                "source": [
                    "\"To examine the attitudes of players and coaches to the use of protective headgear, particularly with respect to the prevention of concussion.\nA questionnaire designed to assess attitudes to headgear was administered to 63 players from four different Canadian teams, each representing a different level of play (high school, university, community club, national). In addition, coaches from all four levels were questioned about team policies and their personal opinions about the use of headgear to prevent concussion.\nAlthough the players tended to believe that the headgear could prevent concussion (62%), the coaches were less convinced (33%). Despite the players' belief that headgear offers protection against concussion, only a minority reported wearing headgear (27%) and few (24%) felt that its use should be made mandatory. Common reasons for not wearing headgear were \"its use is not mandatory\", \"it is uncomfortable\", and \"it costs too much\".\"\nQuestion:\n\"Does rugby headgear prevent concussion?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26879871": {
                "source": [
                    "\"Studies have linked ethnic differences in depression rates with neighbourhood ethnic density although results have not been conclusive. We looked at this using a novel approach analysing whole population data covering just over one million GP patients in four London boroughs.\nUsing a dataset of GP records for all patients registered in Lambeth, Hackney, Tower Hamlets and Newham in 2013 we investigated new diagnoses of depression and antidepressant use for: Indian, Pakistani, Bangladeshi, black Caribbean and black African patients. Neighbourhood effects were assessed independently of GP practice using a cross-classified multilevel model.\nBlack and minority ethnic groups are up to four times less likely to be newly diagnosed with depression or prescribed antidepressants compared to white British patients. We found an inverse relationship between neighbourhood ethnic density and new depression diagnosis for some groups, where an increase of 10% own-ethnic density was associated with a statistically significant (p<0.05) reduced odds of depression for Pakistani [odds ratio (OR) 0.81, 95% confidence interval (CI) 0.70-0.93], Indian (OR 0.88, CI 0.81-0.95), African (OR 0.88, CI 0.78-0.99) and Bangladeshi (OR 0.94, CI 0.90-0.99) patients. Black Caribbean patients, however, showed the opposite effect (OR 1.26, CI 1.09-1.46). The results for antidepressant use were very similar although the corresponding effect for black Caribbeans was no longer statistically significant (p = 0.07).\"\nQuestion:\n\"Does depression diagnosis and antidepressant prescribing vary by location?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12920330": {
                "source": [
                    "\"Evidence suggests substantial comorbidity between symptoms of somatization and depression in clinical as well as nonclinical populations. However, as most existing research has been retrospective or cross-sectional in design, very little is known about the specific nature of this relationship. In particular, it is unclear whether somatic complaints may heighten the risk for the subsequent development of depressive symptoms.\nWe report findings on the link between symptoms of somatization (assessed using the SCL-90-R) and depression 5 years later (assessed using the CES-D) in an initially healthy cohort of community adults, based on prospective data from the RENO Diet-Heart Study.\nGender-stratified multiple regression analyses revealed that baseline CES-D scores were the best predictors of subsequent depressive symptoms for men and women. Baseline scores on the SCL-90-R somatization subscale significantly predicted subsequent self-reported symptoms of depressed mood 5 years later, but only in women. However, somatic complaints were a somewhat less powerful predictor than income and age.\"\nQuestion:\n\"Do somatic complaints predict subsequent symptoms of depression?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23736032": {
                "source": [
                    "\"A multidisciplinary team (MDT) approach to breast cancer management is the gold standard. The aim is to evaluate MDT decision making in a modern breast unit.\nAll referrals to the breast MDT where breast cancer was diagnosed from 1 July 2009 to 30 June 2011 were included. Multidisciplinary team decisions were compared with subsequent patient management and classified as concordant or discordant.\nOver the study period, there were 3230 MDT decisions relating to 705 patients. Overall, 91.5% (2956 out of 3230) of decisions were concordant, 4.5% (146 out of 3230), were discordant and 4% (128 out of 3230) had no MDT decision. Of 146 discordant decisions, 26 (17.8%) were considered 'unjustifiable' as there was no additional information available after the MDT to account for the change in management. The remaining 120 discordant MDT decisions were considered 'justifiable', as management was altered due to patient choice (n=61), additional information available after MDT (n=54) or MDT error (n=5).\"\nQuestion:\n\"Multidisciplinary decisions in breast cancer: does the patient receive what the team has recommended?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9542484": {
                "source": [
                    "\"To determine whether successful completion of the Perinatal Education Programme (PEP) improves obstetric practice.\nThe three midwife obstetric units (MOUs) in a health district of Mpumalanga were included in the study. Two MOUs enrolled in the PEP and the third did not. A 'before-and-after' study design was used to assess any changes in practice, and to monitor whether any changes occurred in the district during the time of the study; data were also collected at the third MOU. Data were collected by scoring of the obstetric files after the patient had delivered.\nWe ascertained whether the obstetric history, syphilis testing, blood group testing, haemoglobin measurement and uterine growth assessment were performed during antenatal care along with whether appropriate action was taken. For intrapartum care, estimation of fetal weight, the performance of pelvimetry, blood pressure monitoring, urine testing, evaluation of head above pelvis, fetal heart rate monitoring, monitoring of contractions and plotting of cervical dilatation, and whether the appropriate actions were taken, were assessed.\nEight of the 13 midwives at the two MOUs completed the PEP and all demonstrated an improvement in knowledge. Case notes of 303 patients from the various clinics were studied. There was no change in the referral patterns of any of the clinics during the study period. The obstetric history was well documented, but in no group was there a satisfactory response to a detected problem; appropriate action was taken in between 0% and 12% of cases. Syphilis testing was performed in 56-82% of cases, with no difference between the groups. The haemoglobin level was measured in only 4-15% of patients, with no difference before or after completion of the PEP. Where a problem in uterine growth was detected, an appropriate response occurred in 0-8% of patients and no difference before or after completion of the PEP was ascertained. In all groups, estimation of fetal weight and pelvimetry were seldom performed, the urine and fetal heart rate documentation were moderately well done and the blood pressure monitoring, assessment of head above pelvis, monitoring of contractions and plotting of cervical dilatation were usually performed. No differences before or after the PEP were detected. Where problems were detected, appropriate actions taken during labour improved, but not significantly.\"\nQuestion:\n\"Does successful completion of the Perinatal Education Programme result in improved obstetric practice?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11570976": {
                "source": [
                    "\"Sulfasalazine is a widely used anti-inflammatory agent in the treatment of inflammatory bowel disease and several rheumatological disorders. Although as many as 20% of treated patients may experience reversible, dose-dependent side effects, less frequent but potentially severe, systemic reactions have also been reported.\nA severe systemic reaction to sulfasalazine developed in a 21-year old female with rheumatoid arthritis characterized by eosinophilia, granulomatous enteritis and myelotoxicity, cholestatic hepatitis, and seizures. The clinical course and management of this patient are presented as well as a review of the incidence and outcome of severe systemic reactions to sulfasalazine.\"\nQuestion:\n\"Is it Crohn's disease?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15280782": {
                "source": [
                    "\"The number of new diagnoses of HIV infection is rising in the northwestern hemisphere and it is becoming increasingly important to understand the mechanisms behind this trend.\nTo evaluate whether reported unsafe sexual behaviour among HIV- infected individuals is changing over time.\nParticipants in the Swiss HIV Cohort Study were asked about their sexual practices every 6 months for 3 years during regular follow-up of the cohort beginning on 1 April 2000.\n: Logistic regression models were fit using generalized estimating equations assuming a constant correlation between responses from the same individual.\nAt least one sexual behaviour questionnaire was obtained for 6545 HIV-infected individuals and the median number of questionnaires completed per individual was five. There was no evidence of an increase in reported unsafe sex over time in this population [odds ratio (OR), 1.0; 95% confidence interval (CI), 0.96-1.05]. Females (OR, 1.38; 95% CI, 1.19-1.60), 15-30 year olds (OR, 1.26; 95% CI, 1.09-1.47), those with HIV-positive partners (OR, 12.58; 95% CI, 10.84-14.07) and those with occasional partners (OR, 3.25; 95% CI, 2.87-3.67) were more likely to report unsafe sex. There was no evidence of a response bias over time, but individuals were less willing to leave questions about their sexual behaviour unanswered or ambiguous (OR, 0.93; 95% CI, 0.90-0.97).\"\nQuestion:\n\"Is unsafe sexual behaviour increasing among HIV-infected individuals?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15223725": {
                "source": [
                    "\"To determine whether there are differences between blood pressure (BP) measured by the nurse (NBP), BP measured by the physician (PBP) and self-measured BP in treated hypertensive patients and, if found, to evaluate their clinical importance.\nAn observational study is carried out with hypertensive patients recruited from two village-based community health centres in Catalonia (Spain) serving an area with a total population of 2800 inhabitants. All patients treated for hypertension visiting the health centre on a specific day of the week and during the same timetable between October 2000 and May 2001 were included.\nThe difference between physician-systolic BP and nurse-systolic BP was 5.16 mmHg (95% CI 2.62-7.7; p<0.001). The difference between physician-systolic BP and self-measured systolic BP was 4.67 mmHg (95% CI 0.89-8.44; p=0.016). The differences between nurse-systolic BP and self-measured systolic BP were not significant (0.49 mmHg; 95% CI 3.71-2.71; p=0.758). With regards to diastolic BP, no significant differences were found between the different ways of measurement. NBP gave the following values: sensitivity (Sn) of 92% and specificity (Sp) of 60%; positive predictive value (PPV) of 65.7% and negative predictive value (NPV) of 90% with a positive coefficient of probability (CP+) of 2.3 and a negative coefficient of probability (CP-) of 0.133. PBP gave the following results: Sn=72%; Sp=66.7%; PPV=64.3%; NPV=74.1%; CP+=2.16 and CP- = 0.420.\"\nQuestion:\n\"Does blood pressure change in treated hypertensive patients depending on whether it is measured by a physician or a nurse?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10158597": {
                "source": [
                    "\"To evaluate the effectiveness of the role of a discharge coordinator whose sole responsibility was to plan and coordinate the discharge of patients from medical wards.\nAn intervention study in which the quality of discharge planning was assessed before and after the introduction of a discharge coordinator. Patients were interviewed on the ward before discharge and seven to 10 days after being discharged home.\nThe three medical wards at the Homerton Hospital in Hackney, East London.\n600 randomly sampled adult patients admitted to the medical wards of the study hospital, who were resident in the district (but not in institutions), were under the care of physicians (excluding psychiatry), and were discharged home from one of the medical wards. The sampling was conducted in three study phases, over 18 months.\nPhase I comprised base line data collection; in phase II data were collected after the introduction of the district discharge planning policy and a discharge form (checklist) for all patients; in phase III data were collected after the introduction of the discharge coordinator.\nThe quality and out come of discharge planning. Readmission rates, duration of stay, appropriateness of days of care, patients' health and satisfaction, problems after discharge, and receipt of services.\nThe discharge coordinator resulted in an improved discharge planning process, and there was a reduction in problems experienced by patients after discharge, and in perceived need for medical and healthcare services. There was no evidence that the discharge coordinator resulted in a more timely or effective provision of community services after discharge, or that the appropriateness or efficiency of bed use was improved.\"\nQuestion:\n\"Does a dedicated discharge coordinator improve the quality of hospital discharge?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21904069": {
                "source": [
                    "\"Knowing the collaterals is essential for a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nTo ascertain the sources of the blood supply to the spleen after a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nPerfusion of the cadaveric left gastric and right gastroepiploic arteries with methylene blue after occlusion of all the arteries except the short gastric arteries (n=10). Intraoperative color Doppler ultrasound was used for the evaluation of the hilar arterial blood flow at distal pancreatectomy (n=23) after 1) clamping of the splenic artery alone, 2) clamping of the splenic and left gastroepiploic arteries and 3) clamping of the splenic and short gastric arteries. CT angiography of the gastric and splenic vessels before and after a spleen-preserving distal pancreatectomy (n=10).\nPerfusion of the cadaveric arteries revealed no effective direct or indirect (through the submucous gastric arterial network) communication between the left gastric and the branches of the short gastric arteries. In no case did intraoperative color Doppler ultrasound detect any hilar arterial blood flow after the clamping of the splenic and left gastroepiploic arteries. The clamping of the short gastric arteries did not change the flow parameters. In none of the cases did a post-spleen-preserving distal pancreatectomy with resection of the splenic vessels CT angiography delineate the short gastric vessels supplying the spleen. In all cases, the gastroepiploic arcade was the main arterial pathway feeding the spleen.\"\nQuestion:\n\"Spleen-preserving distal pancreatectomy with resection of the splenic vessels. Should one rely on the short gastric arteries?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19145527": {
                "source": [
                    "\"The primary objective of the study was to determine emergency medical services (EMS) professionals' opinions regarding participation in disease and injury prevention programs. A secondary objective was to determine the proportion of EMS professionals who had participated in disease prevention programs.\nAs part of the National Registry of Emergency Medical Technicians' biennial reregistration process, EMS professionals reregistering in 2006 were asked to complete an optional survey regarding their opinions on and participation in disease and injury prevention. Demographic characteristics were also collected. Data were analyzed using descriptive statistics and 99% confidence intervals (CIs). The chi-square test was used to compare differences by responder demographics (alpha = 0.01). A 10% difference between groups was determined to be clinically significant.\nThe survey was completed by 27,233 EMS professionals. Of these responders, 82.7% (99% CI: 82.1-83.3) felt that EMS professionals should participate in disease prevention, with those working 20 to 29 hours per week being the least likely to think they should participate (67.4%, p<0.001). About a third, 33.8% (99% CI: 33.1-34.6), of the respondents reported having provided prevention services, with those having a graduate degree (43.5%, p<0.001), those working in EMS for more than 21 years (44%, p<0.001), those working for the military (57%, p<0.001), those working 60 to 69 hours per week (41%, p<0.001), and those responding to zero emergency calls in a typical week (43%, p<0.001) being the most likely to report having provided prevention services. About half, 51.1% (99% CI: 50.4-51.9), of the respondents agreed that prevention services should be provided during emergency calls, and 7.7% (99% CI: 7.3-8.1) of the respondents reported providing prevention services during emergency calls. No demographic differences existed. Those who had participated in prevention programs were more likely to respond that EMS professionals should participate in prevention (92% vs. 82%, p<0.001). Further, those who had provided prevention services during emergency calls were more likely to think EMS professionals should provide prevention services during emergency calls (81% vs. 51%, p<0.001).\"\nQuestion:\n\"Do emergency medical services professionals think they should participate in disease prevention?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15588538": {
                "source": [
                    "\"Reliable longitudinal data of patients with functional somatic symptoms in general practice are lacking.\nTo identify distinctive features in patients with chronic functional somatic symptoms, and to determine whether these symptoms support the hypothesis of the existence of specific somatic syndromes.\nObservational study, with a comparison control group.\nFour primary care practices affiliated with the University of Nijmegen in the Netherlands.\nOne hundred and eighty-two patients diagnosed between 1998 and 2002 as having chronic functional somatic symptoms and 182 controls matched by age, sex, socioeconomic status, and practice were included. Data on comorbidity, referrals, diagnostic tests, and hospital admissions over a period of 10 years prior to the diagnosis were collected. Medication use and number of visits to the general practitioner (GP) were extracted from the moment computerised registration was started.\nIn the 10 years before the diagnosis of chronic functional somatic symptoms, significantly more patients than controls presented functional somatic symptoms in at least two body systems, and used more somatic and psychotropic drugs. They visited the GP twice as much, statistically had significantly more psychiatric morbidity, and were referred more often to mental health workers and somatic specialists. The number of patients undergoing diagnostic tests was higher for patients with chronic functional somatic symptoms than for controls, but hospital admissions rates were equal.\"\nQuestion:\n\"Chronic functional somatic symptoms: a single syndrome?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27448572": {
                "source": [
                    "\"There is increasing pressure on mental health providers to reduce the duration of treatments, while retaining level of quality and effectiveness. The risk is that the population is underserved and therefore needs new treatment episodes. The primary aim of this study was to investigate whether duration of treatment and return into mental health care were related.\nThis study examined Dutch patients with an initial treatment episode in 2009 or 2010 in specialized mental health settings for depressive disorder (N\u00a0=\u00a085,754). Follow-up data about treatment episodes were available up until 2013. The data set included demographic (age, gender), and clinical factors (comorbidity with other DSM-IV Axis; scores on the 'Global Assessment of Functioning'). Cox regression analyses were used to assess whether duration of treatment and relapse into mental health care were related.\nThe majority of patients did not return into mental health care (86\u00a0%). Patients with a shorter duration of treatment (5-250\u00a0min; 251-500\u00a0min and 751-1000\u00a0min) were slightly more likely to return (reference group:>1000\u00a0min) (HR 1.19 95\u00a0% CI 1.13-1.26; HR 1.11 95\u00a0% CI 1.06-1.17; HR 1.18 95\u00a0% CI 1.11-1.25), adjusted for demographic and clinical variables.\"\nQuestion:\n\"Is duration of psychological treatment for depression related to return into treatment?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17916877": {
                "source": [
                    "\"To determine the therapeutic effect (alleviation of vascular type headache) and side effects of a slow intravenous metoclopramide infusion over 15 min compared with those effects of a bolus intravenous metoclopramide infusion over 2 min in the treatment of patients with recent onset vascular type headache.\nAll adults treated with metoclopramide for vascular type headache were eligible for entry into this clinical randomised double blinded trial. This study compared the effects of two different rates of intravenous infusion of metoclopramide over a period of 13 months at a university hospital emergency department. During the trial, side effects and headache scores were recorded at baseline (0 min), and then at 5, 15, 30 and 60 min. Repeated measures analysis of variance was used to compare the medication's efficacy and side effects.\nA total of 120 patients presenting to the emergency department met the inclusion criteria. Of these, 62 patients (51.7%) were given 10 mg metoclopramide as a slow intravenous infusion over 15 min (SIG group) and 58 patients (48.3%) were given 10 mg metoclopramide intravenous bolus infusion over 2 min (BIG group). 17 of the 58 patients in the BIG group (29.3%) and 4 of the 62 patients (6.5%) in the SIG group had akathisia (p = 0.001). There were no significant differences between the BIG and SIG groups in terms of mean headache scores (p = 0.34) and no adverse reactions in the study period. Metoclopramide successfully relieved the headache symptom(s) of patients in both the BIG and SIG groups.\"\nQuestion:\n\"Intravenous administration of metoclopramide by 2 min bolus vs 15 min infusion: does it affect the improvement of headache while reducing the side effects?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26383908": {
                "source": [
                    "\"This quasi-experimental study was conducted using a crossover design among two groups of total 64 nursing students. Participants were asked to create concept maps (group A) or were evaluated with the traditional method of quiz (group B) for eight weeks and then take a cumulative test (no. 1). Consequently, subjects used the alternate method for another eight weeks and then take the second cumulative test (no. 2).\nThe results of this study showed that the mean scores for cumulative tests (both no. 1 and no. 2) was higher in the group that engaged in map construction compared to the group that only take the quizzes. In addition, there was a gradual increase in the mean scores of developed map during the eight sessions of intervention.\"\nQuestion:\n\"Does concept mapping enhance learning outcome of nursing students?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21745056": {
                "source": [
                    "\"The Pathway represents a longitudinal program for medical students, consisting of both domestic and international experiences with poor populations. A previous study reported no significant attitudinal changes toward the medically indigent between Pathway and non-Pathway students.\nThe purpose of this study was to investigate and differentiate the skills and attitudes of Pathway and non-Pathway students in working with culturally diverse populations by conducting quantitative and qualitative analyses.\nSelected items from a cultural assessment were analyzed using independent t-tests and a proportional analysis using approximation of the binomial distribution. In addition, a qualitative assessment of non-Pathway and Pathway students was conducted.\nA statistically significant difference was found at the end of Years 2, 3, and 4 regarding student confidence ratings, and qualitative results had similar findings.\"\nQuestion:\n\"Global Longitudinal Pathway: has medical education curriculum influenced medical students' skills and attitudes toward culturally diverse populations?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24433626": {
                "source": [
                    "\"Medicare beneficiaries who have chronic conditions are responsible for a disproportionate share of Medicare fee-for-service expenditures. The objective of this study was to analyze the change in the health of Medicare beneficiaries enrolled in Part A (hospital insurance) between 2008 and 2010 by comparing the prevalence of 11 chronic conditions.\nWe conducted descriptive analyses using the 2008 and 2010 Chronic Conditions Public Use Files, which are newly available from the Centers for Medicare and Medicaid Services and have administrative (claims) data on 100% of the Medicare fee-for-service population. We examined the data by age, sex, and dual eligibility (eligibility for both Medicare and Medicaid).\nMedicare Part A beneficiaries had more chronic conditions on average in 2010 than in 2008. The percentage increase in the average number of chronic conditions was larger for dual-eligible beneficiaries (2.8%) than for nondual-eligible beneficiaries (1.2%). The prevalence of some chronic conditions, such as congestive heart failure, ischemic heart disease, and stroke/transient ischemic attack, decreased. The deterioration of average health was due to other chronic conditions: chronic kidney disease, depression, diabetes, osteoporosis, rheumatoid arthritis/osteoarthritis. Trends in Alzheimer's disease, cancer, and chronic obstructive pulmonary disease showed differences by sex or dual eligibility or both.\"\nQuestion:\n\"Prevalence of chronic conditions among Medicare Part A beneficiaries in 2008 and 2010: are Medicare beneficiaries getting sicker?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23539689": {
                "source": [
                    "\"To characterize the use and delivery of cough and cold medicines in children younger than 6 presenting to an inner-city pediatric emergency department (PED) following 2007 FDA warnings.\nA cross-sectional observational study was performed using a convenience sampling of PED patients during the fall of 2010. Caregivers were presented with 6 commonly used cough medicine preparations and were asked to demonstrate if and how they would administer these to their children.\nIn all, 65 patients and their caregivers consented and participated in the study. During the demonstration, 82% (53/65) stated that they would treat with cough or cold medicines, and 72% (38/53) incorrectly dosed the medication they desired to give.\"\nQuestion:\n\"Cold preparation use in young children after FDA warnings: do concerns still exist?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21431987": {
                "source": [
                    "\"This study was designed to determine prospectively whether the systematic use of PET/CT associated with conventional techniques could improve the accuracy of staging in patients with liver metastases of colorectal carcinoma. We also assessed the impact on the therapeutic strategy.\nBetween 2006 and 2008, 97 patients who were evaluated for resection of LMCRC were prospectively enrolled. Preoperative workup included multidetector-CT (MDCT) and PET/CT. In 11 patients with liver steatosis or iodinated contrast allergy, MR also was performed. Sixty-eight patients underwent laparotomy. Sensitivity, specificity, positive predictive value (PPV), and negative predictive values for hepatic and extrahepatic staging of MDCT and PET-CT were calculated.\nIn a lesion-by-lesion analysis of the hepatic staging, the sensitivity of MDCT/RM was superior to PET/CT (89.2 vs. 55%, p\u00a0<\u00a00.001). On the extrahepatic staging, PET/CT was superior to MDCT/MR only for the detection of locoregional recurrence (p\u00a0=\u00a00.03) and recurrence in uncommon sites (p\u00a0=\u00a00.016). New findings in PET/CT resulted in a change in therapeutic strategy in 17 patients. However, additional information was correct only in eight cases and wrong in nine patients.\"\nQuestion:\n\"Preoperative staging of patients with liver metastases of colorectal carcinoma. Does PET/CT really add something to multidetector CT?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24922528": {
                "source": [
                    "\"To explore the extent to which parent-adolescent emotional closeness, family conflict, and parental permissiveness moderate the association of puberty and alcohol use in adolescents (aged 10-14).\nCross-sectional survey of 7631 adolescents from 231 Australian schools. Measures included pubertal status, recent (30day) alcohol use, parent-adolescent emotional closeness, family conflict, parental permissiveness of alcohol use and peer alcohol use. The analysis was based on a two-level (individuals nested within schools) logistic regression model, with main effects entered first, and interaction terms added second.\nThe interaction of family factors and pubertal stage did not improve the fit of the model, so a main effect model of family factors and pubertal stage was adopted. There were significant main effects for pubertal stage with boys in middle puberty at increased odds of alcohol use, and girls in advanced puberty at increased odds of alcohol use.\"\nQuestion:\n\"The association of puberty and young adolescent alcohol use: do parents have a moderating role?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25103647": {
                "source": [
                    "\"To examine whether government-funded, low-income vision care programs improve use of eye care services by low-income individuals in Canada.\nCross-sectional survey.\n27,375 white respondents to the Canadian Community Health Survey (CCHS) Healthy Aging 2008/2009.\nGovernment-funded, low-income vision care programs were reviewed. The amount of assistance provided was compared with professional fee schedules for general/routine eye examinations and market prices for eyeglasses. The utilization of eye care providers was derived from the CCHS.\nTo receive low-income vision care assistance, individuals must be in receipt of social assistance. Criteria for receiving social assistance are stringent. The Canadian Financial Capability Survey revealed that 7.9% of Canadians aged 45 to 64 years and 5.5% aged \u226565 years received social assistance in 2009. The CCHS found in 2008/2009 that 12.5% of citizens aged 45 to 64 years and 13.2% of those aged \u226565 years had difficulty paying for basic expenses such as food. In 5 provinces, low-income vision care assistance fully covers a general/routine eye examination. In the remainder, the assistance provided is insufficient for a general/routine eye examination. The assistance for eyeglasses is inadequate in 5 provinces, requiring out-of-pocket copayments. Among middle-aged whites who self-reported not having glaucoma, cataracts, diabetes, or vision problems not corrected by lenses, utilization of eye care providers was 28.1% among those with financial difficulty versus 41.9% among those without (p<0.05), giving a prevalence ratio 0.68 (95% CI 0.57-0.80) adjusted for age, sex and education.\"\nQuestion:\n\"Does government assistance improve utilization of eye care services by low-income individuals?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15502995": {
                "source": [
                    "\"To analyse associations between indicators for adoption of new drugs and to test the hypothesis that physicians' early adoption of new drugs is a personal trait independent of drug groups.\nIn a population-based cohort study using register data, we analysed the prescribing of new drugs by Danish general practitioners. Angiotensin-II antagonists, triptans, selective cyclo-oxygenase-2 antagonists and esomeprazol were used in the assessment. As indicators of new drug uptake, we used adoption time, cumulative incidence, preference proportion, incidence rate and prescription cost and volume. For each measure, we ranked the general practices. Ranks were pair-wise plotted, and Pearson's correlation coefficient ( r) was calculated. Next, we analysed the correlation between ranks across different drug classes.\nFor all indicators, the general practitioners' adoption of one group of drugs was poorly associated with adoption of others ( r</=0.49), indicating that early adoption of one type of drugs is not associated with early adoption of another. For all drug groups, adoption time adjusted for practice size was only weakly associated with other indicators ( r: -0.56 to -0.27). Indicators, based on cost and volume of drugs, were highly correlated ( r: 0.96-0.99), and the others correlated reasonably well ( r: 0.51-0.91).\"\nQuestion:\n\"Does the early adopter of drugs exist?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11555508": {
                "source": [
                    "\"Some of the disagreements on the perception of dyspnea (PD) during bronchoconstriction in asthma patients could depend on the interrelationships among the following: (1) the influence of baseline airflow obstruction on the patient's ability to detect any further increase in airway resistance; (2) the effect of eosinophilic inflammation on the airway; (3) bronchial hyperresponsiveness (BHR); and (4) the effect of inhaled corticosteroids (ICSs).\nWe hypothesized that if the inflammation of the airway wall influences to some extent and in some way the PD in asthma patients, ICSs reverse the effect of airway inflammation on the PD.\nWe studied 100 asthma patients who were divided into the following four groups: patients with obstruction who were either ICS-naive (group I) or were treated with ICSs (group II); and nonobstructed patients who were either ICS-naive (group III) or were treated with ICSs (group IV). PD on the visual analog scale (VAS) was assessed during a methacholine-induced FEV(1) decrease and specifically was quantified as the VAS slope and score at an FEV(1) decrease of 5 to 20%. BHR was assessed in terms of the provocative concentration of methacholine causing a 20% fall in FEV(1) (PC(20)). Eosinophil counts in induced sputum samples also were performed. Regression analysis, univariate analysis of variance, and factor analysis were applied for statistical evaluation.\nFor a 5 to 20% fall in FEV(1) from the lowest point after saline solution induction, VAS score was lowest in group II, slightly higher in group I, slightly higher still in group IV, and the highest in group III. In the patients as a whole, BHR related to PD, but age, clinical score, duration of the disease, and presence of baseline airway obstruction did not. In patients with obstruction who were treated with ICSs, eosinophil counts related to PD negatively. Factor analysis yielded the following four factors that accounted for 70% of the variance in the data: ICS; eosinophil counts; FEV(1); and PC(20) loaded on separated factors with PD loading on the same factors as PC(20). The post hoc analysis carried out dividing the patients into ICS-treated and ICS-naive, showed that in the former group eosinophil counts and BHR proved to be factors negatively associated with PD, while in the latter group eosinophil counts were positively associated with PD.\"\nQuestion:\n\"Do inhaled corticosteroids affect perception of dyspnea during bronchoconstriction in asthma?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26536001": {
                "source": [
                    "\"It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan\u00ae in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors.\nAll 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC.\nIn total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).\"\nQuestion:\n\"Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15466981": {
                "source": [
                    "\"The combined use of free and total prostate-specific antigen (PSA) in early detection of prostate cancer has been controversial. This article systematically evaluates the discriminating capacity of a large number of combination tests.\nFree and total PSA were analyzed in stored serum samples taken prior to diagnosis in 429 cases and 1,640 controls from the Physicians' Health Study. We used a classification algorithm called logic regression to search for clinically useful tests combining total and percent free PSA and receiver operating characteristic analysis and compared these tests with those based on total and complexed PSA. Data were divided into training and test subsets. For robustness, we considered 35 test-train splits of the original data and computed receiver operating characteristic curves for each test data set.\nThe average area under the receiver operating characteristic curve across test data sets was 0.74 for total PSA and 0.76 for the combination tests. Combination tests with higher sensitivity and specificity than PSA>4.0 ng/mL were identified 29 out of 35 times. All these tests extended the PSA reflex range to below 4.0 ng/mL. Receiver operating characteristic curve analysis indicated that the overall diagnostic performance as expressed by the area under the curve did not differ significantly for the different tests.\"\nQuestion:\n\"Prostate-specific antigen and free prostate-specific antigen in the early detection of prostate cancer: do combination tests improve detection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23448747": {
                "source": [
                    "\"To examine whether a history of cancer increased the likelihood of a fall in community-dwelling older adults, and if cancer type, stage, or time since diagnosis increased falls.\nA longitudinal, retrospective, cohort study.\nA home- and community-based waiver program in Michigan.\n862 older adults aged 65 years or older with cancer compared to 8,617 older adults without cancer using data from the Minimum Data Set-Home Care and Michigan cancer registry.\nReports of falls were examined for 90-180 days. Generalized estimating equations were used to compare differences between the groups.\nCancer, falls, patient characteristics, comorbidities, medications, pain, weight loss, vision, memory recall, and activities, as well as cancer type, stage, and time since diagnosis.\nA fall occurred at a rate of 33% in older adults with cancer compared to 29% without cancer (p<0.00). Those with a history of cancer were more likely to fall than those without cancer (adjusted odds ratio 1.16; 95% confidence interval [1.02, 1.33]; p = 0.03). No differences in fall rates were determined by cancer type or stage, and the odds of a fall did not increase when adding time since cancer diagnosis.\"\nQuestion:\n\"Do older adults with cancer fall more often?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18607272": {
                "source": [
                    "\"To compare children's, parents' and physicians' perceptions of children's body size.\nWe administered a structured questionnaire of body size perception using a descriptive Likert scale keyed to body image figures to children ages 12 to 18 years. The same scale was given to parents of children ages 5 to 18 years. The sample consisted of 91 children and their parents being seen in the Pediatric Gastroenterology Clinic for concerns unrelated to overweight. Weight and height of the children were measured, and body mass index (BMI) was calculated. The children's BMI percentiles were categorized as underweight (<15th), normal (15th-85th), overweight (85th-95th), and obese (95th and above). The attending physician independently completed the body image and description scale and indicated the figure that most accurately represented the patient without reference to BMI standards. Accuracy of the patients', parents', and doctors' estimates were statistically compared.\nThe sample population consisted of 6.4% underweight, 70.5% normal weight, 7.7% overweight, and 15.4% obese. Forty-four percent of parents underestimated children's body size using word descriptions and 47% underestimated using figures. Forty percent of the children underestimated their own body size using descriptions and 43% underestimated using figures. The physicians in this study had a higher percentage of correct estimates; however, they underestimated 33% of the patients using both word descriptions and figures. Some obese children were not recognized, and several average children were perceived as underweight.\"\nQuestion:\n\"Body perception: do parents, their children, and their children's physicians perceive body image differently?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16253970": {
                "source": [
                    "\"Controlled ovarian stimulation (COS) with intrauterine insemination (IUI) is a common treatment in couples with unexplained non-conception. Induction of multifollicular growth is considered to improve pregnancy outcome, but it contains an increased risk of multiple pregnancies and ovarian hyperstimulation syndrome. In this study the impact of the number of follicles (>14 mm) on the ongoing pregnancy rate (PR) and multiple PR was evaluated in the first four treatment cycles.\nA retrospective cohort study was performed in all couples with unexplained non-conception undergoing COS-IUI in the Academic Hospital of Maastricht. The main outcome measure was ongoing PR. Secondary outcomes were ongoing multiple PR, number of follicles of>or=14 mm, and order of treatment cycle.\nThree hundred couples were included. No significant difference was found in ongoing PR between women with one, two, three or four follicles respectively (P=0.54), but in women with two or more follicles 12/73 pregnancies were multiples. Ongoing PR was highest in the first treatment cycle and declined significantly with increasing cycle order (P=0.006), while multiple PR did not change.\"\nQuestion:\n\"Is controlled ovarian stimulation in intrauterine insemination an acceptable therapy in couples with unexplained non-conception in the perspective of multiple pregnancies?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22521460": {
                "source": [
                    "\"The route of delivery in eclampsia is controversial. We hypothesized that adverse maternal and perinatal outcomes may not be improved by early cesarean delivery.\nThis was a randomized controlled exploratory trial carried out in a rural teaching institution. In all, 200 eclampsia cases, carrying \u226534 weeks, were allocated to either cesarean or vaginal delivery. Composite maternal and perinatal event rates (death and severe morbidity) were compared by intention-to-treat principle.\nGroups were comparable at baseline with respect to age and key clinical parameters. Maternal event rate was similar: 10.89% in the cesarean arm vs 7.07% for vaginal delivery (relative risk, 1.54; 95% confidence interval, 0.62-3.81). Although the neonatal event rate was less in cesarean delivery-9.90% vs 19.19% (relative risk, 0.52; 95% confidence interval, 0.25-1.05)-the difference was not significant statistically.\"\nQuestion:\n\"Does route of delivery affect maternal and perinatal outcome in women with eclampsia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22768311": {
                "source": [
                    "\"Recent studies have implicated the human cytomegalovirus (HCMV) as a possible pathogen for causing hypertension. We aimed to study the association between HCMV infection and hypertension in the United States National Health and Nutrition Examination Survey (NHANES).\nWe analyzed data on 2979 men and 3324 women in the NHANES 1999-2002. We included participants aged 16-49 years who had valid data on HCMV infection and hypertension.\nOf the participants, 54.7% had serologic evidence of HCMV infection and 17.5% had hypertension. There were ethnic differences in the prevalence of HCMV infection (P<0.001) and hypertension (P<0.001). The prevalence of both increased with age (P<0.001). Before adjustment, HCMV seropositivity was significantly associated with hypertension in women (OR=1.63, 95% CI=1.25-2.13, P=0.001) but not in men. After adjustment for race/ethnicity, the association between HCMV seropositivity and hypertension in women remained significant (OR=1.55, 95% CI=1.20-2.02, P=0.002). Further adjustment for body mass index, diabetes status and hypercholesterolemia attenuated the association (OR=1.44, 95% CI=1.10-1.90, P=0.010). However, after adjusting for age, the association was no longer significant (OR=1.24, 95% CI=0.91-1.67, P=0.162).\"\nQuestion:\n\"Is human cytomegalovirus infection associated with hypertension?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28056802": {
                "source": [
                    "\"It has recently been shown that non-high density lipoprotein cholesterol (non-HDL-C) may be a better predictor of cardiovascular risk than low density lipoprotein cholesterol (LDL-C). Based on known ethic differences in lipid parameters and cardiovascular risk prediction, we sought to study the predictability of attaining non-HDL-C target and long-term major adverse cardiovascular event (MACE) in Thai patients after acute myocardial infarction (AMI) compared to attaining LDL-C target.\nWe retrospectively obtained the data of all patients who were admitted at Maharaj Nakorn Chiang Mai hospital due to AMI during 2006-2013. The mean non-HDL-C and LDL-C during long-term follow-up were used to predict MACE at each time point. The patients were classified as target attainment if non-HDL-C\u2009<100\u00a0mg/dl and/or LDL-C\u2009<70\u00a0mg/dl. The MACE was defined as combination of all-cause death, nonfatal coronary event and nonfatal stroke.\nDuring mean follow-up of 2.6\u2009\u00b1\u20091.6\u00a0years among 868 patients after AMI, 34.4% achieved non-HDL-C target, 23.7% achieved LDL-C target and 21.2% experienced MACEs. LDL-C and non-HDL-C were directly compared in Cox regression model. Compared with non-HDL-C\u2009<100\u00a0mg/dl, patients with non-HDL-C of>130\u00a0mg/dl had higher incidence of MACEs (HR 3.15, 95% CI 1.46-6.80, P\u2009=\u20090.003). Surprisingly, LDL-C\u2009>100\u00a0mg/dl was associated with reduced risk of MACE as compared to LDL\u2009<70\u00a0mg/dl (HR 0.42, 95% CI 0.18-0.98, p\u2009=\u20090.046) after direct pairwise comparison with non-HDL-C level.\"\nQuestion:\n\"Is non-HDL-cholesterol a better predictor of long-term outcome in patients after acute myocardial infarction compared to LDL-cholesterol?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24434052": {
                "source": [
                    "\"The last 20 years has seen a marked improvement in skin cancer awareness campaigns. We sought to establish whether this has affected the presenting Breslow thickness of malignant melanoma in the South West.\nThis is a retrospective study looking at the first presentation of melanomas from 2003 to 2011. Data was accessed using the local online melanoma database.\nA total of 2001 new melanomas presented from 2003 to 2012 (Male:Female = 1:1.062). The average yearly number of melanomas was 200.1 (range = 138-312). The mean age was 62.5 years (range 12-99). Data was analysed using a Chi\u00b2 test. For 0-1 mm melanomas, there is a significant difference in the observed versus expected values over the 10 years (p = 0.0018). There is an increasing proportion of 0-1 mm (thin) melanomas presenting year on year, with a positive linear trend. This is very statistically significant (p<0.0001). The 1-2 mm melanomas are decreasing in proportion with a negative linear trend (p = 0.0013). The 2-4 mm are also decreasing in proportion (p = 0.0253). There is no significant change in the thick>4 mm melanomas (p = 0.1456).\"\nQuestion:\n\"Are we seeing the effects of public awareness campaigns?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27131771": {
                "source": [
                    "\"The alterations of echocardiography and electrocardiogram (ECG) in patients received left atrial appendage LAA occlusion therapy are still unclear. The present study was to evaluate the influence of LAA occlusion device on echocardiography and ECG changes in patients with atrial fibrillation (AF).\nSeventy-three patients who had undergone Watchman, LAmbre and Lefort were enrolled in this study. Echocardiography and ECG results at pre- and post-operation were collected. Besides, echocardiography was also performed during follow-up visits at 1, 6 and 12months after discharge.\nAfter LAA occlusion, a slight and measureable movement of QRS electric axis was observed in most patients. The significant differences were also observed in heart rate (HR) and the mean-mean QT interval between pre- and post-operation for all patients. There existed no significant difference in echocardiographic parameters between before and after device implantation. However, a larger left atrial (LA) diameter was detected by echocardiography during follow-up visit at 6months when compared with pre-operation parameters. Similarly, aortic root diameter (ARD) was also larger during follow-up at 12months than the baseline dimension in pre-operation.\"\nQuestion:\n\"Does left atrial appendage (LAA) occlusion device alter the echocardiography and electrocardiogram parameters in patients with atrial fibrillation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16909975": {
                "source": [
                    "\"Dryness of the mouth is one of the most distressing chronic toxicities of radiation therapy in head and neck cancers. In this study, parotid function was assessed in patients with locally advanced head and neck cancers undergoing intensity-modulated radiotherapy (IMRT) with or without chemotherapy. Parotid function was assessed with the help of a questionnaire and parotid scintigraphy, especially with regards to unilateral sparing of the parotid gland.\nIn total, 19 patients were treated with compensator-based IMRT between February 2003 and March 2004. The dose to the clinical target volume ranged between 66 and 70 Gy in 30-35 fractions to 95% of the isodose volume. Ipsilateral high-risk neck nodes received an average dose of 60 Gy and the contralateral low-risk neck received a dose of 54-56 Gy. Eight of 19 patients also received concomitant chemotherapy.\nSubjective toxicity to the parotid glands was assessed with the help of a questionnaire at 0, 3 and 6 months and objective toxicity was assessed with parotid scintigraphy at 0 and 3 months. The mean dose to the ipsilateral parotid gland ranged from 19.5 to 52.8 Gy (mean 33.14 Gy) and the mean dose to the contralateral gland was 11.1-46.6 Gy (mean 26.85 Gy). At a median follow-up of 13 months, 9/19 patients had no symptoms of dryness of the mouth (grade I), 8/19 had mild dryness of the mouth (grade II) and only 2/19 had grade III xerostomia, although the parotid gland could only be spared on one side in most of the patients.\"\nQuestion:\n\"Can dose reduction to one parotid gland prevent xerostomia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9363244": {
                "source": [
                    "\"To determine the effect of occupational exposure in a nuclear power plant in Griefswald, Germany on male and female fecundity.\nThe frequency of men and women exposed to ionizing radiation through work in a nuclear power plant among 270 infertile couples was retrospectively compared to a control fertile population using a pair-matched analysis. The total cumulative equivalent radiation dose was determined. In addition, the spermiograms of the male partners in both groups were compared and correlated to the degree of exposure.\nNo differences were noted in the frequency of nuclear power plant exposure between sterile and fertile groups. There was a higher rate of anomalous spermiograms in nuclear power plant workers. However, abnormalities were temporary. No correlation was found between the cumulative equivalent radiation dose and abnormal spermiograms.\"\nQuestion:\n\"Does occupational nuclear power plant radiation affect conception and pregnancy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18239988": {
                "source": [
                    "\"Specific markers for differentiation of nonalcoholic (NASH) from alcoholic steatohepatitis (ASH) are lacking. We investigated the role of routine laboratory parameters in distinguishing NASH from ASH.\nLiver biopsies performed at our hospital over a 10-year period were reviewed, 95 patients with steatohepatitis identified and their data prior to biopsy reevaluated. The diagnosis NASH or ASH was assigned (other liver diseases excluded) on the basis of the biopsy and history of alcohol consumption (<140 g/week). Logistic regression models were used for analysis.\nNASH was diagnosed in 58 patients (61%; 30 f) and ASH in 37 (39%; 9 f). High-grade fibrosis (59% vs. 19%, P<0.0001) and an AST/ALT ratio>1 (54.1% vs 20.7%, P = 0.0008) were more common in ASH. The MCV was elevated in 53% of ASH patients and normal in all NASH patients (P<0.0001). Multivariate analysis identified the MCV (P = 0.0013), the AST/ALT ratio (P = 0.011) and sex (P = 0.0029) as relevant regressors (aROC = 0.92). The AST/ALT ratio (P<0.0001) and age (P = 0.00049) were independent predictors of high-grade fibrosis. Differences in MCV were more marked in high-grade fibrosis.\"\nQuestion:\n\"Differentiation of nonalcoholic from alcoholic steatohepatitis: are routine laboratory markers useful?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22154448": {
                "source": [
                    "\"To study the risks of haemodynamic instability, and the possible occurrence of spinal haematoma, meningitis and epidural abscess when epidural analgesia is performed for cytoreductive surgery and hyperthermic intraperitoneal chemotherapy (HIPEC).\nWe retrospectively analyzed the data of 35 patients treated by HIPEC with oxaliplatin or cisplatin. An epidural catheter was inserted before induction of general anaesthesia. Postoperatively, a continuous epidural infusion of ropivacain, then a patient-controlled epidural analgesia were started.\nThe epidural catheter was used peroperatively before HIPEC in 12 subjects (34%), and after HIPEC in 23 subjects (66%). The median dose of ropivacain given peroperatively in the epidural catheter was 40 mg (30-75). Norepinephrin was used in two subjects (6%) peroperatively (median infusion rate 0.325 \u03bcg/kg per minute [0.32-0.33]), and in four subjects (11%) in the postoperative 24 hours. No spinal haematoma, meningitis or epidural abscess were noted. Five subjects (14%) had a thrombopenia or a prothrombin time less than 60% before catheter removal. Two subjects (6%) had a leukopenia before catheter removal. No thrombopenia or blood coagulation disorders were recorded the day of catheter removal.\"\nQuestion:\n\"Epidural analgesia for surgical treatment of peritoneal carcinomatosis: a risky technique?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12145243": {
                "source": [
                    "\"Type 2 diabetes may be present for several years before diagnosis, by which time many patients have already developed diabetic complications. Earlier detection and treatment may reduce this burden, but evidence to support this approach is lacking.\nGlycemic control and clinical and surrogate outcomes were compared for 5,088 of 5,102 U.K. Diabetes Prospective Study participants according to whether they had low (<140 mg/dl [<7.8 mmol/l]), intermediate (140 to<180 mg/dl [7.8 to<10.0 mmol/l]), or high (>or =180 mg/dl [>or =10 mmol/l]) fasting plasma glucose (FPG) levels at diagnosis. Individuals who presented with and without diabetic symptoms were also compared.\nFewer people with FPG in the lowest category had retinopathy, abnormal biothesiometer measurements, or reported erectile dysfunction. The rate of increase in FPG and HbA(1c) during the study was identical in all three groups, although absolute differences persisted. Individuals in the low FPG group had a significantly reduced risk for each predefined clinical outcome except stroke, whereas those in the intermediate group had significantly reduced risk for each outcome except stroke and myocardial infarction. The low and intermediate FPG groups had a significantly reduced risk for progression of retinopathy, reduction in vibration sensory threshold, or development of microalbuminuria.\"\nQuestion:\n\"Are lower fasting plasma glucose levels at diagnosis of type 2 diabetes associated with improved outcomes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24577079": {
                "source": [
                    "\"Older adults typically perform worse on measures of working memory (WM) than do young adults; however, age-related differences in WM performance might be reduced if older adults use effective encoding strategies.\nThe purpose of the current experiment was to evaluate WM performance after training individuals to use effective encoding strategies.\nParticipants in the training group (older adults: n = 39; young adults: n = 41) were taught about various verbal encoding strategies and their differential effectiveness and were trained to use interactive imagery and sentence generation on a list-learning task. Participants in the control group (older: n = 37; young: n = 38) completed an equally engaging filler task. All participants completed a pre- and post-training reading span task, which included self-reported strategy use, as well as two transfer tasks that differed in the affordance to use the trained strategies - a paired-associate recall task and the self-ordered pointing task.\nBoth young and older adults were able to use the target strategies on the WM task and showed gains in WM performance after training. The age-related WM deficit was not greatly affected, however, and the training gains did not transfer to the other cognitive tasks. In fact, participants attempted to adapt the trained strategies for a paired-associate recall task, but the increased strategy use did not benefit their performance.\"\nQuestion:\n\"Does strategy training reduce age-related deficits in working memory?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17890090": {
                "source": [
                    "\"The aim of this study was to determine whether bone scans (BS) can be avoided if pelvis was included in CT thorax and abdomen to detect bony metastases from breast cancer.\nResults of 77 pairs of CT (thorax, abdomen, and pelvis) and BS in newly diagnosed patients with metastatic breast cancer (MBC) were compared prospectively for 12 months. Both scans were blindly assessed by experienced radiologists and discussed at multidisciplinary team meetings regarding the diagnosis of bone metastases.\nCT detected metastatic bone lesions in 43 (98%) of 44 patients with bone metastases. The remaining patient had a solitary, asymptomatic bony metastasis in shaft of femur. BS was positive in all patients with bone metastases. There were 11 cases of false positive findings on BS.\"\nQuestion:\n\"Can computerised tomography replace bone scintigraphy in detecting bone metastases from breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22644412": {
                "source": [
                    "\"To evaluate retrospectively whether technical factors of hepatic arterial embolization affect the prognosis of patients with hepatocellular carcinoma (HCC).\nInclusion criteria of this study were the following: (1) patients received embolization as the initial treatment during 2003-2004, (2) Child A or B liver profile, (3) five or fewer HCCs with maximum diameter of 7 cm or smaller, and (4) no extrahepatic metastasis. Patient data were gathered from 43 centers. Prognostic factors were evaluated using univariate and multivariate analyses.\nEight hundred fifteen patients were enrolled. The 1-, 3-, 5-, and 7-year overall survival rates were 92.0 % (95 % CI 90.1-93.9), 62.9 % (95 % CI 59.3-66.6), 39.0 % (95 % CI 35.1-43.0), and 26.7 % (95 % CI 22.6-30.8) in all patients. Univariate analysis showed a Child-Pugh class-A, alpha-fetoprotein level lower than 100 ng/ml, tumor size of 3 cm or smaller, tumor number of 3 or fewer, one-lobe tumor distribution, nodular tumor type, within the Milan criteria, stage I or II, no portal venous invasion, use of iodized oil, and selective embolization were significantly better prognostic factors. In the multivariate Cox model, the benefit to survival of selective embolization remained significant (hazard ratio 0.68; 95 % CI 0.48-0.97; p = 0.033).\"\nQuestion:\n\"Hepatic arterial embolization for unresectable hepatocellular carcinomas: do technical factors affect prognosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22522271": {
                "source": [
                    "\"Forty obese patients with T2DM without clinical features of Cushing's syndrome were recruited. Plasma, urinary and salivary cortisol were measured directly by an enzyme-linked immunosorbent assay using monoclonal antibodies. The specificities of the three tests using various cutoffs were calculated and compared, employing the assumption that none of the patients had hypercortisolism.\nThe patients had a mean age and BMI of 56 years (range 31-75) and 37 kg/m\u00b2 (31-56) respectively. All 40 provided late-night salivary cortisol samples. Thirty-eight patients completed all three tests. Two patients only completed two screening tests. The specificities of late-night salivary cortisol (cutoff 10 nmol/L), 24hr UFC (400 nmol) and 1mg DST (50 nmol/L) were 70% (95% CI 53-83%), 90% (76-97%) and 72% (55-85%) respectively. The specificity of late-night salivary cortisol was significantly less than 24 hr UFC (P=0.039) but not 1mg DST (P>0.99).\"\nQuestion:\n\"Is late-night salivary cortisol a better screening test for possible cortisol excess than standard screening tests in obese patients with Type 2 diabetes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18403945": {
                "source": [
                    "\"Both medical therapy and laparoscopic antireflux surgery have been shown to improve quality of life in gastro-oesophageal reflux disease. Although patients with poor symptom control or side effects on medical therapy might be expected to have improved quality of life after surgery, our aim was to determine, for the first time, whether patients whose symptoms are well controlled on medical therapy but who decide to undergo surgery (patient preference) would experience improved quality of life.\nRetrospective analysis of our patient database (1998-2003, n=313) identified 60 patients who underwent laparoscopic antireflux surgery for the indication of patient preference. Two generic quality-of-life questionnaires (Short Form 36 and Psychological General Well-Being index) and a gastrointestinal symptom questionnaire (Gastrointestinal Symptom Rating Scale) were completed preoperatively, while on medical therapy, and 6 months after surgery.\nThirty-eight patients completed all three questionnaires at both time intervals: 31 males, seven females; mean age 42 (15-66) years. Preoperative scores while on medical therapy were significantly improved after surgery: Short Form 36 median physical composite scores 52.0 and 54.0 (P=0.034) and mental composite scores 51.0 and 56.0 (P=0.020); Psychological General Well-Being median total scores 78.0 and 90.0 (P=0.0001); Gastrointestinal Symptom Rating Scale median total scores 2.13 and 1.73 (P=0.0007) and reflux scores 2.50 and 1.00 (P<0.0001).\"\nQuestion:\n\"Does laparoscopic antireflux surgery improve quality of life in patients whose gastro-oesophageal reflux disease is well controlled with medical therapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26163474": {
                "source": [
                    "\"Sublingual varices have earlier been related to ageing, smoking and cardiovascular disease. The aim of this study was to investigate whether sublingual varices are related to presence of hypertension.\nIn an observational clinical study among 431 dental patients tongue status and blood pressure were documented. Digital photographs of the lateral borders of the tongue for grading of sublingual varices were taken, and blood pressure was measured. Those patients without previous diagnosis of hypertension and with a noted blood pressure \u2265 140 mmHg and/or \u2265 90 mmHg at the dental clinic performed complementary home blood pressure during one week. Those with an average home blood pressure \u2265 135 mmHg and/or \u2265 85 mmHg were referred to the primary health care centre, where three office blood pressure measurements were taken with one week intervals. Two independent blinded observers studied the photographs of the tongues. Each photograph was graded as none/few (grade 0) or medium/severe (grade 1) presence of sublingual varices. Pearson's Chi-square test, Student's t-test, and multiple regression analysis were applied. Power calculation stipulated a study population of 323 patients.\nAn association between sublingual varices and hypertension was found (OR = 2.25, p<0.002). Mean systolic blood pressure was 123 and 132 mmHg in patients with grade 0 and grade 1 sublingual varices, respectively (p<0.0001, CI 95 %). Mean diastolic blood pressure was 80 and 83 mmHg in patients with grade 0 and grade 1 sublingual varices, respectively (p<0.005, CI 95 %). Sublingual varices indicate hypertension with a positive predictive value of 0.5 and a negative predictive value of 0.80.\"\nQuestion:\n\"Is there a connection between sublingual varices and hypertension?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23359100": {
                "source": [
                    "\"Heterotopic ossification is a common complication after total hip arthroplasty. Non-steroidal anti-inflammatory drugs (NSAIDs) are known to prevent heterotopic ossifications effectively, however gastrointestinal complaints are reported frequently. In this study, we investigated whether etoricoxib, a selective cyclo-oxygenase-2 (COX-2) inhibitor that produces fewer gastrointestinal side effects, is an effective alternative for the prevention of heterotopic ossification.\nWe investigated the effectiveness of oral etoricoxib 90 mg for seven days in a prospective two-stage study design for phase-2 clinical trials in a small sample of patients (n\u2009=\u200942). A cemented primary total hip arthroplasty was implanted for osteoarthritis. Six months after surgery, heterotopic ossification was determined on anteroposterior pelvic radiographs using the Brooker classification.\nNo heterotopic ossification was found in 62 % of the patients that took etoricoxib; 31 % of the patients had Brooker grade 1 and 7 % Brooker grade 2 ossification.\"\nQuestion:\n\"Is etoricoxib effective in preventing heterotopic ossification after primary total hip arthroplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23240452": {
                "source": [
                    "\"In recent years the role of trace elements in lithogenesis has received steadily increasing attention.\nThis study was aimed to attempt to find the correlations between the chemical content of the stones and the concentration of chosen elements in the urine and hair of stone formers.\nThe proposal for the study was approved by the local ethics committee. Specimens were taken from 219 consecutive stone-formers. The content of the stone was evaluated using atomic absorption spectrometry, spectrophotometry, and colorimetric methods. An analysis of 29 elements in hair and 21 elements in urine was performed using inductively coupled plasma-atomic emission spectrometry.\nOnly a few correlations between the composition of stones and the distribution of elements in urine and in hair were found. All were considered incidental.\"\nQuestion:\n\"Can we predict urinary stone composition based on an analysis of microelement concentration in the hair and urine?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18843057": {
                "source": [
                    "\"One of the problems with manual resuscitators is the difficulty in achieving accurate volume delivery. The volume delivered to the patient varies by the physical characteristics of the person and method. This study was designed to compare tidal volumes delivered by the squeezing method, physical characteristics and education and practice levels.\n114 individuals trained in basic life support and bag-valve-mask ventilation participated in this study. Individual characteristics were obtained by the observer and the education and practice level were described by the subjects. Ventilation was delivered with a manual resuscitator connected to a microspirometer and volumes were measured. Subjects completed three procedures: one-handed, two-handed and two-handed half-compression.\nThe mean (standard deviation) volumes for the one-handed method were 592.84 ml (SD 117.39), two-handed 644.24 ml (SD 144.7) and two-handed half-compression 458.31 ml (SD 120.91) (p<0.01). Tidal volume delivered by two hands was significantly greater than that delivered by one hand (r = 0.398, p<0.01). The physical aspects including hand size, volume and grip power had no correlation with the volume delivered. There were slight increases in tidal volume with education and practice, but correlation was weak (r = 0.213, r = 0.281, r = 0.131, p<0.01).\"\nQuestion:\n\"Can you deliver accurate tidal volume by manual resuscitator?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22188074": {
                "source": [
                    "\"To investigate whether problems in instrumental activities of daily living (IADL) can add to conventionally used clinical measurements in helping to predict a diagnosis of dementia at 1- and 2-year follow-up.\nMulticenter prospective cohort study.\nMemory clinics in Europe.\nIndividuals aged 55 and older without dementia.\nIADLs were measured using pooled activities from five informant-based questionnaires. Structural equation modeling (SEM) was used to investigate the relation between IADLs and dementia. Age, sex, education, depression, and cognitive measures (Mini-Mental State Examination and verbal memory) were included in the model.\nFive hundred thirty-one participants had baseline and 1-year follow-up assessments; 69 (13.0%) of these had developed dementia at 1-year follow-up. At 2-year follow-up, 481 participants were seen, of whom 100 (20.8%) had developed dementia. Participants with IADL disabilities at baseline had a higher conversion rate (24.4%) than participants without IADL disabilities (16.7%) (chi-square\u00a0=\u00a04.28, degrees of freedom\u00a0=\u00a01, P\u00a0=\u00a0.04). SEM showed that IADL disability could help predict dementia in addition to the measured variables at 1-year follow-up (odds ratio (OR)\u00a0=\u00a02.20, 95% confidence interval (CI)\u00a0= 1.51-3.13) and 2-year follow-up (OR\u00a0=\u00a02.11, 95% CI\u00a0=\u00a01.33-3.33).\"\nQuestion:\n\"Do instrumental activities of daily living predict dementia at 1- and 2-year follow-up?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11970923": {
                "source": [
                    "\"The prevalence of retinal haemorrhages after convulsions is not well established. As these haemorrhages are considered characteristic of child abuse, we investigated their occurrence after convulsive episodes to see whether the finding of haemorrhage should prompt further investigation.\nProspective study of 153 children (aged 2 months to 2 years), seen in the emergency department after a convulsive episode. After a thorough history and physical examination, a retinal examination was performed by an ophthalmologist. If findings were positive, further investigation was undertaken to rule out systemic disorder or child abuse.\nOne child was found with unilateral retinal haemorrhages following an episode of a simple febrile convulsion. A thorough investigation uncovered no other reason for this finding.\"\nQuestion:\n\"Convulsions and retinal haemorrhage: should we look further?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18274917": {
                "source": [
                    "\"To investigate whether low-tone SD was a precursor of Meniere's disease and whether patients with low-tone SD suffered from endolymphatic hydrops.\nThis was a retrospective case review in the university hospital. A total of 184 patients with low-tone SD were divided into two groups with single and recurrent episodes. The progress, follow-up audiograms, and ECochG results of the patients were reviewed and compared with those of patients with high-tone SD and Meniere's disease.\nIn all, 83 of 177 patients with low-tone SD unaccompanied by vertigo had recurrent hearing loss; 15 of the 83 developed vertiginous attacks. The remaining 94 patients had a single episode. Three of the seven patients with low-tone SD accompanied by vertigo had recurrent hearing loss; two of the three were subsequently confirmed to have Meniere's disease. The other four had a single episode. No difference in rate of progress from SD to Meniere's disease was observed among the low-tone and the high-tone SD groups. The average -SP/AP of each group with a single episode is smaller than that of other groups with recurrent episodes and Meniere's disease.\"\nQuestion:\n\"Prognosis of low-tone sudden deafness - does it inevitably progress to Meniere's disease?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8921484": {
                "source": [
                    "\"After 34 weeks gestation, summary measures of location for birthweight (e.g means and centiles) increase more slowly for Australian Aborigines than for whites. A similar pattern has been observed for blacks in the US. This study tests whether the reported pattern is due to differential misclassification of gestational age.\nSimulation was used to measure the potential effect of differential misclassification of gestational age. Reported gestational age data were obtained from Queensland Perinatal Data Collection (QPDC). Estimates of the true distributions of gestational age were obtained by assuming various (plausible) types of misclassification and applying these to the reported distributions. Previous studies and data from the QPDC were used to help specify the birthweight distributions used in the simulations.\nAt full term, the parameters of the birthweight distributions were robust to gestational age misclassification. At preterm, the 10th centiles were robust to misclassification. In contrast, the 90th centiles were sensitive to even minor misclassification. Extreme types of misclassification were required to remove the divergence in median birthweights for Aborigines and whites.\"\nQuestion:\n\"Does gestational age misclassification explain the difference in birthweights for Australian aborigines and whites?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15223779": {
                "source": [
                    "\"This study was conducted to investigate the expression and functional impact of the proto-oncogene c-kit in uveal melanoma.\nBased on immunohistochemical (IHC) study of paraffin-embedded specimens from 134 uveal melanomas and Western blot analysis on eight fresh-frozen samples the expression of c-kit in uveal melanoma was studied. Furthermore, the phosphorylation of c-kit and the impact of the tyrosine kinase inhibitor STI571 was examined in the three uveal melanoma cell lines OCM-1, OCM-3, and 92-1.\nEighty-four of 134 paraffin-embedded samples and six of eight fresh-frozen samples expressed c-kit. c-Kit was strongly expressed and tyrosine phosphorylated in cultured uveal melanoma cells compared with cutaneous melanoma cells. Moreover, in contrast to cutaneous melanoma cell lines c-kit maintained a high phosphorylation level in serum-depleted uveal melanoma cells. No activation-related mutations in exon 11 of the KIT gene were found. On the contrary, expression of the stem cell growth factor (c-kit ligand) was detected in all three uveal melanoma cell lines, suggesting the presence of autocrine (paracrine) stimulation pathways. Treatment of uveal melanoma cell lines with STI571, which blocks c-kit autophosphorylation, resulted in cell death. The IC(50) of the inhibitory effects on c-kit phosphorylation and cell proliferation was of equal size and less than 2.5 microM.\"\nQuestion:\n\"c-Kit-dependent growth of uveal melanoma cells: a potential therapeutic target?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17715311": {
                "source": [
                    "\"The purpose of this study was to evaluate the clinical usefulness of a fetal anatomic survey on follow-up antepartum sonograms.\nA retrospective follow-up study was conducted at a low-risk maternity clinic from July 1, 2005, to June 30, 2006. Eligible women had at least 1 prior sonographic examination beyond 18 weeks' gestation with a complete and normal fetal anatomic assessment and at least 1 follow-up sonogram. Full fetal anatomic surveys were performed on all follow-up sonograms regardless of the indication. Neonatal charts were reviewed for those patients whose follow-up sonograms revealed unanticipated fetal anomalies. Neonatal intervention was defined as surgical or medical therapy or arranged subspecialty follow-up specifically for the suspected fetal anomaly.\nOf a total of 4269 sonographic examinations performed, 437 (10.2%) were follow-up studies. Of these, 101 (23.1%) were excluded because the initial sonogram revealed a suspected fetal anomaly, and 42 (9.8%) were excluded for other reasons. Of the remaining 294 women, 21 (7.1%) had an unanticipated fetal anomaly, most often renal pyelectasis. Compared with follow-up sonography for other reasons, repeated sonography for fetal growth evaluation yielded a higher incidence of unexpected fetal anomalies: 15 (12.3%) of 122 versus 6 (3.5%) of 172 (P = .01). When compared with the neonates in the nongrowth indications group, those neonates whose mothers had sonographic examinations for fetal growth had a higher rate of neonatal interventions: 6 (40.0%) of 15 versus 0 (0%) of 6 (P = .04).\"\nQuestion:\n\"Is fetal anatomic assessment on follow-up antepartum sonograms clinically useful?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14551704": {
                "source": [
                    "\"Communication with terminally ill patients is a main responsibility of physicians. However, many physicians feel insufficiently prepared for this task. Models of courses resulting in improvements of communicative skills of participants have been published mainly in the Anglo-American literature. This study describes the realization of a 2-day course model based on the experiences of the first three courses of this kind in Rhineland-Palatinate, and analyzes changes of participants' communication behavior.\nAfter each seminary, an evaluation form concerning participants' satisfaction with the course was filled in. Furthermore, all course participants received a questionnaire at the beginning and at the end of the course, as well as 3 months afterwards. The participants were asked to assess their own sense of security in seven different communication settings on a visual analog scale, and to specify perceived changes in their communication behavior 3 months after the course.\nThe first three courses were attended by 31 participants. Course evaluation revealed high satisfaction scores with methods as well as with clarity and relevance of the contents. Self-assessment of participants showed a growing sense of security in different communication settings. Important increases could be demonstrated for communicating a diagnosis of cancer with good or less good prognosis, recurrence of cancer or a far progressive cancer disease without curative approach. 3 months after the course, participants described multiple changes indicating increased sensibility and professionalism in communication behavior.\"\nQuestion:\n\"Can communication with terminally ill patients be taught?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23899611": {
                "source": [
                    "\"Twenty-eight female Sprague Dawley rats were allocated randomly to 4 groups. The sham group (group 1) was only subjected to catheter insertion, not to pneumoperitoneum. Group 2 received a 1 mg/kg dose of 0.9% sodium chloride by the intraperitoneal route for 10 min before pneumoperitoneum. Groups 3 and 4 received 6 and 12 mg/kg edaravone, respectively, by the intraperitoneal route for 10 min before pneumoperitoneum. After 60 min of pneumoperitoneum, the gas was deflated. Immediately after the reperfusion period, both ovaries were excised for histological scoring, caspase-3 immunohistochemistry and biochemical evaluation including glutathione (GSH) and malondialdehyde (MDA) levels. Also, total antioxidant capacity (TAC) was measured in plasma samples to evaluate the antioxidant effect of edaravone.\nOvarian sections in the saline group revealed higher scores for follicular degeneration and edema (p<0.0001) when compared with the sham group. Administration of different doses of edaravone in rats significantly prevented degenerative changes in the ovary (p<0.0001). Caspase-3 expression was only detected in the ovarian surface epithelium in all groups, and there was a significant difference between the treatment groups and the saline group (p<0.0001). Treatment of rats with edaravone reduced caspase-3 expression in a dose-dependent manner. Moreover, biochemical measurements of oxidative stress markers (MDA, GSH and TAC) revealed that prophylactic edaravone treatment attenuated oxidative stress induced by I/R injury.\"\nQuestion:\n\"Attenuation of ischemia/reperfusion-induced ovarian damage in rats: does edaravone offer protection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25446909": {
                "source": [
                    "\"To describe clinical characteristics of oral mucoceles/ranulas, with a focus on human immunodeficiency virus (HIV)-related salivary gland diseases.\nA descriptive and clinical study, with review of patient data.\nWe reviewed 113 referred cases of oral mucocele. The following anatomical sites were identified: lip, tongue, and floor of the mouth (simple ranulas), as well as plunging ranulas. The age and gender data of the patients with oral mucoceles were recorded. The HIV status of the patients and other information were reviewed.\nThere were 30 (26.5%) males and 83 (73.5%) females. Most patients were below 30 years of age, with the peak frequency in the first and second decade. Ranula (simple and plunging) represented 84.1% of the mucocele locations. Mucocele on the lips represented 10.6%. Seventy-two (63.7%) patients were HIV positive; and 97.2% of them had ranulas. Thirty-eight (33.6%) patients presented with plunging ranulas; and 92.1% of them were HIV positive, compared with two patients presenting with plunging ranulas in the HIV-negative group. These results strongly suggest that an HIV-positive patient is statistically (P<0.001) more at risk of presenting with not only a simple, but also a plunging ranula type.\"\nQuestion:\n\"Oral mucocele/ranula: Another human immunodeficiency virus-related salivary gland disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22301406": {
                "source": [
                    "\"CYP2D6 is an important cytochrome P450 enzyme. These enzymes catalyse the oxidative biotransformation of about 25% of clinically important drugs as well as the metabolism of numerous environmental chemical carcinogens. The most frequent null allele of CYP2D6 in European populations, CYP2D6*4, has been studied here in order to elucidate whether a relationship exists between this allele and the risk of developing breast cancer in a Spanish population.\nNinety-six breast cancer Spanish patients and one hundred healthy female volunteers were genotyped for the CYP2D6*4 allele using AmpliChip CYP450 Test technology.\nHomozygous CYP2D6*4 frequency was significant lower in breast cancer patients than in the control group (OR=0.22, p=0.04). The heterozygous CYP2D6*4 group also displayed lower values in patients than in controls but the difference was not significant (OR=0.698, p=0.28). Therefore, the presence of the CYP2D6*4 allele seems to decrease susceptibility to breast carcinoma in the selected population.\"\nQuestion:\n\"CYP2D6*4 allele and breast cancer risk: is there any association?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17224424": {
                "source": [
                    "\"The aim of the present study was to assess the effects of exercise training on heart rate, QT interval, and on the relation between ventricular repolarization and heart rate in men and women.\nA 24 h Holter recording was obtained in 80 healthy subjects (40 males) who differed for the degree of physical activity. Trained individuals showed a lower heart rate and a higher heart rate variability than sedentary subjects, independent of the gender difference in basal heart rate. Mean 24 h QTc was similar in trained and non-trained men, while a significant difference was observed between trained and non-trained women. Exercise training reduced the QT/RR slope in both genders. This effect on the QT/RR relation was more marked in women; in fact, the gender difference in the ventricular repolarization duration at low heart rate observed in sedentary subjects was no longer present among trained individuals.\"\nQuestion:\n\"Effects of exercise training on heart rate and QT interval in healthy young individuals: are there gender differences?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23422012": {
                "source": [
                    "\"Vancomycin is the primary treatment for infections caused by methicilin-resistant Staphylococcus aureus (MRSA). The association of vancomycin treatment failures with increased vancomycin minimum inhibitory concentration (MIC) is a well-recognized problem. A number of single-centre studies have identified progressive increases in glycopeptide MICs for S. aureus strains over recent years - a phenomenon known as vancomycin MIC creep. It is unknown if this is a worldwide phenomenon or if it is localized to specific centers.\nThe aim of this study was to evaluate the trend of vancomycin MIC for isolates of MRSA over a 3-year period in a tertiary university hospital in Portugal. MRSA isolates from samples of patients admitted from January 2007 to December 2009 were assessed. Etest method was used to determine the respective vancomycin MIC. Only one isolate per patient was included in the final analysis.\nA total of 93 MRSA isolates were studied. The vancomycin MICs were 0.75, 1, 1.5 and 2 mg/L for 1 (1.1%), 19 (20.4%), 38 (40.9%), 35 (37.6%) isolates, respectively. During the 3 year period, we observed a significant fluctuation in the rate of MRSA with a vancomycin MIC\u2009>\u20091 mg/L (2007: 86.2%; 2008: 93.3%; 2009: 58.8%, p\u2009=\u20090.002). No MRSA isolate presented a MIC\u2009>\u20092 mg/L.\"\nQuestion:\n\"Is vancomycin MIC creep a worldwide phenomenon?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22683044": {
                "source": [
                    "\"Some studies suggest that open access articles are more often cited than non-open access articles. However, the relationship between open access and citations count in a discipline such as intensive care medicine has not been studied to date. The present article analyzes the effect of open access publishing of scientific articles in intensive care medicine journals in terms of citations count.\nWe evaluated a total of 161 articles (76% being non-open access articles) published in Intensive Care Medicine in the year 2008. Citation data were compared between the two groups up until April 30, 2011. Potentially confounding variables for citation counts were adjusted for in a linear multiple regression model.\nThe median number (interquartile range) of citations of non-open access articles was 8 (4-12) versus 9 (6-18) in the case of open access articles (p=0.084). In the highest citation range (>8), the citation count was 13 (10-16) and 18 (13-21) (p=0.008), respectively. The mean follow-up was 37.5 \u00b1 3 months in both groups. In the 30-35 months after publication, the average number (mean \u00b1 standard deviation) of citations per article per month of non-open access articles was 0.28 \u00b1 0.6 versus 0.38 \u00b1 0.7 in the case of open access articles (p=0.043). Independent factors for citation advantage were the Hirsch index of the first signing author (\u03b2=0.207; p=0.015) and open access status (\u03b2=3.618; p=0.006).\"\nQuestion:\n\"Does open access publishing increase the impact of scientific articles?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22108230": {
                "source": [
                    "\"To investigate the presence of inflammatory cytokines and the fibronectin-aggrecan complex (FAC) in persons undergoing surgical treatment for cervical radiculopathy caused by disk herniation.\nSingle-center, prospective, consecutive case series.\nA single large academic institution.\nA total of 11 patients with radiculopathic pain and magnetic resonance imaging findings positive for disk herniation elected to undergo single-level cervical diskectomy.\nLavage was performed by needle injection and aspiration upon entering the disk space for fluoroscopic localization before diskectomy.\nThe lavage fluid was assayed for pH and the FAC, as well as for the cytokines interleukin-6 (IL-6), interferon-\u03b3, monocyte chemotactic protein (MCP), and macrophage inhibitory protein-1\u03b2.\nThe subjects were 7 women and 4 men with a mean age of 50.6 years (SE 9.7; range, 36-70 years). The mean concentrations (SE; range) in picograms per milliliter were 7.9 (4.4; 0-44) for IL-6, 25.3 (15.5; 0-159) for interferon-\u03b3, 16.1 (11.9; 0-121) for MCP, and 6.1 (2.8; 0-29) for macrophage inhibitory protein-1\u03b2. The optical density of the FAC at 450 nm was 0.151 (0.036; 0.1-0.32), and the pH was 6.68 (0.1; 6.10-7.15). Statistically significant correlations were found between MCP and FAC (P = .036) and between FAC and pH (P = .008).\"\nQuestion:\n\"Is the fibronectin-aggrecan complex present in cervical disk disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15954832": {
                "source": [
                    "\"Laparoscopy has rapidly emerged as the preferred surgical approach to a number of different diseases because it allows for a correct diagnosis and proper treatment. It seems to be moving toward the use of mini-instruments (5 mm or less in diameter). The aim of this paper is to illustrate retrospectively the results of an initial experience of minilaparoscopic transabdominal preperitoneal (miniTAPP) repair of groin hernia defects performed at two institutions.\nBetween February 2000 and December 2003, a total of 303 patients (mean age, 45 years) underwent a miniTAPP procedure: 213 patients (70.2%) were operated on bilaterally and 90 (28.7%) for a unilateral defect, with a total of 516 hernia defects repaired. The primary endpoint was the feasibility rate for miniTAPP. The secondary endpoint was the incidence of mini-TAPP-related complications.\nNo conversions to laparoscopy or an anterior open approach were required. There were no major complications, while minor complications ranged as high as 0.3%.\"\nQuestion:\n\"Is minilaparoscopic inguinal hernia repair feasible?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15222284": {
                "source": [
                    "\"The aim of this prospective, randomized study was to compare the hemodynamic performance of the Medtronic Mosaic and Edwards Perimount bioprostheses in the aortic position, and to evaluate prosthesis-specific differences in valve sizing and valve-size labeling.\nBetween August 2000 and September 2002, 139 patients underwent isolated aortic valve replacement (AVR) with the Mosaic (n = 67) or Perimount (n = 72) bioprosthesis. Intraoperatively, the internal aortic annulus diameter was measured by insertion of a gauge (Hegar dilator), while prosthesis size was determined by using the original sizers. Transthoracic echocardiography was performed to determine hemodynamic and dimensional data. As the aim of AVR is to achieve a maximal effective orifice area (EOA) within a given aortic annulus, the ratio of EOA to patient aortic annulus area was calculated, the latter being based on annulus diameter measured intraoperatively.\nOperative mortality was 2.2% (Mosaic 3.0%; Perimount 1.4%; p = NS). Upsizing (using a prosthesis larger in labeled valve size than the patient's measured internal aortic annulus diameter) was possible in 28.4% of Mosaic patients and 8.3% of Perimount patients. The postoperative mean systolic pressure gradient ranged from 10.5 to 22.2 mmHg in the Mosaic group, and from 9.4 to 12.6 mmHg in the Perimount group; it was significantly lower for 21 and 23 Perimount valves than for 21 and 23 Mosaic valves. The EOA ranged from 0.78 to 2.37 cm2 in Mosaic patients, and from 0.95 to 2.12 cm2 in Perimount patients. When indexing EOA by calculating the ratio of EOA to patient aortic annulus area to adjust for variables such as patient anatomy and valve dimensions, there was no significant difference between the two bioprostheses.\"\nQuestion:\n\"The effective orifice area/patient aortic annulus area ratio: a better way to compare different bioprostheses?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23806388": {
                "source": [
                    "\"To examine the ability of various postoperative nomograms to predict prostate cancer-specific mortality (PCSM) and to validate that they could predict aggressive biochemical recurrence (BCR). Prostate-specific antigen (PSA), grade, and stage are the classic triad used to predict BCR after radical prostatectomy (RP). Multiple nomograms use these to predict risk of BCR. A previous study showed that several nomograms could predict aggressive BCR (prostate-specific antigen doubling time [PSADT]\u00a0<9 months) more accurately than BCR. However, it remains unknown if they can predict more definitive endpoints, such as PCSM.\nWe performed Cox analyses to examine the ability of 4 postoperative nomograms, the Duke Prostate Center (DPC) nomogram, the Kattan postoperative nomogram, the Johns Hopkins Hospital (JHH) nomogram, and the joint Center for Prostate Disease Research(CPDR)/Cancer of the Prostate Strategic Urologic Research Endeavor (CaPSURE) nomogram to predict BCR and PCSM among 1778 men in the Shared Equal Access Regional Cancer Hospital (SEARCH) database who underwent RP between 1990 and 2009. We also compared their ability to predict BCR and aggressive BCR in a subset of men. We calculated the c-index for each nomogram to determine its predictive accuracy for estimating actual outcomes.\nWe found that each nomogram could predict aggressive BCR and PCSM in a statistically significant manner and that they all predicted PCSM more accurately than they predicted BCR (ie, with higher c-index values).\"\nQuestion:\n\"Do nomograms designed to predict biochemical recurrence (BCR) do a better job of predicting more clinically relevant prostate cancer outcomes than BCR?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19058191": {
                "source": [
                    "\"Quality of Life (QoL) assessment remains integral in the investigation of women with lower urinary tract dysfunction. Previous work suggests that physicians tend to underestimate patients' symptoms and the bother that they cause. The aim of this study was to assess the relationship between physician and patient assessed QoL using the Kings Health Questionnaire (KHQ).\nPatients complaining of troublesome lower urinary tract symptoms (LUTS) were recruited from a tertiary referral urodynamic clinic. Prior to their clinic appointment they were sent a KHQ, which was completed before attending. After taking a detailed urogynecological history, a second KHQ was filled in by the physician, blinded to the patient responses, on the basis of their impression of the symptoms elicited during the interview. These data were analyzed by an independent statistician. Concordance between patient and physician assessment for individual questions was assessed using weighted kappa analysis. QoL scores were compared using Wilcoxons signed rank test.\nSeventy-five patients were recruited over a period of 5 months. Overall, the weighted kappa showed relatively poor concordance between the patient and physician responses; mean kappa: 0.33 (range 0.18-0.57). The physician underestimated QoL score in 4/9 domains by a mean of 5.5% and overestimated QoL score in 5/9 domains by a mean of 6.9%. In particular, physicians underestimated the impact of LUTS on social limitations and emotions (P<0.05).\"\nQuestion:\n\"Is there a discrepancy between patient and physician quality of life assessment?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26126304": {
                "source": [
                    "\"To compare in vitro fertilization (IVF) outcomes in low responders stimulated with microdose leuprolide protocol (ML) following pretreatment with either oral contraceptive pill (OCP) or luteal estradiol (E2) + GnRH antagonist (E2 + antag) for follicular synchronization prior to controlled ovarian hyperstimulation (COH).\nThis was a retrospective study of 130 women, who were poor responders, undergoing IVF with either OCP/ML or E2+ antag/ML protocols. The main outcome measures were ongoing pregnancy rates, number of oocytes retrieved, and cancellation rate.\nBoth groups were similar in baseline characteristics. There were no significant differences in gonadotropin requirement, cancellation rate, and number of embryos transferred. Ongoing pregnancy rates (40% vs. 15%) were significantly higher in the OCP/ML group. Trends toward greater number of oocytes retrieved (7.7 \u00b1 3.4 vs. 5.9 \u00b1 4.2) and improved implantation rates (20% vs. 12%) were also noted, but these did not reach statistical significance.\"\nQuestion:\n\"Estradiol and Antagonist Pretreatment Prior to Microdose Leuprolide in in Vitro Fertilization. Does It Improve IVF Outcomes in Poor Responders as Compared to Oral Contraceptive Pill?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25432938": {
                "source": [
                    "\"The objective of the current study is to determine to what extent the reduction of Chile's traffic fatalities and injuries during 2000-2012 was related to the police traffic enforcement increment registered after the introduction of its 2005 traffic law reform.\nA unique dataset with assembled information from public institutions and analyses based on ordinary least square and robust random effects models was carried out. Dependent variables were traffic fatality and severe injury rates per population and vehicle fleet. Independent variables were: (1) presence of new national traffic law; (2) police officers per population; (3) number of traffic tickets per police officer; and (4) interaction effect of number of traffic tickets per police officer with traffic law reform. Oil prices, alcohol consumption, proportion of male population 15-24 years old, unemployment, road infrastructure investment, years' effects and regions' effects represented control variables.\nEmpirical estimates from instrumental variables suggest that the enactment of the traffic law reform in interaction with number of traffic tickets per police officer is significantly associated with a decrease of 8% in traffic fatalities and 7% in severe injuries. Piecewise regression model results for the 2007-2012 period suggest that police traffic enforcement reduced traffic fatalities by 59% and severe injuries by 37%.\"\nQuestion:\n\"Did Chile's traffic law reform push police enforcement?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23598835": {
                "source": [
                    "\"Severe, immediate postprocedural pain and the need for analgesics after vertebroplasty can be a discouraging experience for patients and caregivers. The goal of this study was to investigate whether the presence of severe pain immediately after vertebroplasty predicts short- and long-term pain relief.\nA chart review was performed to categorize patients regarding pain severity and analgesic usage immediately after vertebroplasty (<4 h). \"Severe\" pain was defined as at least 8 of 10 with the 10-point VAS. Outcomes were pain severity and pain medication score and usage at 1 month and 1 year after vertebroplasty. Outcomes and clinical characteristics were compared between groups by using the Wilcoxon signed-rank test and the Fisher exact test.\nOf the 429 vertebroplasty procedures identified, 69 (16%) were associated with severe pain, and 133 (31%) were associated with analgesic administration immediately after the procedure. The group experiencing severe pain had higher preprocedure median VAS rest pain scores (5 [IQR, 2-7]) and activitypain scores (10 [IQR, 8-10]) compared with patients who did not experience severe pain (3 [IQR, 1-6]; P = .0208, and 8 [IQR, 7-10]; P = .0263, respectively). At 1 month postprocedure, VAS rest and activity pain scores were similar between the severe pain group and the nonsevere pain group (P = .16 and P = .25, respectively) and between the group receiving pain medication and the group not receiving pain medication (P = .25 and P = .67, respectively). This similarity continued for 1 year after the procedure. Analgesic usage was similar among all groups at 1 year postprocedure.\"\nQuestion:\n\"Is severe pain immediately after spinal augmentation a predictor of long-term outcomes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21951591": {
                "source": [
                    "\"Chronic low back pain (CLBP) is often accompanied by an abnormal motor performance. However, it has not been clarified yet whether these deviations also occur during motor tasks not involving the back and whether the performance is influenced by pain and pain-related cognitions. Therefore, the aim of the present study is to get insight in the contribution of both pain experience and pain-related cognitions to general motor task performance in CLBP.\n13 CLBP patients and 15 healthy subjects performed a hand-function task in three conditions: sitting, lying prone (lying) and lying prone without trunk support (provoking). The last condition was assumed to provoke pain-related cognitions, which was considered successful when a patients' pain expectancy on a numeric rating scale was at least 1 point higher than actual pain experienced. Subjects' performance was expressed in reaction time and movement time. Repeated measures analysis of variance was performed to detect main effect for group and condition. Special interest was given to group*condition interaction, since significant interaction would indicate that patients and healthy subjects performed differently throughout the three conditions.\nPatients were slower throughout all conditions compared to healthy subjects. With respect to the provoking condition, patients showed deteriorated performance compared to lying while healthy subjects' performance remained equal between these two conditions. Further analysis of patients' data showed that provocation was successful in 54% of the patients. Especially this group showed deteriorated performance in the provoking condition.\"\nQuestion:\n\"Motor performance in chronic low back pain: is there an influence of pain-related cognitions?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23999452": {
                "source": [
                    "\"Hypoglycaemia caused by glucose-lowering therapy has been linked to cardiovascular (CV) events. The ORIGIN trial provides an opportunity to further assess this relationship.\nA total of 12 537 participants with dysglycaemia and high CV-risk were randomized to basal insulin glargine titrated to a fasting glucose of \u2264 5.3 mmol/L (95 mg/dL) or standard glycaemic care. Non-severe hypoglycaemia was defined as symptoms confirmed by glucose \u2264 54 mg/dL and severe hypoglycaemia as a requirement for assistance or glucose \u2264 36 mg/dL. Outcomes were: (i) the composite of CV death, non-fatal myocardial infarction or stroke; (ii) mortality; (iii) CV mortality; and (iv) arrhythmic death. Hazards were estimated before and after adjustment for a hypoglycaemia propensity score. During a median of 6.2 years (IQR: 5.8-6.7), non-severe hypoglycaemic episodes occurred in 41.7 and 14.4% glargine and standard group participants, respectively, while severe episodes occurred in 5.7 and 1.8%, respectively. Non-severe hypoglycaemia was not associated with any outcome following adjustment. Conversely, severe hypoglycaemia was associated with a greater risk for the primary outcome (HR: 1.58; 95% CI: 1.24-2.02, P<0.001), mortality (HR: 1.74; 95% CI: 1.39-2.19, P<0.001), CV death (HR: 1.71; 95% CI: 1.27-2.30, P<0.001) and arrhythmic death (HR: 1.77; 95% CI: 1.17-2.67, P = 0.007). Similar findings were noted for severe nocturnal hypoglycaemia for the primary outcome and mortality. The severe hypoglycaemia hazard for all four outcomes was higher with standard care than with insulin glargine.\"\nQuestion:\n\"Does hypoglycaemia increase the risk of cardiovascular events?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22348433": {
                "source": [
                    "\"The optimum protocol for expander volume adjustment with respect to the timing and application of radiotherapy remains controversial.\nEighteen New Zealand rabbits were divided into three groups. Metallic port integrated anatomic breast expanders of 250 cc were implanted on the back of each animal and controlled expansion was performed. Group I underwent radiotherapy with full expanders while in Group II, expanders were partially deflated immediately prior to radiotherapy. Control group did not receive radiotherapy.The changes in blood flow at different volume adjustments were investigated in Group II by laser Doppler flowmetry. Variations in the histopathologic properties of the irradiated tissues including the skin, capsule and the pocket floor, were compared in the biopsy specimens taken from different locations in each group.\nA significant increase in skin blood flow was detected in Group II with partial expander deflation. Overall, histopathologic exam revealed aggravated findings of chronic radiodermatitis (epidermal atrophy, dermal inflammation and fibrosis, neovascularisation and vascular changes as well as increased capsule thickness) especially around the lower expander pole, in Group II.\"\nQuestion:\n\"Does partial expander deflation exacerbate the adverse effects of radiotherapy in two-stage breast reconstruction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8245806": {
                "source": [
                    "\"Family medicine has aspired to train residents and conduct research in settings that closely resemble community practice. The purpose of this study was to compare the patient characteristics of the ambulatory teaching centers of a consortium of seven community-based university-affiliated family practice residency programs in northeast Ohio with the National Ambulatory Medical Care Survey (NAMCS) results for family physicians (FPs) and general practitioners (GPs).\nNinety-eight faculty and resident physicians at the residency training site of the Northeastern Ohio Universities College of Medicine collected data on all ambulatory patient visits (N = 1498) for one randomly chosen week between July 1, 1991, and June 30, 1992. We compared these data with patient visits reported in the 1990 NAMCS for FPs and GPs.\nThe residency training sites saw slightly more children, women, blacks, and Medicare and Medicaid patients. The most common reason for an office visit in both populations was an undifferentiated symptom. Fifteen of the top 20 \"reason for visit\" codes were identical, as were 14 of the top 20 diagnoses. More preventive and therapeutic services were offered or performed at our residency training sites but fewer diagnostic services were performed. There were fewer consultations requested at our residency training sites but similar hospitalization rates for patients. The mean duration of visit differed by only 1 minute.\"\nQuestion:\n\"Does family practice at residency teaching sites reflect community practice?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26701174": {
                "source": [
                    "\"To ascertain whether hospital type is associated with differences in total cost and outcomes for inpatient tonsillectomy.\nCross-sectional analysis of the 2006, 2009, and 2012 Kids' Inpatient Database (KID).\nChildren \u226418 years of age undergoing tonsillectomy with/without adenoidectomy were included. Risk-adjusted generalized linear models assessed for differences in hospital cost and length of stay (LOS) among children managed by (1) non-children's teaching hospitals (NCTHs), (2) children's teaching hospitals (CTHs), and (3) nonteaching hospitals (NTHs). Risk-adjusted logistic regression compared the odds of major perioperative complications (hemorrhage, respiratory failure, death). Models accounted for clustering of patients within hospitals, were weighted to provide national estimates, and controlled for comorbidities.\nThe 25,685 tonsillectomies recorded in the KID yielded a national estimate of 40,591 inpatient tonsillectomies performed in 2006, 2009, and 2012. The CTHs had significantly higher risk-adjusted total cost and LOS per tonsillectomy compared with NCTHs and NTHs ($9423.34/2.8 days, $6250.78/2.11 days, and $5905.10/2.08 days, respectively; P<.001). The CTHs had higher odds of complications compared with NCTHs (odds ratio [OR], 1.48; 95% CI, 1.15-1.91; P = .002) but not when compared with NTHs (OR, 1.19; 95% CI, 0.89-1.59; P = .23). The CTHs were significantly more likely to care for patients with comorbidities (P<.001).\"\nQuestion:\n\"Inpatient Pediatric Tonsillectomy: Does Hospital Type Affect Cost and Outcomes of Care?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17051586": {
                "source": [
                    "\"Several studies have suggested a protective effect of folic acid (FA) on congenital heart anomalies. Down syndrome (DS) infants are known to have a high frequency of heart anomalies. Not all children with DS suffer from heart anomalies, which raises the question whether maternal factors might affect the risk of these anomalies. Our objectives were to investigate whether first-trimester FA use protects against heart anomalies among DS children.\nWomen with liveborn DS children participating in the Slone Epidemiology Center Birth Defects Study between 1976 and 1997 were included. We performed case-control analyses using DS, with heart anomalies as cases and DS, without heart anomalies as controls. Subanalyses were performed for defects that have been associated with FA in non-DS populations (conotruncal, ventricular septal [VSD]) and for those that are associated with DS (ostium secundum type atrial septal defects [ASD]and endocardial cushion defects [ECD]). Exposure was defined as the use of any FA-containing product for an average of at least 4 days per week during the first 12 weeks of pregnancy, whereas no exposure was defined as no use of FA in these 12 weeks.\nOf the 223 cases, 110 (49%) were exposed versus 84 (46%) of the 184 controls. After adjustment for possible confounders, no protective effect of FA was found on heart anomalies overall (OR 0.95, 95% CI: 0.61-1.47) nor separately for conotruncal defects, VSDs, ASDs, or ECDs.\"\nQuestion:\n\"Can folic acid protect against congenital heart defects in Down syndrome?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15962678": {
                "source": [
                    "\"This prospective, randomized, double-blind study was designed to determine and compare the usefulness of preloading colloids (Haemaccel) 10 ml/Kg before positioning whether it can prevent hemodynamic changes during seated positioning or not.\nThe authors studied 20 patients by randomly dividing them into 2 groups. The control group was given crystalloid as maintenance and deposit replacement but the study group was given extra colloids 10 ml/Kg 30 minutes before starting general anesthesia. Both groups were monitored and given anesthesia, balanced technique. Systolic and diastolic blood pressures, heart rate, central venous pressure (CVP) at different time intervals in the sitting position for 30 minutes were recorded. Statistical analysis was done by Student t-test, Chi-square test and ANOVA (p-value<0. 05 considered significant).\nThe results showed that systolic blood pressure at 15, 20, 30 minutes and CVP at 15, 25, 30 minutes after positioning in the study group was maintained significantly compared to the control group and there were no significant changes in diastolic blood pressure and heart rate. There were no other complications during the sitting period.\"\nQuestion:\n\"Does preloading with colloids prevent hemodynamic changes when neurosurgical patients are subsequently changed to the seated position?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17032327": {
                "source": [
                    "\"The quality of surgical excision is held to be a major determinant of outcome following surgery for rectal cancer. Macroscopic examination of the excised mesorectum allows for reproducible assessment of the quality of surgery. We aimed to determine whether quality of excision undertaken by colorectal trainees under supervision was comparable with that performed by consultants, as measured using mesorectal grades.\nA total of 130 consecutive patients undergoing potentially curative resection for primary adenocarcinoma of the rectum in our centre from 2001 to 2003 were included in the study. The pathologists graded the excised mesorectum according to staged classification proposed by Quirke. The outcome (quality of mesorectal excision and secondary outcomes including local recurrence and overall recurrence) of operations performed by consultants was compared with that of trainees. Statistical significance was tested using Pearson chi(2) test.\nEighty-nine operations were performed by consultants and 41 by senior colorectal trainees with consultant supervision. Forty-four patients (49%) had good mesorectum when operated by consultants in comparison with 17 (41.5%) by the trainees. There was no statistically significant difference (P = 0.717) between the two groups in terms of quality of mesorectum excised after potentially curative resection. Furthermore, there were seven local recurrences in patients operated by consultants (7.8%) when compared with four in the trainee group (9.5%) and once again there was no statistical significance between the two groups (P = 0.719).\"\nQuestion:\n\"Do supervised colorectal trainees differ from consultants in terms of quality of TME surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18565233": {
                "source": [
                    "\"Epidemiologic studies have suggested that hypertriglyceridemia and insulin resistance are related to the development of colon cancer. Nuclear peroxisome proliferator-activated receptors (PPAR), which play a central role in lipid and glucose metabolism, had been hypothesized as being involved in colon cancerogenesis. In animal studies the lipid-lowering PPAR ligand bezafibrate suppressed colonic tumors. However, the effect of bezafibrate on colon cancer development in humans is unknown. Therefore, we proposed to investigate a possible preventive effect of bezafibrate on the development of colon cancer in patients with coronary artery disease during a 6-year follow-up.\nOur population included 3011 patients without any cancer diagnosis who were enrolled in the randomized, double blind Bezafibrate Infarction Prevention (BIP) Study. The patients received either 400 mg of bezafibrate retard (1506 patients) or placebo (1505 patients) once a day. Cancer incidence data were obtained by matching a subject's identification numbers with the National Cancer Registry. Each matched record was checked for correct identification.\nDevelopment of new cancer (all types) was recorded in 177 patients: in 79 (5.25%) patients from the bezafibrate group vs. 98 (6.51%) from the placebo group. Development of colon cancer was recorded in 25 patients: in 8 (0.53%) patients from the bezafibrate group vs. 17 (1.13%) from the placebo group, (Fisher's exact test: one side p = 0.05; two side p = 0.07). A difference in the incidence of cancer was only detectable after a 4 year lag and progressively increased with continued follow-up. On multivariable analysis the colon cancer risk in patients who received bezafibrate tended to be lower with a hazard ratio of 0.47 and 95% confidence interval 0.2-1.1.\"\nQuestion:\n\"Does the lipid-lowering peroxisome proliferator-activated receptors ligand bezafibrate prevent colon cancer in patients with coronary artery disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23076787": {
                "source": [
                    "\"To explain China's cigarette pricing mechanism and the role of the Chinese State Tobacco Monopoly Administration (STMA) on cigarette pricing and taxation.\nPublished government tobacco tax documentation and statistics published by the Chinese STMA are used to analyse the interrelations among industry profits, taxes and retail price of cigarettes in China.\nThe 2009 excise tax increase on cigarettes in China has not translated into higher retail prices because the Chinese STMA used its policy authority to ensure that retail cigarette prices did not change. The government tax increase is being collected at both the producer and wholesale levels. As a result, the 2009 excise tax increase in China has resulted in higher tax revenue for the government and lower profits for the tobacco industry, with no increase in the retail price of cigarettes for consumers.\"\nQuestion:\n\"Can increases in the cigarette tax rate be linked to cigarette retail prices?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17682349": {
                "source": [
                    "\"To identify gender differences in delay time and the reasons why African Americans delay in seeking medical care for symptoms of acute myocardial infarction (AMI).\nCross-sectional.\nFive hospitals in the San Francisco and East Bay areas.\nSixty-one African American men and women diagnosed with an AMI.\nPrehospital delay time.\nMedian delay time was longer for women compared to men (4.4 hours vs 3.5 hours), although the difference was not significant. Single women delayed longer than single men (P = .03), and women who were alone when symptoms began delayed longer than women with someone (P = .03). Women who received advice to seek help or call 911 upon symptom onset had shorter delays compared to women who were not advised to call 911 (P = .01). Men at home delayed longer than men who experienced their symptoms outside the home (P = .01). Men with emergency room insurance delayed longer than men without emergency room insurance (P = .03), and men who took an ambulance to the hospital had shorter delay times than men who took other means of transportation (P = .04).\"\nQuestion:\n\"Are there gender differences in the reasons why African Americans delay in seeking medical help for symptoms of an acute myocardial infarction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16827975": {
                "source": [
                    "\"Few studies have assessed whether the advantage chemotherapy has been shown to have in treating advanced non-small lung carcinoma in clinical trials is transferrable to normal health care activity. This could explain the skepticism of a large number of pneumologists towards this treatment. The objective of our study was to analyze prognostic factors related to survival and to see whether cytostatic treatment was an independent predictor.\nPatients enrolled in the study had been diagnosed with non-small cell carcinoma in stages IV or IIIB with pleural or N2-N3 involvement and with a performance status of 2 or below according to the Eastern Cooperative Oncology Group (ECOG). Survival was analyzed with regard to the following variables: age, sex, comorbidity, weight loss, laboratory test results, histological type, ECOG score, TNM staging, and treatment. The Student t test, the chi(2) test, the Kaplan-Meier method, the log-rank test, and Cox regression analysis were used in the statistical analysis.\nWe enrolled 190 patients (157 men and 33 women) with a mean (SD) age of 61.75 (10.85) years (range, 33-85 years). Of these patients, 144 received cytostatic treatment and 46 palliative treatment. The median survival was 31 weeks and was related to absence of weight loss (hazard ratio [HR], 1.73; 95% confidence interval [CI], 1.26-2.39; P=.001), cytostatic treatment (HR, 1.85; 95% CI, 1.25-2.76; P=.002), and ECOG score of 0 to 1 (HR, 2.84; 95% CI, 1.62-5.00; P=.0001). In patients with ECOG scores of 0 to 1, weight loss and treatment were significant prognostic factors. Survival in the ECOG 2 group was 15 weeks for patients undergoing cytostatic treatment and 11 weeks for patients with symptomatic treatment.\"\nQuestion:\n\"Chemotherapy and survival in advanced non-small cell lung carcinoma: is pneumologists' skepticism justified?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10922093": {
                "source": [
                    "\"The use of open access endoscopy is increasing. Its effect on the adequacy of patient informed consent, procedure acceptance and the impact on subsequent communication/transfer of procedure results to the patient have not been evaluated. The aim of our study was to compare the extent of preknowledge of procedures and test explanation, patient medical complexity, information transfer and overall patient satisfaction between a patient group referred for outpatient open access endoscopy versus a patient group from a gastrointestinal (GI) subspecialty clinic.\nInformation was obtained from all patients presenting for outpatient upper and lower endoscopy by using a 1-page questionnaire. Patients from the two groups who had an outpatient upper/lower endoscopic procedure were contacted by phone after the procedure to obtain information with a standardized questionnaire.\nThe open access patients reported receiving significantly less information to help them identify the procedure (p<0.01) and less explanation concerning the nature of the procedure than the group of patients referred from the subspecialty clinic (p<0.005). There was no difference between the two groups in satisfaction scores for examinations performed under conscious sedation. For flexible sigmoidoscopy without sedation, however, the GI clinic patient group were more satisfied with their procedure. The majority of patients, regardless of access, were more likely to receive endoscopic results from a gastroenterologist than the referring physician. Furthermore, the patients in the GI clinic group who underwent colonoscopy felt significantly better at follow-up.\"\nQuestion:\n\"Does open access endoscopy close the door to an adequately informed patient?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18019905": {
                "source": [
                    "\"To illustrate how maternal mortality audit identifies different causes of and contributing factors to maternal deaths in different settings in low- and high-income countries and how this can lead to local solutions in reducing maternal deaths.\nDescriptive study of maternal mortality from different settings and review of data on the history of reducing maternal mortality in what are now high-income countries.\nKalabo district in Zambia, Farafenni division in The Gambia, Onandjokwe district in Namibia, and the Netherlands.\nPopulation of rural areas in Zambia and The Gambia, peri-urban population in Namibia and nationwide data from The Netherlands.\nData from facility-based maternal mortality audits from three African hospitals and data from the latest confidential enquiry in The Netherlands.\nMaternal mortality ratio (MMR), causes (direct and indirect) and characteristics.\nMMR ranged from 10 per 100,000 (the Netherlands) to 1540 per 100,000 (The Gambia). Differences in causes of deaths were characterized by HIV/AIDS in Namibia, sepsis and HIV/AIDS in Zambia, (pre-)eclampsia in the Netherlands and obstructed labour in The Gambia.\"\nQuestion:\n\"The use of audit to identify maternal mortality in different settings: is it just a difference between the rich and the poor?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25280365": {
                "source": [
                    "\"Clinical pathologists (CPs) report RBC morphologic (RBC-M) changes to assist clinicians in prioritizing differential diagnoses. However, reporting is subjective, semiquantitative, and potentially biased. Reporting decisions vary among CPs, and reports may not be interpreted by clinicians as intended.\nThe aims of this study were to survey clinicians and CPs about RBC-M terms and their clinical value, and identify areas of agreement and discordance.\nOnline surveys were distributed to small animal clinicians via the Veterinary Information Network and to CPs via the ASVCP listserv. A quiz assessed understanding of RBC-M terms among respondent groups. Descriptive statistics were used to analyze responses to survey questions, and quiz scores were compared among groups.\nAnalyzable responses were obtained from 1662 clinicians and 82 CPs. Both clinicians and CPs considered some terms, e.g., agglutination, useful, whereas only CPs considered other terms, e.g., ghost cells, useful. All groups interpreted certain terms, e.g., Heinz bodies, correctly, whereas some clinicians misinterpreted others, e.g., eccentrocytes. Responses revealed that CPs often do not report RBC-M they consider insignificant, when present in low numbers. Twenty-eight percent of clinicians think CPs review all blood smears while only 19% of CPs report reviewing all smears.\"\nQuestion:\n\"Reporting and interpreting red blood cell morphology: is there discordance between clinical pathologists and clinicians?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22011946": {
                "source": [
                    "\"Many insurance payors mandate that bariatric surgery candidates undergo a medically supervised weight management (MSWM) program as a prerequisite for surgery. However, there is little evidence to support this requirement. We evaluated in a randomized controlled trial the hypothesis that participation in a MSWM program does not predict outcomes after laparoscopic adjustable gastric banding (LAGB) in a publicly insured population.\nThis pilot randomized trial was conducted in a large academic urban public hospital. Patients who met NIH consensus criteria for bariatric surgery and whose insurance did not require a mandatory 6-month MSWM program were randomized to a MSWM program with monthly visits over 6 months (individual or group) or usual care for 6 months and then followed for bariatric surgery outcomes postoperatively. Demographics, weight, and patient behavior scores, including patient adherence, eating behavior, patient activation, and physical activity, were collected at baseline and at 6 months (immediately preoperatively and postoperatively).\nA total of 55 patients were enrolled in the study with complete follow-up on 23 patients. Participants randomized to a MSWM program attended an average of 2 sessions preoperatively. The majority of participants were female and non-Caucasian, mean age was 46 years, average income was less than $20,000/year, and most had Medicaid as their primary insurer, consistent with the demographics of the hospital's bariatric surgery program. Data analysis included both intention-to-treat and completers' analyses. No significant differences in weight loss and most patient behaviors were found between the two groups postoperatively, suggesting that participation in a MSWM program did not improve weight loss outcomes for LAGB. Participation in a MSWM program did appear to have a positive effect on physical activity postoperatively.\"\nQuestion:\n\"Does a preoperative medically supervised weight loss program improve bariatric surgery outcomes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24799031": {
                "source": [
                    "\"The objective was to evaluate the efficacy of diffusion-weighted imaging (DWI) in predicting the development of vascularization in hypovascular hepatocellular lesions (HHLs).\nForty-two HHLs that were diagnosed by computed tomographic (CT) arteriography were evaluated retrospectively. The lesion on DWI was classified as isointense, hypointense, or hyperintense. Follow-up studies that included intravenous dynamic CT or magnetic resonance imaging were performed.\nThe 730-day cumulative developments of vascularization in hypointense, isointense, and hyperintense lesions were 17%, 30%, and 40%, respectively. The differences among these developments were not statistically significant.\"\nQuestion:\n\"Is diffusion-weighted imaging a significant indicator of the development of vascularization in hypovascular hepatocellular lesions?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25891436": {
                "source": [
                    "\"Previous studies have reported that the total bilirubin (TB) level is associated with coronary artery disease, heart failure and atrial fibrillation. These heart diseases can produce cardiogenic cerebral embolism and cause cardioembolic stroke. However, whether the serum TB could be a biomarker to differentiate cardioembolic stroke from other stroke subtypes is unclear.\nOur study consisted of 628 consecutive patients with ischaemic stroke. Various clinical and laboratory variables of the patients were analysed according to serum TB quartiles and stroke subtypes.\nThe higher TB quartile group was associated with atrial fibrillation, larger left atrium diameter, lower left ventricular fractional shortening and cardioembolic stroke (P<0.001, P = 0.001, P = 0.033, P<0.001, respectively). Furthermore, serum TB was a statistically significant independent predictor of cardioembolic stroke in a multivariable setting (Continuous, per unit increase OR = 1.091, 95%CI: 1.023-1.164, P = 0.008).\"\nQuestion:\n\"Is serum total bilirubin useful to differentiate cardioembolic stroke from other stroke subtypes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10173769": {
                "source": [
                    "\"To consider whether the Barthel Index alone provides sufficient information about the long term outcome of stroke.\nCross sectional follow up study with a structured interview questionnaire and measures of impairment, disability, handicap, and general health. The scales used were the hospital anxiety and depression scale, mini mental state examination, Barthel index, modified Rankin scale, London handicap scale, Frenchay activities index, SF36, Nottingham health profile, life satisfaction index, and the caregiver strain index.\nSouth east London.\nPeople, and their identified carers, resident in south east London in 1989-90 when they had their first in a life-time stroke aged under 75 years.\nObservational study.\nComparison and correlation of the individual Barthel index scores with the scores on other outcome measures.\nOne hundred and twenty three (42%) people were known to be alive, of whom 106 (86%) were interviewed. The median age was 71 years (range 34-79). The mean interval between the stroke and follow up was 4.9 years. The rank correlation coefficients between the Barthel and the different dimensions of the SF36 ranged from r = 0.217 (with the role emotional dimension) to r = 0.810 (with the physical functioning dimension); with the Nottingham health profile the range was r = -0.189 (with the sleep dimension, NS) to r = -0.840 (with the physical mobility dimension); with the hospital and anxiety scale depression component the coefficient was r = -0.563, with the life satisfaction index r = 0.361, with the London handicap scale r = 0.726 and with the Frenchay activities index r = 0.826.\"\nQuestion:\n\"Longer term quality of life and outcome in stroke patients: is the Barthel index alone an adequate measure of outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20571467": {
                "source": [
                    "\"Kidneys from elderly donors tend to be implanted in recipients who are also elderly. We present the results obtained after 10 years of evolution on transplanting elderly kidneys into young recipients.\nNinety-one consecutive transplants are studied, carried out in our center with kidneys from cadaver donors older than 60 years implanted in recipients younger than 60 years. The control group is made up of 91 transplants, matched with those from the study group, whose donor and recipient were younger than 60 years.\nThere were no differences between groups with regard to recipient age, sex, cause of death and renal function of the donor, hepatitis C and cytomegalovirus serologies, cold ischemia time, tubular necrosis, immediate diuresis, need for dialysis, human leukocyte antigen incompatibilities, hypersensitized patients, acute rejection, waiting time on dialysis, and days of admission. Survival in both groups at 1, 5, and 10 years was 97.6%, 87.2%, and 76.6% vs. 98.8%, 87.5%, and 69.5% for the patient (P=0.642), 92.9%, 81.3%, and 64.2% vs. 93.9%, 76.4%, and 69.5% for the graft (P=0.980), and 94.4%, 92.6%, and 77.4% vs. 94.3%, 86.7%, and 84.4% for the graft with death censured (P=0.747), respectively. Creatininaemias at 1, 5, and 10 years were 172, 175, and 210 vs. 139, 134, and 155 (P<0.05).\"\nQuestion:\n\"Is it appropriate to implant kidneys from elderly donors in young recipients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19309468": {
                "source": [
                    "\"A variable effect of inflammation on alloimmunization to transfused red blood cells (RBCs) in mice has been recently reported. We investigated whether RBC alloimmunization in humans was affected by transfusion of blood products in temporal proximity to experiencing a febrile transfusion reaction (FTR) to platelets (PLTs), an event predominantly mediated by inflammatory cytokines.\nBlood bank databases were used to identify patients who experienced an FTR or possible FTR to PLTs from August 2000 to March 2008 (FTR group). The control group of patients received a PLT transfusion on randomly selected dates without experiencing an FTR. The \"event\" was defined as the PLT transfusion that caused the FTR in the FTR group or the index PLT transfusion in the control group. The number of transfused blood products and their proximity to the event were recorded along with other recipient data. The primary endpoint was the rate of RBC alloimmunization between the two groups.\nThere were 190 recipients in the FTR group and 245 in the control group. Overall, the recipients in the control group were younger and received more blood products on the day of their event and over the subsequent 10 days. The alloimmunization rate among recipients in the FTR group was higher than in the control group (8% vs. 3%, respectively; p = 0.026).\"\nQuestion:\n\"Does a febrile reaction to platelets predispose recipients to red blood cell alloimmunization?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24973051": {
                "source": [
                    "\"Establishing a core curriculum for undergraduate Emergency Medicine (EM) education is crucial to development of the specialty. The Clerkship Directors in Emergency Medicine (CDEM) National Curriculum Task Force recommended that all students in a 4(th)-year EM clerkship be exposed to 10 emergent clinical conditions.\nTo evaluate the feasibility of encountering recommended core conditions in a clinical setting during a 4(th)-year EM clerkship.\nStudents from three institutions participated in this ongoing, prospective observation study. Students' patient logs were collected during 4-week EM clerkships between July 2011 and June 2012. De-identified logs were reviewed and the number of patient encounters for each of the CDEM-identified emergent conditions was recorded. The percentage of students who saw each of the core complaints was calculated, as was the average number of core complaints seen by each.\nData from 130 students at three institutions were captured; 15.4% of students saw all 10 conditions during their rotation, and 76.9% saw at least eight. The average number of conditions seen per student was 8.4 (range of 7.0-8.6). The percentage of students who saw each condition varied, ranging from 100% (chest pain and abdominal pain) to 31% (cardiac arrest).\"\nQuestion:\n\"Medical student education in emergency medicine: do students meet the national standards for clinical encounters of selected core conditions?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12684740": {
                "source": [
                    "\"Alcohol-associated cues elicit craving in human addicts but little is known about craving mechanisms. Current animal models focus on relapse and this may confound the effect of environmental cues. OBJECTIVES. To develop a model to study the effects of environmental cues on alcohol consumption in animals not experiencing withdrawal or relapse.\nRats were trained to orally self-administer an alcohol (5% w/v)/saccharin (0.2%) solution 30 min a day for 20 days. After stable responding on a free choice between alcohol/saccharin and water, rats were exposed to 5, 10 or 15 min of alcohol-associated cues or 5 min of non-alcohol associated cues. The effect of a 5-min cue was measured after a 10-day break from training or pre-treatment with 0.03, 0.1 or 1 mg/kg naltrexone.\nRats given 5 min of alcohol-associated cues responded significantly more on the active lever (26% increase) and consumed more alcohol as verified by increased blood alcohol levels (8.9 mM versus control 7.5 mM). Ten or 15 min of cues did not change alcohol consumption and 5 min in a novel environment decreased response by 66%. After a 10-day break in training, 5 min of alcohol-associated cues still increased alcohol consumption (29% increase) and the cue effect could be dose-dependently blocked by naltrexone (143% decrease at 0.03 mg/kg).\"\nQuestion:\n\"Cue-induced behavioural activation: a novel model of alcohol craving?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16510651": {
                "source": [
                    "\"In this study we investigated whether the association between measures of fetal growth restriction and intellectual performance was mediated by socioeconomic or familial factors.\nThis was a population-based cohort study of 357,768 Swedish males born as singletons without congenital malformations between 1973 and 1981. The main outcome measure was intellectual performance at military conscription.\nCompared with men born with appropriate birth weight for gestational age, men born light for gestational age suffered an increased risk of low intellectual performance after adjustment for maternal and socioeconomic factors. The increase in risk of low intellectual performance related to a decrease in birth weight for gestational age was similar between families and within families. Men born short or with a small head circumference for gestational age were also at increased risk of low intellectual performance, both when adjusting for maternal and socioeconomic factors and within families.\"\nQuestion:\n\"Birth characteristics and risk of low intellectual performance in early adulthood: are the associations confounded by socioeconomic factors in adolescence or familial effects?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17919952": {
                "source": [
                    "\"(i) To examine the association between self-reported mechanical factors and chronic oro-facial pain. (ii) To test the hypothesis that this relationship could be explained by: (a) reporting of psychological factors, (b) common association of self-reported mechanical factors with other unexplained syndromes.\nA population based cross-sectional study of 4200 randomly selected adults registered with a General Medical Practice in North West, England. The study examined the association of chronic oro-facial pain with a variety of self-reported mechanical factors: teeth grinding, facial trauma, missing teeth and the feeling that the teeth did not fit together properly. Information was also collected on demographic factors, psychological factors and the reporting of other frequently unexplained syndromes.\nAn adjusted response rate of 72% was achieved. Only two mechanical factors: teeth grinding (odds ratio (OR) 2.0, 95% CI 1.3-3.0) and facial trauma (OR 2.0; 95% CI 1.3-2.9) were independently associated with chronic oro-facial pain after adjusting for psychological factors. However, these factors were also commonly associated with the reporting of other frequently unexplained syndromes: teeth grinding (odds ratio (OR) 1.8, 95% CI 1.5-2.2), facial trauma (OR 2.1; 95% CI 1.7-2.6).\"\nQuestion:\n\"Are reports of mechanical dysfunction in chronic oro-facial pain related to somatisation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17076091": {
                "source": [
                    "\"We sought to determine whether patients with obstructive sleep apnea (OSA) had an objective change in aerobic fitness during cycle ergometry compared to a normal population. The most accurate test of aerobic fitness is measurement of maximum oxygen consumption (VO2max) with cycle ergometry.\nWe performed a retrospective cohort analysis (247 patients with OSA) of VO2max from annual cycle ergometry tests compared to a large control group (normative data from 1.4 million US Air Force tests) in a tertiary care setting.\nOverall, individuals with OSA had increased VO2max when compared to the normalized US Air Force data (p<.001). Patients with an apnea-hypopnea index of greater than 20 demonstrated a decreased VO2max as compared to normalized values (p<.001). No differences in VO2max were observed after either medical or surgical therapy for OSA.\"\nQuestion:\n\"Does obstructive sleep apnea affect aerobic fitness?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26418441": {
                "source": [
                    "\"Polyps identified at colonoscopy are predominantly diminutive (<5\u2009mm) with a small risk (>1%) of high-grade dysplasia or carcinoma; however, the cost of histological assessment is substantial.AIM: The aim of this study was to determine whether prediction of colonoscopy surveillance intervals based on real-time endoscopic assessment of polyp histology is accurate and cost effective.\nA prospective cohort study was conducted across a tertiary care and private community hospital. Ninety-four patients underwent colonoscopy and polypectomy of diminutive (\u22645\u2009mm) polyps from October 2012 to July 2013, yielding a total of 159 polyps. Polyps were examined and classified according to the Sano-Emura classification system. The endoscopic assessment (optical diagnosis) of polyp histology was used to predict appropriate colonoscopy surveillance intervals. The main outcome measure was the accuracy of optical diagnosis of diminutive colonic polyps against the gold standard of histological assessment.\nOptical diagnosis was correct in 105/108 (97.2%) adenomas. This yielded a sensitivity, specificity and positive and negative predictive values (with 95%CI) of 97.2% (92.1-99.4%), 78.4% (64.7-88.7%), 90.5% (83.7-95.2%) and 93% (80.9-98.5%) respectively. Ninety-two (98%) patients were correctly triaged to their repeat surveillance colonoscopy. Based on these findings, a cut and discard approach would have resulted in a saving of $319.77 per patient.\"\nQuestion:\n\"Can we ease the financial burden of colonoscopy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19683101": {
                "source": [
                    "\"In this study, we investigated D-dimer serum level as a diagnostic parameter for acute appendicitis.\nForty-nine patients were enrolled in the study. Patients were classified according to age; sex; duration between the beginning of pain and referral to a hospital or clinic; Alvarado scores; and in physical examination, presence of muscular defense, the number of leukocytes, preoperative ultrasonography, and D-dimer levels of histopathologic study groups were analyzed.\nOf the patients enrolled in the study, 26.5% were females and 73.5% males. The average age was 21 years (range, 16-38 years) and 81.7% acute appendicitis (AA). According the duration of pain, 63.2% of the patients were referred to the hospital within the first 24 hours, 26.5% of the patients were referred to the hospital within 24 to 48 hours, and 10.3% were referred to the hospital within a period of more than 48 hours. No statistically significant difference was determined regarding D-dimer levels between the histopathologic study groups (P>.05). Alvarado scores lower than 7 were found in 36.7% and 7 or higher in 63.3% of the patients. There was no statistically significant difference related with D-dimer levels between histopathologic study groups (P>.05). The ratio of cases with a number of leukocytes below the upper limit were determined respectively as 32.7% and 67.3%, and no statistically significant difference was found regarding d-dimer levels between histopathologic study groups (P>.05).\"\nQuestion:\n\"Can D-dimer become a new diagnostic parameter for acute appendicitis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20337874": {
                "source": [
                    "\"Infants referred to our institution with a final diagnosis of ARM were retrospectively reviewed between 2001 and 2009. The first cohort consisted of patients that were referred between November 2001 and November 2006 with the diagnosis of an ARM that had been delayed for more than 48 h. The second cohort was those referred between December 2006 and May 2009 with whom the diagnosis of ARM had not been made within 24 h of birth.\nNineteen infants were referred with delayed diagnosis of an ARM over the 7.5 years of the study. Of 44 patients referred to our institution between December 2006 and May 2009, diagnosis of an ARM was delayed more than 24 h in 14 (32%). There was no difference in gender, birth weight, prematurity, type of malformation or presence of associated anomalies between those with timely and delayed diagnosis of their ARM. A significantly greater proportion of those with a delayed diagnosis presented with obstructive symptoms (86% vs. 27%, P<0.001), including abdominal distension (57%) and delayed passage of meconium or stool (29%). Despite undergoing neonatal examination, the diagnosis of ARM was missed in 12 patients overall.\"\nQuestion:\n\"Delayed diagnosis of anorectal malformations: are current guidelines sufficient?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10375486": {
                "source": [
                    "\"to describe variation in utilisation of carotid endarterectomy (CEA) within two English health regions and explore relationships between use, need and proximity to services.\nconsecutive case series of operations. Comparison at a population level with district stroke mortality, hospital admissions and material deprivation.\nstandardised utilisation rates for CEA and measures of inter-district variability. Spearman's rank correlation coefficients for associations between variables.\nvariation in utilisation rates was considerable (14-fold difference across district populations). More individuals had bilateral surgery in the Yorkshire region than in the Northern (11.7% vs. 5.5%, p=0.002). There was no association between utilisation rates for CEA and district stroke mortality (r=-0.06, 95% CI -0.41 to 0.30) or admission rates for stroke (r=0.17, 95% CI -0.2 to 0.49). There was a strong relationship between residence in districts where services were located and higher utilisation. Rates of CEA were lowest in the regions' most affluent wards.\"\nQuestion:\n\"Are variations in the use of carotid endarterectomy explained by population Need?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26215326": {
                "source": [
                    "\"The objective of the study was to determine whether risk of recurrent preterm birth differs based on the clinical presentation of a prior spontaneous preterm birth (SPTB): advanced cervical dilatation (ACD), preterm premature rupture of membranes (PPROM), or preterm labor (PTL).\nThis retrospective cohort study included singleton pregnancies from 2009 to 2014 complicated by a history of prior SPTB. Women were categorized based on the clinical presentation of their prior preterm delivery as having ACD, PPROM, or PTL. Risks for sonographic short cervical length and recurrent SPTB were compared between women based on the clinical presentation of their prior preterm birth. Log-linear regression was used to control for confounders.\nOf 522 patients included in this study, 96 (18.4%) had prior ACD, 246 (47.1%) had prior PPROM, and 180 (34.5%) had prior PTL. Recurrent PTB occurred in 55.2% of patients with a history of ACD compared with 27.2% of those with PPROM and 32.2% with PTL (P = .001). The mean gestational age at delivery was significantly lower for those with a history of ACD (34.0 weeks) compared with women with prior PPROM (37.2 weeks) or PTL (37.0 weeks) (P = .001). The lowest mean cervical length prior to 24 weeks was significantly shorter in patients with a history of advanced cervical dilation when compared with the other clinical presentations.\"\nQuestion:\n\"Does the clinical presentation of a prior preterm birth predict risk in a subsequent pregnancy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18832500": {
                "source": [
                    "\"The gap between evidence-based treatments and routine care has been well established. Findings from the Sequenced Treatments Alternatives to Relieve Depression (STAR*D) emphasized the importance of measurement-based care for the treatment of depression as a key ingredient for achieving response and remission; yet measurement-based care approaches are not commonly used in clinical practice.\nThe Nine-Item Patient Health Questionnaire (PHQ-9) for monitoring depression severity was introduced in 19 diverse psychiatric practices. During the one-year course of the project the helpfulness and feasibility of implementation of PHQ-9 in these psychiatric practices were studied. The project was modeled after the Institute for Healthcare Improvement Breakthrough Series. Two of the 19 practices dropped out during the course of the project.\nBy the conclusion of the study, all remaining 17 practices had adopted PHQ-9 as a routine part of depression care in their practice. On the basis of responses from 17 psychiatrists from those practices, PHQ-9 scores influenced clinical decision making for 93% of 6,096 patient contacts. With the additional information gained from the PHQ-9 score, one or more treatment changes occurred during 40% of these clinical contacts. Changing the dosage of antidepressant medication and adding another medication were the most common treatment changes recorded by psychiatrists, followed by starting or increasing psychotherapy and by switching or initiating antidepressants. In 3% of the patient contacts, using the PHQ-9 led to additional suicide risk assessment.\"\nQuestion:\n\"Systematic use of patient-rated depression severity monitoring: is it helpful and feasible in clinical psychiatry?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26298839": {
                "source": [
                    "\"To investigate the effectiveness of acupuncture in treating phonotraumatic vocal fold lesions.STUDY DESIGN/\nA total of 123 dysphonic individuals with benign vocal pathologies were recruited. They were given either genuine acupuncture (n\u00a0=\u00a040), sham acupuncture (n\u00a0=\u00a044), or no treatment (n\u00a0=\u00a039) for 6\u00a0weeks (two 30-minute sessions/wk). The genuine acupuncture group received needles puncturing nine voice-related acupoints for 30\u00a0minutes, two times a week for 6\u00a0weeks, whereas the sham acupuncture group received blunted needles stimulating the skin surface of the nine acupoints for the same frequency and duration. The no-treatment group did not receive any intervention but attended just the assessment sessions. One-hundred seventeen subjects completed the study (genuine acupuncture\u00a0=\u00a040; sham acupuncture\u00a0=\u00a043; and no treatment\u00a0=\u00a034), but only 84 of them had a complete set of vocal functions and quality of life measures (genuine acupuncture\u00a0=\u00a029; sham acupuncture\u00a0=\u00a033; and no-treatment\u00a0=\u00a022) and 42 of them with a complete set of endoscopic data (genuine acupuncture\u00a0=\u00a016; sham acupuncture\u00a0=\u00a015; and no treatment\u00a0=\u00a011).\nSignificant improvement in vocal function, as indicated by the maximum fundamental frequency produced, and also perceived quality of life, were found in both the genuine and sham acupuncture groups, but not in the no-treatment group. Structural (morphological) improvements were, however, only noticed in the genuine acupuncture group, which demonstrated a significant reduction in the size of the vocal fold lesions.\"\nQuestion:\n\"Is Acupuncture Efficacious for Treating Phonotraumatic Vocal Pathologies?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21689015": {
                "source": [
                    "\"Canine-assisted therapy has been receiving growing attention as a means of aiding children with autism spectrum disorder (ASD). Yet, only limited studies have been done and a great deal of literature related to this intervention is anecdotal. The present study aims at providing additional quantitative evidence on the potential of dogs to positively modulate the behavior of children with ASD.SETTINGS/\nA 12-year-old boy diagnosed with ASD was exposed, at his usual treatment location (the Portuguese Association for Developmental Disorders and Autism at Vila Nova de Gaia, Portugal), to the following treatment conditions: (1) one-to-one structured activities with a therapist assisted by a certified therapy dog, and (2) one-to-one structured activities with the same therapist alone (as a control). To accurately assess differences in the behavior of the participant between these treatment conditions, the therapist followed a strict research protocol. The behavior of the participant was continuously video-recorded during both treatment conditions for further analysis and comparison. Treatment outcomes: In the presence of the dog, the participant exhibited more frequent and longer durations of positive behaviors (such as smiling and positive physical contacting) as well as less frequent and shorter durations of negative behaviors (such as aggressive manifestations).\"\nQuestion:\n\"Can dogs prime autistic children for therapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18693227": {
                "source": [
                    "\"This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center.\nBetween June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan.\nPatient characteristics included a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.\"\nQuestion:\n\"Does a geriatric oncology consultation modify the cancer treatment plan for elderly patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23690198": {
                "source": [
                    "\"Social and cultural factors combined with little information may prevent the diffusion of epidural analgesia for pain relief during childbirth. The present study was launched contemporarily to the implementation of analgesia for labor in our Department in order to perform a 2 years audit on its use. The goal is to evaluate the epidural acceptance and penetration into hospital practice by women and care givers and safety and efficacy during childbirth.\nThis audit cycle measured epidural analgesia performance against 4 standards: (1) Implementation of epidural analgesia for labor to all patients; (2) Acceptance and good satisfaction level reported by patients and caregivers. (3) Effectiveness of labor analgesia; (4) No maternal or fetal side effects.\nDuring the audit period epidural analgesia increased from 15.5% of all labors in the first trimester of the study to 51% in the last trimester (p<0.005). Satisfaction levels reported by patients and care givers were good. A hierarchical clustering analysis identified two clusters based on VAS (Visual Analogue Scale) time course: in 226 patients (cluster 1) VAS decreased from 8.5\u00b11.4 before to 4.1\u00b11.3 after epidural analgesia; in 1002 patients (cluster 2) VAS decreased from 8.12\u00b11.7 before (NS vs cluster 1), to 0.76\u00b10.79 after (p<0.001 vs before and vs cluster 2 after). No other differences between clusters were observed.\"\nQuestion:\n\"Implementation of epidural analgesia for labor: is the standard of effective analgesia reachable in all women?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26209118": {
                "source": [
                    "\"Children with sickle cell disease (SCD) are at risk of bone infarcts and acute osteomyelitis. The clinical differentiation between a bone infarct and acute osteomyelitis is a diagnostic challenge. Unenhanced T1-W fat-saturated MR images have been proposed as a potential tool to differentiate bone infarcts from osteomyelitis.\nTo evaluate the reliability of unenhanced T1-W fat-saturated MRI for differentiation between bone infarcts and acute osteomyelitis in children with SCD.\nWe retrospectively reviewed the records of 31 children (20 boys, 11 girls; mean age 10.6 years, range 1.1-17.9 years) with SCD and acute bone pain who underwent MR imaging including unenhanced T1-W fat-saturated images from 2005 to 2010. Complete clinical charts were reviewed by a pediatric hematologist with training in infectious diseases to determine a clinical standard to define the presence or absence of osteomyelitis. A pediatric radiologist reviewed all MR imaging and was blinded to clinical information. Based on the signal intensity in T1-W fat-saturated images, the children were further classified as positive for osteomyelitis (low bone marrow signal intensity) or positive for bone infarct (high bone marrow signal intensity).\nBased on the clinical standard, 5 children were classified as positive for osteomyelitis and 26 children as positive for bone infarct (negative for osteomyelitis). The bone marrow signal intensity on T1-W fat-saturated imaging was not significant for the differentiation between bone infarct and osteomyelitis (P\u2009=\u20090.56). None of the additional evaluated imaging parameters on unenhanced MRI proved reliable in differentiating these diagnoses.\"\nQuestion:\n\"Utility of unenhanced fat-suppressed T1-weighted MRI in children with sickle cell disease -- can it differentiate bone infarcts from acute osteomyelitis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19398929": {
                "source": [
                    "\"Cholecystectomy for GB polyps that are larger than 10 mm is generally recommended because of the high probability of neoplasm. In contrast, a follow-up strategy is preferred for GB polyps smaller than 10 mm. However, there are no treatment guidelines for polyps that grow in size during the follow-up period.STUDY: We retrospectively investigated 145 patients with GB polyps who underwent at least 1 ultrasonographic follow-up examination over an interval greater than 6 months, before cholecystectomy at Samsung medical center, South Korea, from 1994 to 2007. The growth rate was determined based on the change in size per time interval between 2 ultrasonographic examinations (mm/mo).\nThe median age of the patients was 48 years (range: 25 to 75). One hundred twenty-five non-neoplastic polyps and 20 neoplastic polyps were found. Neoplastic polyps were more frequently found in patients older than 60 years, those with hypertension, a polyp size greater than 10 mm, and a rapid growth rate greater than 0.6 mm/mo. On multivariate analysis, however, the growth rate was not related to the neoplastic nature of a polyp, but older age (>60 y) and large size (>10 mm) were significantly associated with neoplastic polyps.\"\nQuestion:\n\"Can the growth rate of a gallbladder polyp predict a neoplastic polyp?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18507507": {
                "source": [
                    "\"Specialty pharmaceuticals have evolved beyond their status as niche drugs designed to treat rare conditions and are now poised to become the standard of care in a wide variety of common chronic illnesses. Due in part to the cost of these therapies, payers are increasingly demanding evidence of their value. Determining the value of these medications is hampered by a lack of robust pharmacoeconomic data.\nTo outline emerging strategies and case study examples for the medical and pharmacy benefits management of specialty pharmaceuticals.\nThe promise of specialty pharmaceuticals: increased life expectancy, improved quality of life, enhanced workplace productivity, decreased burden of disease, and reduced health care spending comes at a significant cost. These agents require special handling, administration, patient education, clinical support, and risk mitigation. Additionally, specialty drugs require distribution systems that ensure appropriate patient selection and data collection. With the specialty pharmaceutical pipeline overflowing with new medicines and an aging population increasingly relying on these novel treatments to treat common diseases, the challenge of managing the costs associated with these agents can be daunting. Aided by sophisticated pharmacoeconomic models to assess value, the cost impacts of these specialty drugs can be appropriately controlled.\"\nQuestion:\n\"The promise of specialty pharmaceuticals: are they worth the price?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21190419": {
                "source": [
                    "\"This study was planned to evaluate whether increased nuchal translucency (NT) thickness in the first trimester of gestation can be related to onset of gestational diabetes mellitus (GDM) during pregnancy.\nFrom January 2006 to August 2008, a group of 678 singleton pregnancies who had developed GDM has been selected as a study group among a total of 3966 pregnant women who had undergone first trimester screening for aneuploidies at 11-14 weeks of gestation. A group of 420 single pregnant women with physiological pregnancy were enrolled as control group. Both fetal structural and karyotype's anomalies were excluded in the two groups. NT was mesured by a Fetal Medicine Foundation certificated operator; GDM was diagnosed at 24-28 weeks of gestation following Carpenter and Coustan criteria. In the analyses of continuos variables, study and control group were compared by Student's t-test and Anova test.\nThere was no significative difference (p = 0.585) between NT values in the study (mean = 1.56) and control group (mean = 1.54).\"\nQuestion:\n\"Does nuchal translucency thickness in the first trimester predict GDM onset during pregnancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20674150": {
                "source": [
                    "\"A new edition of the TNM was recently released that includes modifications for the staging system of kidney cancers. Specifically, T2 cancers were subclassified into T2a and T2b (<or =10 cm vs>10 cm), tumors with renal vein involvement or perinephric fat involvement were classified as T3a cancers, and those with adrenal involvement were classified as T4 cancers.\nOur aim was to validate the recently released edition of the TNM staging system for primary tumor classification in kidney cancer.\nOur multicenter retrospective study consisted of 5339 patients treated in 16 academic Italian centers.\nPatients underwent either radical or partial nephrectomy.\nUnivariable and multivariable Cox regression models addressed cancer-specific survival (CSS) after surgery.\nIn the study, 1897 patients (35.5%) were classified as pT1a, 1453 (27%) as pT1b, 437 (8%) as pT2a, 153 (3%) as pT2b, 1059 (20%) as pT3a, 117 (2%) as pT3b, 26 (0.5%) as pT3c, and 197 (4%) as pT4. At a median follow-up of 42 mo, 786 (15%) had died of disease. In univariable analysis, patients with pT2b and pT3a tumors had similar CSS, as did patients with pT3c and pT4 tumors. Moreover, both pT3a and pT3b stages included patients with heterogeneous outcomes. In multivariable analysis, the novel classification of the primary tumor was a powerful independent predictor of CSS (p for trend<0.0001). However, the substratification of pT1 tumors did not retain an independent predictive role. The major limitations of the study are retrospective design, lack of central pathologic review, and the small number of patients included in some substages.\"\nQuestion:\n\"Validation of the 2009 TNM version in a large multi-institutional cohort of patients treated for renal cell carcinoma: are further improvements needed?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22236315": {
                "source": [
                    "\"Distance to provider might be an important barrier to timely diagnosis and treatment for cancer patients who qualify for Medicaid coverage. Whether driving time or driving distance is a better indicator of travel burden is also of interest.\nDriving distances and times from patient residence to primary care provider were calculated for 3,917 breast, colorectal (CRC) and lung cancer Medicaid patients in Washington State from 1997 to 2003 using MapQuest.com. We fitted regression models of stage at diagnosis and time-to-treatment (number of days between diagnosis and surgery) to test the hypothesis that travel burden is associated with timely diagnosis and treatment of cancer.\nLater stage at diagnosis for breast cancer Medicaid patients is associated with travel burden (OR = 1.488 per 100 driving miles, P= .037 and OR = 1.270 per driving hour, P= .016). Time-to-treatment after diagnosis of CRC is also associated with travel burden (14.57 days per 100 driving miles, P= .002 and 5.86 days per driving hour, P= .018).\"\nQuestion:\n\"Is distance to provider a barrier to care for medicaid patients with breast, colorectal, or lung cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18086459": {
                "source": [
                    "\"The aim of this study was to determine whether postictal psychotic episodes (PIPE) are predictive of the development of interictal psychotic episodes (IPE).\nThis was a retrospective study of 18 consecutive adults with a partial seizure disorder and PIPE (study group) and 36 patients with a partial seizure disorder but without PIPE (control group). These two groups were compared with respect to the likelihood of developing IPE over an 8-year follow-up period and the variables operant in the development of IPE. Statistical analyses consisted of logistic regression models to identify the variables predictive of the development of IPE. Predictors included: number and location of ictal foci, seizure type, etiology, age at seizure onset, duration of seizure disorder, MRI abnormalities, and psychiatric history prior to the index video/EEG monitoring (other than PIPE).\nSeven patients with PIPE and one control patient went on to develop an IPE. Predictors of IPE in univariate logistic regression analyses included a history of PIPE (P=0.006), male gender (P=0.028), and having bilateral ictal foci (P=0.048). Significance disappeared for all of these variables when they were entered into a multivariate analysis.\"\nQuestion:\n\"Long-term significance of postictal psychotic episodes II. Are they predictive of interictal psychotic episodes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            }
        }
    }
}
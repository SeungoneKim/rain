{
    "pubmed_qa": {
        "pubmed_qa": {
            "29112560": {
                "source": [
                    "\"It is unclear whether traveling long distances to high-volume centers would compensate for travel burden among patients undergoing rectal cancer resection.\nThe purpose of this study was to determine whether operative volume outweighs the advantages of being treated locally by comparing the outcomes of patients with rectal cancer treated at local, low-volume centers versus far, high-volume centers.\nThis was a population-based study.\nThe National Cancer Database was queried for patients with rectal cancer.\nPatients with stage II or III rectal cancer who underwent surgical resection between 2006 and 2012 were included.\nThe outcomes of interest were margins, lymph node yield, receipt of neoadjuvant chemoradiation, adjuvant chemotherapy, readmission within 30 days, 30-day and 90-day mortality, and 5-year overall survival.\nA total of 18,605 patients met inclusion criteria; 2067 patients were in the long-distance/high-volume group and 1362 in the short-distance/low-volume group. The median travel distance was 62.6 miles for the long-distance/high-volume group and 2.3 miles for the short-distance/low-volume group. Patients who were younger, white, privately insured, and stage III were more likely to have traveled to a high-volume center. When controlled for patient factors, stage, and hospital factors, patients in the short-distance/low-volume group had lower odds of a lymph node yield \u226512 (OR = 0.51) and neoadjuvant chemoradiation (OR = 0.67) and higher 30-day (OR = 3.38) and 90-day mortality (OR = 2.07) compared with those in the long-distance/high-volume group. The short-distance/low-volume group had a 34% high risk of overall mortality at 5 years compared with the long-distance/high-volume group.\nWe lacked data regarding patient and physician decision making and surgeon-specific factors.\"\nQuestion:\n\"Is the Distance Worth It?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26134053": {
                "source": [
                    "\"Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\nWe distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\nWe received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\"\nQuestion:\n\"Outcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27217036": {
                "source": [
                    "\"Longer duration of neoadjuvant (NA) imatinib\u00a0(IM) used for locally advanced (LA) gastrointestinal stromal tumours (GIST) is not based on biology of the tumour reflected by kit mutation analysis.\nLA or locally recurrent (LR) GIST treated with NA IM from May 2008 to March 2015 from a prospective database were included in\u00a0the analysis. Archived formalin-fixed paraffin-embedded tissues (FFPE) were used for testing KIT exons 9, 11, 13 and 17 by PCR.\nOne hundred twenty-five patients with LA or LR GIST were treated with NA IM. Forty-five patients (36\u00a0%) had undergone c-kit mutation testing. Exon 11 was seen in 25 patients (55.5\u00a0%), 3 with exon 9 (6.7\u00a0%) and 2 with exon 13 (4.4\u00a0%). Twelve were wild type (26.6\u00a0%) and \u00a03 (6.7 %) were declared uninterpretable. Response rate (RR) for the exon 11 mutants was higher than the non-exon 11 mutant group (84 vs. 40\u00a0%, p\u2009=\u20090.01). Disease stabilization rate (DSR) rates were also higher in the exon 11 subgroup than non-exon 11 group (92 vs. 75\u00a0%). Eighty-four per cent exon 11 and 75\u00a0% non-exon 11 mutants were surgical candidates. Patients undergoing surgery had significantly improved event free survival (EFS) (p\u2009<\u20090.001) compared to patients not undergoing surgery, with the same trend seen in OS (p\u2009=\u20090.021). Patients with a SD on response to NA IM had a lower EFS (p\u2009=\u20090.076) and OS compared to patients achieving CR/PR. There were no differences between the various exon variants in terms of outcomes and responses\"\nQuestion:\n\"Neoadjuvant Imatinib in Locally Advanced Gastrointestinal stromal Tumours, Will Kit Mutation Analysis Be a Pathfinder?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23794696": {
                "source": [
                    "\"To investigate the effect of bracket-ligature combination on the amount of orthodontic space closure over three months.\nRandomized clinical trial with three parallel groups.\nA hospital orthodontic department (Chesterfield Royal Hospital, UK).\nForty-five patients requiring upper first premolar extractions.\nInformed consent was obtained and participants were randomly allocated into one of three groups: (1) conventional pre-adjusted edgewise brackets and elastomeric ligatures; (2) conventional pre-adjusted edgewise brackets and Super Slick(\u00ae) low friction elastomeric ligatures; (3) Damon 3MX(\u00ae) passive self-ligating brackets. Space closure was undertaken on 0\u00b7019\u00d70\u00b7025-inch stainless steel archwires with nickel-titanium coil springs. Participants were recalled at four weekly intervals. Upper alginate impressions were taken at each visit (maximum three). The primary outcome measure was the mean amount of space closure in a 3-month period.\nA one-way ANOVA was undertaken [dependent variable: mean space closure (mm); independent variable: group allocation]. The amount of space closure was very similar between the three groups (1 mm per 28 days); however, there was a wide variation in the rate of space closure between individuals. The differences in the amount of space closure over three months between the three groups was very small and non-significant (P\u200a=\u200a0\u00b7718).\"\nQuestion:\n\"Does the bracket-ligature combination affect the amount of orthodontic space closure over three months?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27642458": {
                "source": [
                    "\"Polio eradication is now feasible after removal of Nigeria from the list of endemic countries and global reduction of cases of wild polio virus in 2015 by more than 80%. However, all countries must remain focused to achieve eradication. In August 2015, the Catholic bishops in Kenya called for boycott of a polio vaccination campaign citing safety concerns with the polio vaccine. We conducted a survey to establish if the coverage was affected by the boycott.\nA cross sectional survey was conducted in all the 32 counties that participated in the campaign. A total of 90,157 children and 37,732 parents/guardians were sampled to determine the vaccination coverage and reasons for missed vaccination.\nThe national vaccination coverage was 93% compared to 94% in the November 2014 campaign. The proportion of parents/guardians that belonged to Catholic Church was 31% compared to 7% of the children who were missed. Reasons for missed vaccination included house not being visited (44%), children not being at home at time of visit (38%), refusal by parents (12%), children being as leep (1%), and various other reasons (5%). Compared to the November 2014 campaign, the proportion of children who were not vaccinated due to parent's refusal significantly increased from 6% to 12% in August 2015.\"\nQuestion:\n\"Did the call for boycott by the Catholic bishops affect the polio vaccination coverage in Kenya in 2015?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18182265": {
                "source": [
                    "\"In this study, the authors discussed the feasibility and value of diffusion-weighted (DW) MR imaging in the detection of uterine endometrial cancer in addition to conventional nonenhanced MR images.\nDW images of endometrial cancer in 23 patients were examined by using a 1.5-T MR scanner. This study investigated whether or not DW images offer additional incremental value to conventional nonenhanced MR imaging in comparison with histopathological results. Moreover, the apparent diffusion coefficient (ADC) values were measured in the regions of interest within the endometrial cancer and compared with those of normal endometrium and myometrium in 31 volunteers, leiomyoma in 14 patients and adenomyosis in 10 patients. The Wilcoxon rank sum test was used, with a p<0.05 considered statistically significant.\nIn 19 of 23 patients, endometrial cancers were detected only on T2-weighted images. In the remaining 4 patients, of whom two had coexisting leiomyoma, no cancer was detected on T2-weighted images. This corresponds to an 83% detection sensitivity for the carcinomas. When DW images and fused DW images/T2-weighted images were used in addition to the T2-weighted images, cancers were identified in 3 of the remaining 4 patients in addition to the 19 patients (overall detection sensitivity of 96%). The mean ADC value of endometrial cancer (n=22) was (0.97+/-0.19)x10(-3)mm(2)/s, which was significantly lower than those of the normal endometrium, myometrium, leiomyoma and adenomyosis (p<0.05).\"\nQuestion:\n\"Body diffusion-weighted MR imaging of uterine endometrial cancer: is it helpful in the detection of cancer in nonenhanced MR imaging?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19401574": {
                "source": [
                    "\"To evaluate the diagnostic accuracy of gadofosveset-enhanced magnetic resonance (MR) angiography in the assessment of carotid artery stenosis, with digital subtraction angiography (DSA) as the reference standard, and to determine the value of reading first-pass, steady-state, and \"combined\" (first-pass plus steady-state) MR angiograms.\nThis study was approved by the local ethics committee, and all subjects gave written informed consent. MR angiography and DSA were performed in 84 patients (56 men, 28 women; age range, 61-76 years) with carotid artery stenosis at Doppler ultrasonography. Three readers reviewed the first-pass, steady-state, and combined MR data sets, and one independent observer evaluated the DSA images to assess stenosis degree, plaque morphology and ulceration, stenosis length, and tandem lesions. Interobserver agreement regarding MR angiographic findings was analyzed by using intraclass correlation and Cohen kappa coefficients. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated by using the McNemar test to determine possible significant differences (P<.05).\nInterobserver agreement regarding all MR angiogram readings was substantial. For grading stenosis, sensitivity, specificity, PPV, and NPV were, respectively, 90%, 92%, 91%, and 91% for first-pass imaging; 95% each for steady-state imaging; and 96%, 99%, 99%, and 97% for combined imaging. For evaluation of plaque morphology, respective values were 84%, 86%, 88%, and 82% for first-pass imaging; 98%, 97%, 98%, and 97% for steady-state imaging; and 98%, 100%, 100%, and 97% for combined imaging. Differences between the first-pass, steady-state, and combined image readings for assessment of stenosis degree and plaque morphology were significant (P<.001).\"\nQuestion:\n\"Gadofosveset-enhanced MR angiography of carotid arteries: does steady-state imaging improve accuracy of first-pass imaging?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25079920": {
                "source": [
                    "\"As parents of young children are often unaware their child is overweight, screening provides the opportunity to inform parents and provide the impetus for behaviour change. We aimed to determine if parents could recall and understand the information they received about their overweight child after weight screening.\nRandomised controlled trial of different methods of feedback.\nParticipants were recruited through primary and secondary care but appointments took place at a University research clinic.\n1093 children aged 4-8\u2005years were screened. Only overweight children (n=271, 24.7%) are included in this study. Parents of overweight children were randomised to receive feedback regarding their child's weight using best practice care (BPC) or motivational interviewing (MI) at face-to-face interviews typically lasting 20-40\u2005min. 244 (90%) parents participated in a follow-up interview 2\u2005weeks later to assess recall and understanding of information from the feedback session.\nInterviews were audio-taped and transcribed verbatim before coding for amount and accuracy of recall. Scores were calculated for total recall and sub-categories of interest.\nOverall, 39% of the information was recalled (mean score 6.3 from possible score of 16). Parents given feedback via BPC recalled more than those in the MI group (difference in total score 0.48; 95% CI 0.05 to 0.92). Although 94% of parents were able to correctly recall their child's weight status, fewer than 10 parents could accurately describe what the measurements meant. Maternal education (0.81; 0.25 to 1.37) and parental ratings of how useful they found the information (0.19; 0.04 to 0.35) were significant predictors of recall score in multivariate analyses.\"\nQuestion:\n\"Do parents recall and understand children's weight status information after BMI screening?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19106867": {
                "source": [
                    "\"Recent suicide bombings pose the novel problem for Trauma Centers of the massive simultaneous arrival of many gravely wounded patients.\nWe report the experience of the French-German Military Trauma Group, a Level 2 Trauma Center, in Afghanistan during the wave of suicide bombings in February 2007.\nFourteen casualties were received. A first triage was carried out by the U S Army Level I group prior to evacuation. A second surgical triage was carried out with systematic ultrasound exam. Four cases (ISS>25) were re-categorized and underwent emergency surgical procedures.\"\nQuestion:\n\"The Main Gate Syndrome: a new format in mass-casualty victim \"surge\" management?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25168472": {
                "source": [
                    "\"The intent of this study was to determine if the use of a single or combination of static foot posture measurements can be used to predict rearfoot, midfoot, and forefoot plantar surface area in individuals with pronated or normal foot types.\nTwelve foot measurements were collected on 52 individuals (mean age 25.8 years) with the change in midfoot width used to place subjects in a pronated or normal foot mobility group. Dynamic plantar contact area was collected during walking with a pressure sensor platform. The 12 measures were entered into a stepwise regression analysis to determine the optimal set of measures associated with regional plantar surface area.\nA two variable model was found to describe the relationship between the foot measurements and forefoot plantar contact area (r(2)=0.79, p<0.0001). A four variable model was found to describe the relationship between the foot measurements and midfoot plantar contact area (r(2)=0.85, p<0.0001) in those individuals with a 1.26cm or greater change in midfoot width.\"\nQuestion:\n\"Can static foot posture measurements predict regional plantar surface area?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9191526": {
                "source": [
                    "\"In an attempt to improve the care they provide for their patients with breast cancer, the authors' institution developed a multidisciplinary breast cancer clinic (MDBCC) to offer \"one-stop shopping\" consultation and support for newly diagnosed breast cancer patients.\nOne hundred sixty-two patients, the control group for this study, were evaluated at Henry Ford Hospital during the year prior to the opening of the MDBCC. These patients, who were referred in the traditional sequential consultation manner, were compared with the first 177 patients seen during the first year of the clinic's operation. Retrospective chart reviews were conducted to assess treatment timeliness, and anonymous questionnaires were used to assess patient satisfaction.\nThe authors found that the MDBCC increased patient satisfaction by encouraging involvement of patients' families and friends and by helping patients make treatment decisions (P<0.001). The time between diagnosis and the initiation of treatment was also significantly decreased (42.2 days vs. 29.6 days; P<0.0008).\"\nQuestion:\n\"Multidisciplinary breast cancer clinics. Do they work?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23761381": {
                "source": [
                    "\"Testosterone measurement by liquid chromatography tandem mass spectrometry (LC-MS/MS) is well accepted as the preferred technique for the analysis of testosterone. Variation is seen between assays and this may be due to differences in calibration as commercial calibrators for this assay are not readily available. We investigated the effects calibration in routine clinical LC-MS/MS assays.\nAll LC-MS/MS users that were registered with the UKNEQAS external quality assurance scheme for testosterone were invited to take part in the study. A set of seven serum samples and serum-based calibrators were sent to all laboratories that expressed an interest. The laboratories were instructed to analyse all samples using there own calibrators and return the results and a method questionnaire for analysis.\nFifteen laboratories took part in the study. There was no consensus on supplier of testosterone or matrix for the preparation of calibrators and all were prepared in-house. Also, a wide variety of mass spectrometers, internal standards, chromatography conditions and sample extractions were used. The variation in results did not improve when the results were corrected with a common calibrator.\"\nQuestion:\n\"Is calibration the cause of variation in liquid chromatography tandem mass spectrometry testosterone measurement?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22954812": {
                "source": [
                    "\"Recent reports indicate that the prevalence of bipolar disorder (BD) in patients with an acute major depressive episode might be higher than previously thought. We aimed to study systematically all patients who sought therapy for major depressive episode (MDE) within the BRIDGE study in Germany, reporting on an increased number (increased from 2 in the international BRIDGE report to 5) of different diagnostic algorithms.\nA total of 252 patients with acute MDE (DSM-IV confirmed) were examined for the existence of BD (a) according to DSM-IV criteria, (b) according to modified DSM-IV criteria (without the exclusion criterion of 'mania not induced by substances/antidepressants'), (c) according to a Bipolarity Specifier Algorithm which expands the DSM-IV criteria, (d) according to HCL-32R (Hypomania-Checklist-32R), and (e) according to a criteria-free physician's diagnosis.\nThe five different diagnostic approaches yielded immensely variable prevalences for BD: (a) 11.6; (b) 24.8%; (c) 40.6%; (d) 58.7; e) 18.4% with only partial overlap between diagnoses according to the physician's diagnosis or HCL-32R with diagnoses according to the three DSM-based algorithms.\"\nQuestion:\n\"Are bipolar disorders underdiagnosed in patients with depressive episodes?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23810330": {
                "source": [
                    "\"Intraoperative neuromonitoring (IONM) aims to control nerve-sparing total mesorectal excision (TME) for rectal cancer in order to improve patients' functional outcome. This study was designed to compare the urogenital and anorectal functional outcome of TME with and without IONM of innervation to the bladder and the internal anal sphincter.\nA consecutive series of 150 patients with primary rectal cancer were analysed. Fifteen match pairs with open TME and combined urogenital and anorectal functional assessment at follow up were established identical regarding gender, tumour site, tumour stage, neoadjuvant radiotherapy and type of surgery. Urogenital and anorectal function was evaluated prospectively on the basis of self-administered standardized questionnaires, measurement of residual urine volume and longterm-catheterization rate.\nNewly developed urinary dysfunction after surgery was reported by 1 of 15 patients in the IONM group and by 6 of 15 in the control group (p\u00a0=\u00a00.031). Postoperative residual urine volume was significantly higher in the control group. At follow up impaired anorectal function was present in 1 of 15 patients undergoing TME with IONM and in 6 of 15 without IONM (p\u00a0=\u00a00.031). The IONM group showed a trend towards a lower rate of sexual dysfunction after surgery.\"\nQuestion:\n\"Is intraoperative neuromonitoring associated with better functional outcome in patients undergoing open TME?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22680064": {
                "source": [
                    "\"To determine the ability of early sonogram to predict the presentation of twin A at birth.\nA retrospective cohort study was conducted on all twin pregnancies evaluated at our Fetal Evaluation Unit from 2007 to 2009. Sonogram records were reviewed for the presentation of twin A at seven gestational age intervals and inpatient medical records were reviewed for the presentation of twin A at delivery. The positive predictive value, sensitivity, and specificity of presentation as determined by ultrasound, at each gestational age interval, for the same presentation at delivery were calculated.\nTwo hundred and thirty-eight twin pregnancies met inclusion criteria. A total of 896 ultrasounds were reviewed. The positive predictive value of cephalic presentation of twin A as determined by ultrasound for the persistence of cephalic presentation at delivery reached 95% after 28 weeks gestation. The positive predictive value for noncephalic presentation as established by sonogram for noncephalic at delivery was>90% after 32 weeks gestation.\"\nQuestion:\n\"Can third trimester ultrasound predict the presentation of the first twin at delivery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26126304": {
                "source": [
                    "\"To compare in vitro fertilization (IVF) outcomes in low responders stimulated with microdose leuprolide protocol (ML) following pretreatment with either oral contraceptive pill (OCP) or luteal estradiol (E2) + GnRH antagonist (E2 + antag) for follicular synchronization prior to controlled ovarian hyperstimulation (COH).\nThis was a retrospective study of 130 women, who were poor responders, undergoing IVF with either OCP/ML or E2+ antag/ML protocols. The main outcome measures were ongoing pregnancy rates, number of oocytes retrieved, and cancellation rate.\nBoth groups were similar in baseline characteristics. There were no significant differences in gonadotropin requirement, cancellation rate, and number of embryos transferred. Ongoing pregnancy rates (40% vs. 15%) were significantly higher in the OCP/ML group. Trends toward greater number of oocytes retrieved (7.7 \u00b1 3.4 vs. 5.9 \u00b1 4.2) and improved implantation rates (20% vs. 12%) were also noted, but these did not reach statistical significance.\"\nQuestion:\n\"Estradiol and Antagonist Pretreatment Prior to Microdose Leuprolide in in Vitro Fertilization. Does It Improve IVF Outcomes in Poor Responders as Compared to Oral Contraceptive Pill?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27288618": {
                "source": [
                    "\"To determine whether prophylactic inhaled heparin is effective for the prevention and treatment of pneumonia patients receiving mechanical ventilation (MV) in the intensive care unit.\nA phase 2, double blind randomized controlled trial stratified for study center and patient type (non-operative, post-operative) was conducted in three university-affiliated intensive care units. Patients aged \u226518years and requiring invasive MV for more than 48hours were randomized to usual care, nebulization of unfractionated sodium heparin (5000 units in 2mL) or placebo nebulization with 0.9% sodium chloride (2mL) four times daily with the main outcome measures of the development of ventilator associated pneumonia (VAP), ventilator associated complication (VAC) and sequential organ failure assessment scores in patients with pneumonia on admission or who developed VAP.\nAustralian and New Zealand Clinical Trials Registry ACTRN12612000038897.\nTwo hundred and fourteen patients were enrolled (72 usual care, 71 inhaled sodium heparin, 71 inhaled sodium chloride). There were no differences between treatment groups in terms of the development of VAP, using either Klompas criteria (6-7%, P=1.00) or clinical diagnosis (24-26%, P=0.85). There was no difference in the clinical consistency (P=0.70), number (P=0.28) or the total volume of secretions per day (P=.54). The presence of blood in secretions was significantly less in the usual care group (P=0.005).\"\nQuestion:\n\"Is inhaled prophylactic heparin useful for prevention and Management of Pneumonia in ventilated ICU patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17940352": {
                "source": [
                    "\"To evaluate the impact of HER2 immunoreactivity on clinical outcome in locally advanced urothelial carcinoma patients who received surgery alone, or methotrexate, vinblastine, epirubicin, and cisplatin (M-VEC) as adjuvant chemotherapy.\nWe studied 114 formalin-fixed paraffin-embedded specimens obtained from locally advanced urothelial carcinoma patients receiving surgery alone or adjuvant M-VEC. The authors evaluated HER2 immunoreactivity using immunohistochemical staining and explored the influence of pathological parameters and HER2 immunoreactivity on progression-free survival (PFS) and disease-specific overall survival (OS) using univariate and multivariate Cox's analyses.\nUrothelial carcinoma of the bladder had a significantly higher frequency of HER2 immunoreactivity than that of the upper urinary tract (60.7 vs. 20.7%, p<0.0001). Overall, nodal status was a strong and independent prognostic indicator for clinical outcome. The HER2 immunoreactivity was significantly associated with PFS (p = 0.02) and disease-specific OS (p = 0.005) in advanced urothelial carcinoma patients. As for patients with adjuvant M-VEC, HER2 immunoreactivity was a significant prognostic factor for PFS (p = 0.03) and disease-specific OS (p = 0.02) using univariate analysis, but not multivariate analysis, and not for patients receiving watchful waiting.\"\nQuestion:\n\"Does HER2 immunoreactivity provide prognostic information in locally advanced urothelial carcinoma patients receiving adjuvant M-VEC chemotherapy?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10375486": {
                "source": [
                    "\"to describe variation in utilisation of carotid endarterectomy (CEA) within two English health regions and explore relationships between use, need and proximity to services.\nconsecutive case series of operations. Comparison at a population level with district stroke mortality, hospital admissions and material deprivation.\nstandardised utilisation rates for CEA and measures of inter-district variability. Spearman's rank correlation coefficients for associations between variables.\nvariation in utilisation rates was considerable (14-fold difference across district populations). More individuals had bilateral surgery in the Yorkshire region than in the Northern (11.7% vs. 5.5%, p=0.002). There was no association between utilisation rates for CEA and district stroke mortality (r=-0.06, 95% CI -0.41 to 0.30) or admission rates for stroke (r=0.17, 95% CI -0.2 to 0.49). There was a strong relationship between residence in districts where services were located and higher utilisation. Rates of CEA were lowest in the regions' most affluent wards.\"\nQuestion:\n\"Are variations in the use of carotid endarterectomy explained by population Need?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15466981": {
                "source": [
                    "\"The combined use of free and total prostate-specific antigen (PSA) in early detection of prostate cancer has been controversial. This article systematically evaluates the discriminating capacity of a large number of combination tests.\nFree and total PSA were analyzed in stored serum samples taken prior to diagnosis in 429 cases and 1,640 controls from the Physicians' Health Study. We used a classification algorithm called logic regression to search for clinically useful tests combining total and percent free PSA and receiver operating characteristic analysis and compared these tests with those based on total and complexed PSA. Data were divided into training and test subsets. For robustness, we considered 35 test-train splits of the original data and computed receiver operating characteristic curves for each test data set.\nThe average area under the receiver operating characteristic curve across test data sets was 0.74 for total PSA and 0.76 for the combination tests. Combination tests with higher sensitivity and specificity than PSA>4.0 ng/mL were identified 29 out of 35 times. All these tests extended the PSA reflex range to below 4.0 ng/mL. Receiver operating characteristic curve analysis indicated that the overall diagnostic performance as expressed by the area under the curve did not differ significantly for the different tests.\"\nQuestion:\n\"Prostate-specific antigen and free prostate-specific antigen in the early detection of prostate cancer: do combination tests improve detection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19309468": {
                "source": [
                    "\"A variable effect of inflammation on alloimmunization to transfused red blood cells (RBCs) in mice has been recently reported. We investigated whether RBC alloimmunization in humans was affected by transfusion of blood products in temporal proximity to experiencing a febrile transfusion reaction (FTR) to platelets (PLTs), an event predominantly mediated by inflammatory cytokines.\nBlood bank databases were used to identify patients who experienced an FTR or possible FTR to PLTs from August 2000 to March 2008 (FTR group). The control group of patients received a PLT transfusion on randomly selected dates without experiencing an FTR. The \"event\" was defined as the PLT transfusion that caused the FTR in the FTR group or the index PLT transfusion in the control group. The number of transfused blood products and their proximity to the event were recorded along with other recipient data. The primary endpoint was the rate of RBC alloimmunization between the two groups.\nThere were 190 recipients in the FTR group and 245 in the control group. Overall, the recipients in the control group were younger and received more blood products on the day of their event and over the subsequent 10 days. The alloimmunization rate among recipients in the FTR group was higher than in the control group (8% vs. 3%, respectively; p = 0.026).\"\nQuestion:\n\"Does a febrile reaction to platelets predispose recipients to red blood cell alloimmunization?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18565233": {
                "source": [
                    "\"Epidemiologic studies have suggested that hypertriglyceridemia and insulin resistance are related to the development of colon cancer. Nuclear peroxisome proliferator-activated receptors (PPAR), which play a central role in lipid and glucose metabolism, had been hypothesized as being involved in colon cancerogenesis. In animal studies the lipid-lowering PPAR ligand bezafibrate suppressed colonic tumors. However, the effect of bezafibrate on colon cancer development in humans is unknown. Therefore, we proposed to investigate a possible preventive effect of bezafibrate on the development of colon cancer in patients with coronary artery disease during a 6-year follow-up.\nOur population included 3011 patients without any cancer diagnosis who were enrolled in the randomized, double blind Bezafibrate Infarction Prevention (BIP) Study. The patients received either 400 mg of bezafibrate retard (1506 patients) or placebo (1505 patients) once a day. Cancer incidence data were obtained by matching a subject's identification numbers with the National Cancer Registry. Each matched record was checked for correct identification.\nDevelopment of new cancer (all types) was recorded in 177 patients: in 79 (5.25%) patients from the bezafibrate group vs. 98 (6.51%) from the placebo group. Development of colon cancer was recorded in 25 patients: in 8 (0.53%) patients from the bezafibrate group vs. 17 (1.13%) from the placebo group, (Fisher's exact test: one side p = 0.05; two side p = 0.07). A difference in the incidence of cancer was only detectable after a 4 year lag and progressively increased with continued follow-up. On multivariable analysis the colon cancer risk in patients who received bezafibrate tended to be lower with a hazard ratio of 0.47 and 95% confidence interval 0.2-1.1.\"\nQuestion:\n\"Does the lipid-lowering peroxisome proliferator-activated receptors ligand bezafibrate prevent colon cancer in patients with coronary artery disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9107172": {
                "source": [
                    "\"If long-term use of left ventricular assist devices (LVADs) as bridges to transplantation is successful, the issue of permanent device implantation in lieu of transplantation could be addressed through the creation of appropriately designed trials. Our medium-term experience with both pneumatically and electrically powered ThermoCardiosystems LVADs is presented to outline the benefits and limitations of device support in lieu of transplantation.\nDetailed records were kept prospectively for all patients undergoing LVAD insertion. Fifty-eight LVADs were inserted over 5 years, with a survival rate of 74%. Mean patient age was 50 years, and duration of support averaged 98 days. Although common, both preexisting infection and infection during LVAD support were not associated with increased mortality or decreased rate of successful transplantation. Thromboembolic complications were rare, occurring in only three patients (5%) despite the absence of anticoagulation. Ventricular arrhythmias were well tolerated in all patients except in cases of early perioperative right ventricular failure, with no deaths. Right ventricular failure occurred in one third of patients and was managed in a small percentage by right ventricular assist device (RVAD) support and/or inhaled nitric oxide therapy. There were no serious device malfunctions, but five graft-related hemorrhages resulted in two deaths. Finally, a variety of noncardiac surgical procedures were performed in LVAD recipients, with no major morbidity and mortality.\"\nQuestion:\n\"Bridge experience with long-term implantable left ventricular assist devices. Are they an alternative to transplantation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10575390": {
                "source": [
                    "\"To compare adherence to follow-up recommendations for colposcopy or repeated Papanicolaou (Pap) smears for women with previously abnormal Pap smear results.\nRetrospective cohort study.\nThree northern California family planning clinics.\nAll women with abnormal Pap smear results referred for initial colposcopy and a random sample of those referred for repeated Pap smear. Medical records were located and reviewed for 90 of 107 women referred for colposcopy and 153 of 225 women referred for repeated Pap smears.\nRoutine clinic protocols for follow-up--telephone call, letter, or certified letter--were applied without regard to the type of abnormality seen on a Pap smear or recommended examination.\nDocumented adherence to follow-up within 8 months of an abnormal result. Attempts to contact the patients for follow-up, adherence to follow-up recommendations, and patient characteristics were abstracted from medical records. The probability of adherence to follow-up vs the number of follow-up attempts was modeled with survival analysis. Cox proportional hazards models were used to examine multivariate relationships related to adherence.\nThe rate of overall adherence to follow-up recommendations was 56.0% (136/243). Adherence to a second colposcopy was not significantly different from that to a repeated Pap smear (odds ratio, 1.40; 95% confidence interval, 0.80-2.46). The use of as many as 3 patient reminders substantially improved adherence to follow-up. Women without insurance and women attending 1 of the 3 clinics were less likely to adhere to any follow-up recommendation (hazard ratio for no insurance, 0.43 [95% confidence interval, 0.20-0.93], and for clinic, 0.35 [95% confidence interval, 0.15-0.73]).\"\nQuestion:\n\"Do follow-up recommendations for abnormal Papanicolaou smears influence patient adherence?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28407529": {
                "source": [
                    "\"Patient outcome after resection of colorectal liver metastases (CLM) following second-line preoperative chemotherapy (PCT) performed for insufficient response or toxicity of the first-line, is little known and has here been compared to the outcome following first-line.\nFrom January 2005 to June 2013, 5624 and 791 consecutive patients of a prospective international cohort received 1 and 2 PCT lines before CLM resection (group 1 and 2, respectively). Survival and prognostic factors were analysed.\nAfter a mean follow-up of 30.1 months, there was no difference in survival from CLM diagnosis (median, 3-, and 5-year overall survival [OS]: 58.6 months, 76% and 49% in group 2 versus 58.9 months, 71% and 49% in group 1, respectively, P\u00a0=\u00a00.32). After hepatectomy, disease-free survival (DFS) was however shorter in group 2: 17.2 months, 27% and 15% versus 19.4 months, 32% and 23%, respectively (P\u00a0=\u00a00.001). Among the initially unresectable patients of group 1 and 2, no statistical difference in OS or DFS was observed. Independent predictors of worse OS in group 2 were positive primary lymph nodes, extrahepatic disease, tumour progression on second line, R2 resection\u00a0and number of hepatectomies/year<50. Positive primary nodes, synchronous and bilateral metastases were predictors of shorter DFS. Initial unresectability did not impact OS or DFS in group 2.\"\nQuestion:\n\"Resection of colorectal liver metastases after second-line chemotherapy: is it worthwhile?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23359100": {
                "source": [
                    "\"Heterotopic ossification is a common complication after total hip arthroplasty. Non-steroidal anti-inflammatory drugs (NSAIDs) are known to prevent heterotopic ossifications effectively, however gastrointestinal complaints are reported frequently. In this study, we investigated whether etoricoxib, a selective cyclo-oxygenase-2 (COX-2) inhibitor that produces fewer gastrointestinal side effects, is an effective alternative for the prevention of heterotopic ossification.\nWe investigated the effectiveness of oral etoricoxib 90 mg for seven days in a prospective two-stage study design for phase-2 clinical trials in a small sample of patients (n\u2009=\u200942). A cemented primary total hip arthroplasty was implanted for osteoarthritis. Six months after surgery, heterotopic ossification was determined on anteroposterior pelvic radiographs using the Brooker classification.\nNo heterotopic ossification was found in 62 % of the patients that took etoricoxib; 31 % of the patients had Brooker grade 1 and 7 % Brooker grade 2 ossification.\"\nQuestion:\n\"Is etoricoxib effective in preventing heterotopic ossification after primary total hip arthroplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26449554": {
                "source": [
                    "\"The pressures delivered by autotitrating continuous positive airways pressure (CPAP) devices not only treat obstructive sleep apnoea (OSA) effectively but also give potentially interesting physiological information about the forces impinging on the pharynx. In earlier work from this unit, we used correlations between autoCPAP pressure and both OSA severity and obesity, to construct an algorithm to estimate the fixed CPAP pressure a patient required for subsequent clinical use. We wished to discover if these relationships could be reliably extended to a much more obese group.\nWe performed a prospective cohort study in an obese population. Measurements of obesity were made, OSA severity was recorded, and the 95th centile autoCPAP pressure was recorded during 1\u00a0week of autoCPAP. Spearman's rank correlation was performed between measurements of obesity and autoCPAP pressure, and between OSA severity and autoCPAP pressure.\nFifty-four obese individuals (median body mass index (BMI) 43.0\u00a0kg/m(2)), 52\u00a0% of whom had OSA (apnoea-hypopnoea index (AHI)\u2009\u2265\u200915), had a median 95th centile autoCPAP pressure of 11.8\u2009cmH2O. We found no significant correlation between autoCPAP pressure and neck circumference, waist circumference or BMI. There was a moderate correlation between autoCPAP pressure and OSA severity (AHI r\u2009=\u20090.34, p\u2009=\u20090.02; oxygen desaturation index (ODI) r\u2009=\u20090.48, p\u2009<\u20090.001).\"\nQuestion:\n\"Does either obesity or OSA severity influence the response of autotitrating CPAP machines in very obese subjects?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12769830": {
                "source": [
                    "\"Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\"\nQuestion:\n\"Should tumor depth be included in prognostication of soft tissue sarcoma?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24073931": {
                "source": [
                    "\"In recent years, many advances in pancreatic surgery have been achieved. Nevertheless, the rate of pancreatic fistula following pancreatic tail resection does not differ between various techniques, still reaching up to 30% in prospective multicentric studies. Taking into account contradictory results concerning the usefulness of covering resection margins after distal pancreatectomy, we sought to perform a systematic, retrospective analysis of patients that underwent distal pancreatectomy at our center.\nWe retrospectively analysed the data of 74 patients that underwent distal pancreatectomy between 2001 and 2011 at the community hospital in Neuss. Demographic factors, indications, postoperative complications, surgical or interventional revisions, and length of hospital stay were registered to compare the outcome of patients undergoing distal pancreatectomy with coverage of the resection margins vs. patients undergoing distal pancreatectomy without coverage of the resection margins. Differences between groups were calculated using Fisher's exact and Mann-Whitney U test.\nMain indications for pancreatic surgery were insulinoma (n=18, 24%), ductal adenocarcinoma (n=9, 12%), non-single-insulinoma-pancreatogenic-hypoglycemia-syndrome (NSIPHS) (n=8, 11%), and pancreatic cysts with pancreatitis (n=8, 11%). In 39 of 74 (53%) patients no postoperative complications were noted. In detail we found that 23/42 (55%) patients with coverage vs. 16/32 (50%) without coverage of the resection margins had no postoperative complications. The most common complications were pancreatic fistulas in eleven patients (15%), and postoperative bleeding in nine patients (12%). Pancreatic fistulas occurred in patients without coverage of the resection margins in 7/32 (22%) vs. 4/42 (1011%) with coverage are of the resection margins, yet without reaching statistical significance. Postoperative bleeding ensued with equal frequency in both groups (12% with coverage versus 13% without coverage of the resection margins). The reoperation rate was 8%. The hospital stay for patients without coverage was 13 days (5-60) vs. 17 days (8-60) for patients with coverage.\"\nQuestion:\n\"Is the covering of the resection margin after distal pancreatectomy advantageous?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18158048": {
                "source": [
                    "\"There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant.\nAn institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis.\nFifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.\"\nQuestion:\n\"Histologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10798511": {
                "source": [
                    "\"Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations.\nWe reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination.\nOf 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\"\nQuestion:\n\"Blunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9645785": {
                "source": [
                    "\"Changes in the spectrum of general surgery and the delivery of surgical care have placed the requirement for a mandatory general surgery rotation in the surgical clerkship in question.\nWe tested the hypothesis that equal mastery of surgical clerkship objectives can be obtained in a clerkship with and without general surgery. Students chose any two surgical rotations and were assessed by written examination, objective structured clinical examination (OSCE), ward evaluations, self-assessment objectives questionnaire, and satisfaction survey.\nData for 54 students showed no differences in scores between groups on any parameter. No specific concerns related to the absence of general surgery were identified.\"\nQuestion:\n\"Is a mandatory general surgery rotation necessary in the surgical clerkship?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26175531": {
                "source": [
                    "\"It is unclear whether intravenous glycoprotein IIb/IIIa inhibitors or ischemic time might modify any clinical benefits observed with aspiration thrombectomy before primary percutaneous coronary intervention (PCI) in patients with ST-segment-elevation myocardial infarction.\nElectronic databases were searched for trials that randomized ST-segment-elevation myocardial infarction patients to aspiration thrombectomy before PCI versus conventional PCI. Summary estimates were constructed using a DerSimonian-Laird model. Seventeen trials with 20\u2009960 patients were available for analysis. When compared with conventional PCI, aspiration thrombectomy was not associated with a significant reduction in the risk of mortality 2.8% versus 3.2% (risk ratio [RR], 0.89; 95% confidence interval [CI], 0.76-1.04; P=0.13), reinfarction 1.3% versus 1.4% (RR, 0.93; 95% CI, 0.73-1.17; P=0.52), the combined outcome of mortality or reinfarction 4.1% versus 4.6% (RR, 0.90; 95% CI, 0.79-1.02; P=0.11), or stent thrombosis 0.9% versus 1.2% (RR, 0.82; 95% CI, 0.62-1.08; P=0.15). Aspiration thrombectomy was associated with a nonsignificant increase in the risk of stroke 0.6% versus 0.4% (RR, 1.45; 95% CI, 0.96-2.21; P=0.08). Meta-regression analysis did not identify a difference for the log RR of mortality, reinfarction, and the combined outcome of mortality or reinfarction with intravenous glycoprotein IIb/IIIa inhibitors (P=0.17, 0.70, and 0.50, respectively) or with ischemic time (P=0.29, 0.66, and 0.58, respectively).\"\nQuestion:\n\"Is Aspiration Thrombectomy Beneficial in Patients Undergoing Primary Percutaneous Coronary Intervention?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11340218": {
                "source": [
                    "\"In primary and secondary prevention trials, statins have been shown to reduce the risk of stroke. In addition to lipid lowering, statins have a number of antiatherothrombotic and neuroprotective properties. In a preliminary observational study, we explored whether clinical outcome is improved in patients who are on treatment with statins when stroke occurs.\nWe conducted a population-based case-referent study of 25- to 74-year-old stroke patients with, for each case of a patient who was on statin treatment at the onset of stroke (n=125), 2 referent patients who were not treated with statins but were matched for age, gender, year of onset, and stroke subtype (n=250).\nThe unadjusted odds ratio for early discharge to home (versus late discharge or death) was 1.41 (95% CI 0.91 to 2.17) when patients on statin treatment were compared with referent stroke patients not on statins. Prognostic factors were, in general, more unfavorable among patients on statins. When this was adjusted for in a logistic regression model, the use of statins was a moderately strong but statistically nonsignificant predictor of discharge to home (multiple-adjusted odds ratio 1.42, 95% CI 0.90 to 2.22).\"\nQuestion:\n\"Does pretreatment with statins improve clinical outcome after stroke?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24487044": {
                "source": [
                    "\"We sought to determine the target populations and drug efficacy, toxicity, cost, and initiation age thresholds under which a pharmacologic regimen for knee osteoarthritis (OA) prevention could be cost-effective.\nWe used the Osteoarthritis Policy (OAPol) Model, a validated state-transition simulation model of knee OA, to evaluate the cost-effectiveness of using disease-modifying OA drugs (DMOADs) as prophylaxis for the disease. We assessed four cohorts at varying risk for developing OA: (1) no risk factors, (2) obese, (3) history of knee injury, and (4) high-risk (obese with history of knee injury). The base case DMOAD was initiated at age 50 with 40% efficacy in the first year, 5% failure per subsequent year, 0.22% major toxicity, and annual cost of $1,000. Outcomes included costs, quality-adjusted life expectancy (QALE), and incremental cost-effectiveness ratios (ICERs). Key parameters were varied in sensitivity analyses.\nFor the high-risk cohort, base case prophylaxis increased quality-adjusted life-years (QALYs) by 0.04 and lifetime costs by $4,600, and produced an ICER of $118,000 per QALY gained. ICERs>$150,000/QALY were observed when comparing the base case DMOAD to the standard of care in the knee injury only cohort; for the obese only and no risk factors cohorts, the base case DMOAD was less cost-effective than the standard of care. Regimens priced at $3,000 per year and higher demonstrated ICERs above cost-effectiveness thresholds consistent with current US standards.\"\nQuestion:\n\"Pharmacologic regimens for knee osteoarthritis prevention: can they be cost-effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21881325": {
                "source": [
                    "\"Recent studies have demonstrated that statins have pleiotropic effects, including anti-inflammatory effects and atrial fibrillation (AF) preventive effects. The objective of this study was to assess the efficacy of preoperative statin therapy in preventing AF after coronary artery bypass grafting (CABG).\n221 patients underwent CABG in our hospital from 2004 to 2007. 14 patients with preoperative AF and 4 patients with concomitant valve surgery were excluded from this study. Patients were divided into two groups to examine the influence of statins: those with preoperative statin therapy (Statin group, n = 77) and those without it (Non-statin group, n = 126). In addition, patients were divided into two groups to determine the independent predictors for postoperative AF: those with postoperative AF (AF group, n = 54) and those without it (Non-AF group, n = 149). Patient data were collected and analyzed retrospectively.\nThe overall incidence of postoperative AF was 26%. Postoperative AF was significantly lower in the Statin group compared with the Non-statin group (16% versus 33%, p = 0.005). Multivariate analysis demonstrated that independent predictors of AF development after CABG were preoperative statin therapy (odds ratio [OR]0.327, 95% confidence interval [CI] 0.107 to 0.998, p = 0.05) and age (OR 1.058, 95% CI 1.004 to 1.116, p = 0.035).\"\nQuestion:\n\"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26237424": {
                "source": [
                    "\"To evaluate the impact of patient-prosthesis mismatch (PPM) on survival, functional status, and quality of life (QoL) after aortic valve replacement (AVR) with small prosthesis size in elderly patients.\nBetween January 2005 and December 2013, 152 patients with pure aortic stenosis, aged at least 75 years, underwent AVR, with a 19 or 21\u200amm prosthetic heart valve. PPM was defined as an indexed effective orifice area less than 0.85\u200acm/m. Median age was 82 years (range 75-93 years). Mean follow-up was 56 months (range 1-82 months) and was 98% complete. Late survival rate, New York Heart Association functional class, and QoL (RAND SF-36) were assessed.\nOverall, PPM was found in 78 patients (53.8%). Among them, 42 patients (29%) had an indexed effective orifice area less than 0.75\u200acm/m and 17 less than 0.65\u200acm/m (11.7%). Overall survival at 5 years was 78\u200a\u00b1\u200a4.5% and was not influenced by PPM (P\u200a=\u200aNS). The mean New York Heart Association class for long-term survivors with PPM improved from 3.0 to 1.7 (P\u200a<\u200a0.001). QoL (physical functioning 45.18\u200a\u00b1\u200a11.35, energy/fatigue 49.36\u200a\u00b1\u200a8.64, emotional well being 58.84\u200a\u00b1\u200a15.44, social functioning 61.29\u200a\u00b1\u200a6.15) was similar to that of no-PPM patients (P\u200a=\u200aNS).\"\nQuestion:\n\"Does patient-prosthesis mismatch after aortic valve replacement affect survival and quality of life in elderly patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24684514": {
                "source": [
                    "\"Optimization of the preoperative hemoglobin (Hb) level is an effective way to reduce allogeneic transfusion in total knee arthroplasty (TKA) though the procedure is expensive, requires close monitoring and is often inconvenient for patients with reduced mobility. Our aim was to investigate the value of preoperative Hb levels to predict transfusion and thereby tailoring Hb optimization to patient characteristics.\nAll consecutive patients who undergone primary TKA in our center over 2\u00a0years, and received tranexamic acid intraoperatively, were reviewed. The adjusted association between preoperative Hb levels and transfusion was assessed by multivariate logistic regression, and the estimated probability of transfusion for individual patients was derived from the logistic model.\nOut of the 784 patients who meet the inclusion criteria, risk of transfusion was associated with poorer performance status, as measured by the America Association of Anestesiology (ASA) score III/IV (OR: 3\u00b73, P\u00a0<\u00a00\u00b7001) and lower preoperative Hb level (OR 3\u00b78 for each g/dl below 13\u00a0g/dl; P\u00a0<\u00a00\u00b7001). According to the Hb level, the estimated probability of transfusion was 0\u00b703 (range: 0\u00b703-0\u00b764) for ASA I/II patients and 0\u00b710 (range: 0\u00b710-0\u00b784) for ASA III/IV.\"\nQuestion:\n\"Should all patients be optimized to the same preoperative hemoglobin level to avoid transfusion in primary knee arthroplasty?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "7860319": {
                "source": [
                    "\"We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/\nHealth Care Financing Administration (HCFA) hospital mortality data for FY 1989.\nUsing hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality.\nFor the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes.\"\nQuestion:\n\"Measuring hospital mortality rates: are 30-day data enough?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12684740": {
                "source": [
                    "\"Alcohol-associated cues elicit craving in human addicts but little is known about craving mechanisms. Current animal models focus on relapse and this may confound the effect of environmental cues. OBJECTIVES. To develop a model to study the effects of environmental cues on alcohol consumption in animals not experiencing withdrawal or relapse.\nRats were trained to orally self-administer an alcohol (5% w/v)/saccharin (0.2%) solution 30 min a day for 20 days. After stable responding on a free choice between alcohol/saccharin and water, rats were exposed to 5, 10 or 15 min of alcohol-associated cues or 5 min of non-alcohol associated cues. The effect of a 5-min cue was measured after a 10-day break from training or pre-treatment with 0.03, 0.1 or 1 mg/kg naltrexone.\nRats given 5 min of alcohol-associated cues responded significantly more on the active lever (26% increase) and consumed more alcohol as verified by increased blood alcohol levels (8.9 mM versus control 7.5 mM). Ten or 15 min of cues did not change alcohol consumption and 5 min in a novel environment decreased response by 66%. After a 10-day break in training, 5 min of alcohol-associated cues still increased alcohol consumption (29% increase) and the cue effect could be dose-dependently blocked by naltrexone (143% decrease at 0.03 mg/kg).\"\nQuestion:\n\"Cue-induced behavioural activation: a novel model of alcohol craving?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20538207": {
                "source": [
                    "\"It is generally considered that kidney grafts should be preserved at 4 degrees C during cold storage. However, actual temperature conditions are not known. We decided to study the temperature levels during preservation with the Biotainer storage can and Vitalpack transport pack.\nTemperature was monitored using the Thermobouton probe during preservation of pig kidneys, in the same conditions used with human grafts. The probe recorded the temperature level every 10 minutes during four days. We compared the results found with the new storage can with results obtained in the same conditions with the storage can formerly used by our team. We also studied the best position of the probe for temperature monitoring and the influence of the amount of ice within the transport pack on the temperature level. We then monitored the temperature during the conservation of actual human kidney grafts harvested at our institution from August 2007 to May 2008.\nThe temperature levels were the same regardless of the position of the probe within the transport pack. The lowest temperature was maintained during 15 hours, and the temperature level stayed below 5 degrees C for 57 hours with the new storage can. The former storage can maintained the lowest temperature level for 80 minutes, and temperature reached 5 degrees C after 10 hours 40 minutes. Temperature levels were similar when 2 or 4 kg of crushed ice were used. We observed similar results when monitoring the conservation of human grafts.\"\nQuestion:\n\"Should temperature be monitorized during kidney allograft preservation?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18322741": {
                "source": [
                    "\"Atrial fibrillation, which occurs in 12% of all major foregut surgeries, can prolong hospital stay and increase morbidity. Minimally invasive techniques in foregut surgery have been suggested to cause less tissue trauma. We examined the factors associated with new-onset atrial fibrillation after foregut surgery at our institution.\nWe retrospectively examined the records of 154 adult patients who underwent major foregut surgery which included esophagectomy, partial or total gastrectomy, redo Heller myotomy, redo or transthoracic fundoplications. Univariate and multivariate logistic regression analysis with standard modeling techniques were performed to determine risk factors for new-onset atrial fibrillation.\nOf the 154 patients, 14 patients developed new-onset atrial fibrillation with a higher mean age of 67.1 years (+/-8.8 years) versus 56.4 years (+/-14.1 years) (p = 0.006). Laparoscopic (p = 0.004) and nonthoracic surgeries (p = 0.01) were associated with lower risk of atrial fibrillation. Patients with atrial fibrillation had received more fluid (6.5 +/- 2.8 liters versus 5.3 +/- 2.0 liters) and had longer operations (370 +/- 103 min versus 362 +/- 142 min), none of which were statistically significant. The average intensive care length of stay of patients was longer: 7.5 +/- 6.8 days versus 4.0 +/- 7.1 days (p = 0.004). Multivariate analysis revealed an association of atrial fibrillation with age (OR 1.08, 95% CI 1.02-1.14, p = 0.01), and laparoscopic surgery (OR 0.09, 95% CI 0.01-0.95, p = 0.04) after adjusting for surgery type.\"\nQuestion:\n\"Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16418930": {
                "source": [
                    "\"Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\n100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\nDifferences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\"\nQuestion:\n\"Landolt C and snellen e acuity: differences in strabismus amblyopia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24267613": {
                "source": [
                    "\"In this prospective non randomized observational cohort study we have evaluated the influence of age on outcome of laparoscopic total fundoplication for GERD.\nSix hundred and twenty consecutive patients underwent total laparoscopic fundoplication for GERD. Five hundred and twenty-four patients were younger than 65 years (YG), and 96 patients were 65 years or older (EG). The following parameters were considered in the preoperative and postoperative evaluation: presence, duration, and severity of GERD symptoms, presence of a hiatal hernia, manometric and 24 hour pH-monitoring data, duration of operation, incidence of complications and length of hospital stay.\nElderly patients more often had atypical symptoms of GERD and at manometric evaluation had a higher rate of impaired esophageal peristalsis in comparison with younger patients. The duration of the operation was similar between the two groups. The incidence of intraoperative and postoperative complications was low and the difference was not statistically significant between the two groups. An excellent outcome was observed in 93.0% of young patients and in 88.9% of elderly patients (p = NS).\"\nQuestion:\n\"Is the advanced age a contraindication to GERD laparoscopic surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18568239": {
                "source": [
                    "\"To evaluate the influence of the urologist's experience on the surgical results and complications of transurethral resection of the prostate (TURP).\nSixty-seven patients undergoing transurethral resection of the prostate without the use of a video camera were randomly allocated into three groups according to the urologist's experience: a urologist having done 25 transurethral resections of the prostate (Group I - 24 patients); a urologist having done 50 transurethral resections of the prostate (Group II - 24 patients); a senior urologist with vast transurethral resection of the prostate experience (Group III - 19 patients). The following were recorded: the weight of resected tissue, the duration of the resection procedure, the volume of irrigation used, the amount of irrigation absorbed and the hemoglobin and sodium levels in the serum during the procedure.\nThere were no differences between the groups in the amount of irrigation fluid used per operation, the amount of irrigation fluid absorbed or hematocrit and hemoglobin variation during the procedure. The weight of resected tissue per minute was approximately four times higher in group III than in groups I and II. The mean absorbed irrigation fluid was similar between the groups, with no statistical difference between them (p=0.24). Four patients (6%) presented with TUR syndrome, without a significant difference between the groups.\"\nQuestion:\n\"Is the ability to perform transurethral resection of the prostate influenced by the surgeon's previous experience?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18496363": {
                "source": [
                    "\"To characterize the gender dimorphism after injury with specific reference to the reproductive age of the women (young,<48 yrs of age, vs. old,>52 yrs of age) in a cohort of severely injured trauma patients for which significant variation in postinjury care is minimized.\nSecondary data analysis of an ongoing prospective multicenter cohort study.\nAcademic, level I trauma and intensive care unit centers.\nBlunt-injured adults with hemorrhagic shock.\nNone.\nSeparate Cox proportional hazard regression models were formulated based on all patients to evaluate the effects of gender on mortality, multiple organ failure, and nosocomial infection, after controlling for all important confounders. These models were then used to characterize the effect of gender in young and old age groups. Overall mortality, multiple organ failure, and nosocomial infection rates for the entire cohort (n = 1,036) were 20%, 40%, and 45%, respectively. Mean Injury Severity Score was 32 +/- 14 (mean +/- SD). Men (n = 680) and women (n = 356) were clinically similar except that men required higher crystalloid volumes, more often had a history of alcoholism and liver disease, and had greater ventilatory and intensive care unit requirements. Female gender was independently associated with a 43% and 23% lower risk of multiple organ failure and nosocomial infection, respectively. Gender remained an independent risk factor in young and old subgroup analysis, with the protection afforded by female gender remaining unchanged.\"\nQuestion:\n\"Characterization of the gender dimorphism after injury and hemorrhagic shock: are hormonal differences responsible?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26104852": {
                "source": [
                    "\"Hereditary transthyretin (ATTR) amyloidosis with increased left ventricular wall thickness could easily be misdiagnosed by echocardiography as hypertrophic cardiomyopathy (HCM). Our aim was to create a diagnostic tool based on echocardiography and ECG that could optimise identification of ATTR amyloidosis.\nData were analysed from 33 patients with biopsy proven ATTR amyloidosis and 30 patients with diagnosed HCM. Conventional features from ECG were acquired as well as two dimensional and Doppler echocardiography, speckle tracking derived strain and tissue characterisation analysis. Classification trees were used to select the most important variables for differentiation between ATTR amyloidosis and HCM.\nThe best classification was obtained using both ECG and echocardiographic features, where a QRS voltage>30\u2009mm was diagnostic for HCM, whereas in patients with QRS voltage<30\u2009mm, an interventricular septal/posterior wall thickness ratio (IVSt/PWt)>1.6 was consistent with HCM and a ratio<1.6 supported the diagnosis of ATTR amyloidosis. This classification presented both high sensitivity (0.939) and specificity (0.833).\"\nQuestion:\n\"Can echocardiography and ECG discriminate hereditary transthyretin V30M amyloidosis from hypertrophic cardiomyopathy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17565137": {
                "source": [
                    "\"To evaluate the effect of an antismoking advertisement on young people's perceptions of smoking in movies and their intention to smoke.SUBJECTS/\n3091 cinema patrons aged 12-24 years in three Australian states; 18.6% of the sample (n = 575) were current smokers.DESIGN/\nQuasi-experimental study of patrons, surveyed after having viewed a movie. The control group was surveyed in week 1, and the intervention group in weeks 2 and 3. Before seeing the movie in weeks 2 and 3, a 30 s antismoking advertisement was shown, shot in the style of a movie trailer that warned patrons not to be sucked in by the smoking in the movie they were about to see.\nAttitude of current smokers and non-smokers to smoking in the movies; intention of current smokers and non-smokers to smoke in 12 months.\nAmong non-smokers, 47.8% of the intervention subjects thought that the smoking in the viewed movie was not OK compared with 43.8% of the control subjects (p = 0.04). However, there was no significant difference among smokers in the intervention (16.5%) and control (14.5%) groups (p = 0.4). A higher percentage of smokers in the intervention group indicated that they were likely to be smoking in 12 months time (38.6%) than smokers in the control group (25.6%; p<0.001). For non-smokers, there was no significant difference in smoking intentions between groups, with 1.2% of intervention subjects and 1.6% of controls saying that they would probably be smoking in 12 months time (p = 0.54).\"\nQuestion:\n\"Out of the smokescreen II: will an advertisement targeting the tobacco industry affect young people's perception of smoking in movies and their intention to smoke?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26548832": {
                "source": [
                    "\"Longitudinally following patients requires a full-time employee (FTE)-dependent data inflow infrastructure. There are efforts to capture patient-reported outcomes (PROs) by the use of non-FTE-dependent methodologies. In this study, we set out to assess the reliability of PRO data captured via FTE-dependent compared with non-FTE-dependent methodologies.\nA total of 119 adult patients (65 men) who underwent 1-and 2-level lumbar fusions at Duke University Medical Center were enrolled in this prospective study. Enrollment criteria included available demographic, clinical, and PRO data. All patients completed 2 sets of questionnaires--the first a phone interviews and the second a self-survey. There was at least a 2-week period between the phone interviews and self-survey. Questionnaires included the Oswestry Disability Index (ODI), the visual analog scale for back pain (VAS-BP), and the visual analog scale for leg pain (VAS-LP). Repeated-measures analysis of variance was used to compare the reliability of baseline PRO data captured.\nA total of 39.49% of patients were smokers, 21.00% had diabetes, and 11.76% had coronary artery disease; 26.89% reported history of anxiety disorder, and 28.57% reported history of depression. A total of 97.47% of patients had a high-school diploma or General Education Development, and 49.57% attained a 4-year college degree or postgraduate degree. We observed a high correlation between baseline PRO data captured between FTE-dependent versus non-FTE dependent methodologies (ODI: r = -0.89, VAS-BP: r = 0.74, VAS-LP: r = 0.70). There was no difference in PROs of baseline pain and functional disability between FTE-dependent and non-FTE-dependent methodologies: baseline ODI (FTE-dependent: 47.73 \u00b1 16.77 [mean \u00b1 SD] vs. non-FTE-dependent: 45.81 \u00b1 12.11, P = 0.39), VAS-LP (FTE-dependent: 6.13 \u00b1 2.78 vs. non-FTE-dependent: 6.46 \u00b1 2.79, P = 0.36) and VAS-BP (FTE-dependent: 6.33 \u00b1 2.90 vs. non-FTE-dependent: 6.53 \u00b1 2.48, P = 0.57).\"\nQuestion:\n\"Assessing Patient Reported Outcomes Measures via Phone Interviews Versus Patient Self-Survey in the Clinic: Are We Measuring the Same Thing?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19100463": {
                "source": [
                    "\"Tacrolimus is a potent immunosuppressive drug used in organ transplantation. Because of its substantial toxic effects, narrow therapeutic index, and interindividual pharmacokinetic variability, therapeutic drug monitoring of whole-blood tacrolimus concentrations has been recommended. We investigated the comparability of the results of 2 immunoassay systems, affinity column-mediated immunoassay (ACMIA) and microparticle enzyme immunoassay (MEIA), comparing differences in the tacrolimus concentrations measured by the 2 methods in relation to the hematologic and biochemical values of hepatic and renal functions.\nA total of 154 samples from kidney or liver transplant recipients were subjected to Dimension RxL HM with a tacrolimus Flex reagent cartilage for the ACMIA method and IMx tacrolimus II for the MEIA method.\nTacrolimus concentrations measured by the ACMIA method (n = 154) closely correlated with those measured by the MEIA method (r = 0.84). The Bland-Altman plot using concentration differences between the 2 methods and the average of the 2 methods showed no specific trends. The tacrolimus levels determined by both the MEIA method and the ACMIA method were not influenced by hematocrit levels, but the difference between the 2 methods (ACMIA - MEIA) tended to be larger in low hematocrit samples (P<.001).\"\nQuestion:\n\"Is the affinity column-mediated immunoassay method suitable as an alternative to the microparticle enzyme immunoassay method as a blood tacrolimus assay?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24481006": {
                "source": [
                    "\"79 adjacent proximal surfaces without restorations in permanent teeth were examined. Patients suspected to have carious lesions after a visual clinical and a bitewing examination participated in a CBCT examination (Kodak 9000 3D, 5 \u00d7 3.7 cm field of view, voxel size 0.07 mm). Ethical approval and informed consent were obtained according to the Helsinki Declaration. Radiographic assessment recording lesions with or without cavitation was performed by two observers in bitewings and CBCT sections. Orthodontic separators were placed interdentally between two lesion-suspected surfaces. The separator was removed after 3 days and the surfaces recorded as cavitated (yes/no), i.e. validated clinically. Differences between the two radiographic modalities (sensitivity, specificity and overall accuracy) were estimated by analyzing the binary data in a generalized linear model.\nFor both observers, sensitivity was significantly higher for CBCT than for bitewings (average difference 33%, p<0.001) while specificity was not significantly different between the methods (p = 0.19). The overall accuracy was also significantly higher for CBCT (p<0.001).\"\nQuestion:\n\"Should cavitation in proximal surfaces be reported in cone beam computed tomography examination?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17598882": {
                "source": [
                    "\"A genetic component is well established in the etiology of breast cancer. It is not well known, however, whether genetic traits also influence prognostic features of the malignant phenotype.\nWe carried out a population-based cohort study in Sweden based on the nationwide Multi-Generation Register. Among all women with breast cancer diagnosed from 1961 to 2001, 2,787 mother-daughter pairs and 831 sister pairs with breast cancer were identified; we achieved complete follow-up and classified 5-year breast cancer-specific prognosis among proband (mother or oldest sister) into tertiles as poor, intermediary, or good. We used Kaplan-Meier estimates of survival proportions and Cox models to calculate relative risks of dying from breast cancer within 5 years depending on the proband's outcome.\nThe 5-year survival proportion among daughters whose mothers died within 5 years was 87% compared to 91% if the mother was alive (p = 0.03). Among sisters, the corresponding proportions were 70% and 88%, respectively (p = 0.001). After adjustment for potential confounders, daughters and sisters of a proband with poor prognosis had a 60% higher 5-year breast cancer mortality compared to those of a proband with good prognosis (hazard ratio [HR], 1.6; 95% confidence interval [CI], 1.2 to 2.2; p for trend 0.002). This association was slightly stronger among sisters (HR, 1.8; 95% CI, 1.0 to 3.4) than among daughters (HR, 1.6; 95% CI, 1.1 to 2.3).\"\nQuestion:\n\"Is breast cancer prognosis inherited?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23414523": {
                "source": [
                    "\"The potential effects of binge drinking during pregnancy on child motor function have only been assessed in a few, small studies. We aimed to examine the effects of binge alcohol consumption during early pregnancy, including number of binge episodes and timing of binge drinking, on child motor function at age 5.\nWe performed a prospective follow-up study of 678 women and their children sampled from the Danish National Birth Cohort based on maternal alcohol consumption during pregnancy. At 5 years of age, the children were tested with the Movement Assessment Battery for Children. Parental education, maternal IQ, prenatal maternal smoking, the child's age at testing, sex of child, and tester were considered core confounders, while the full model also controlled for prenatal maternal average alcohol intake, maternal age and prepregnancy body mass index, parity, home environment, postnatal parental smoking, health status, participation in organized sport, and indicators for hearing and vision impairment.\nThere were no systematic or significant differences in motor function between children of mothers reporting isolated episodes of binge drinking and children of mothers with no binge episodes. No association was observed with respect to the number of binge episodes (maximum of 12) and timing of binge drinking.\"\nQuestion:\n\"Does binge drinking during early pregnancy increase the risk of psychomotor deficits?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22303473": {
                "source": [
                    "\"Family caregivers of dementia patients are at increased risk of developing depression or anxiety. A multi-component program designed to mobilize support of family networks demonstrated effectiveness in decreasing depressive symptoms in caregivers. However, the impact of an intervention consisting solely of family meetings on depression and anxiety has not yet been evaluated. This study examines the preventive effects of family meetings for primary caregivers of community-dwelling dementia patients.\nA randomized multicenter trial was conducted among 192 primary caregivers of community dwelling dementia patients. Caregivers did not meet the diagnostic criteria for depressive or anxiety disorder at baseline. Participants were randomized to the family meetings intervention (n\u200a=\u200a96) or usual care (n\u200a=\u200a96) condition. The intervention consisted of two individual sessions and four family meetings which occurred once every 2 to 3 months for a year. Outcome measures after 12 months were the incidence of a clinical depressive or anxiety disorder and change in depressive and anxiety symptoms (primary outcomes), caregiver burden and quality of life (secondary outcomes). Intention-to-treat as well as per protocol analyses were performed.\nA substantial number of caregivers (72/192) developed a depressive or anxiety disorder within 12 months. The intervention was not superior to usual care either in reducing the risk of disorder onset (adjusted IRR 0.98; 95% CI 0.69 to 1.38) or in reducing depressive (randomization-by-time interaction coefficient\u200a=\u200a-1.40; 95% CI -3.91 to 1.10) or anxiety symptoms (randomization-by-time interaction coefficient\u200a=\u200a-0.55; 95% CI -1.59 to 0.49). The intervention did not reduce caregiver burden or their health related quality of life.\"\nQuestion:\n\"Does a family meetings intervention prevent depression and anxiety in family caregivers of dementia patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12040336": {
                "source": [
                    "\"The role of early revascularization among patients with acute myocardial infarction complicated by cardiogenic shock remains controversial. Angioplasty registries, while suggesting a benefit, are subject to selection bias, and clinical trials have been underpowered to detect early benefits. If an invasive strategy is beneficial in this population, patients admitted to hospitals with onsite coronary revascularization might be expected to have a better prognosis. We sought to determine whether access to cardiovascular resources at the admitting hospital influenced the prognosis of patients with acute myocardial infarction complicated by cardiogenic shock.\nBy use of the Cooperative Cardiovascular Project database (a retrospective medical record review of Medicare patients discharged with acute myocardial infarction), we identified patients aged>or =65 years whose myocardial infarction was complicated by cardiogenic shock.\nOf the 601 patients with cardiogenic shock, 287 (47.8%) were admitted to hospitals without revascularization services and 314 (52.2%) were admitted to hospitals with coronary angioplasty and coronary artery bypass surgery facilities. Clinical characteristics were similar across the subgroups. Patients admitted to hospitals with revascularization services were more likely to undergo coronary revascularization during the index hospitalization and during the first month after acute myocardial infarction. After adjustment for demographic, clinical, hospital, and treatment strategies, the presence of onsite revascularization services was not associated with a significantly lower 30-day (odds ratio 0.83, 95% CI 0.47, 1.45) or 1-year mortality (odds ratio 0.91, 95% CI 0.49, 1.72).\"\nQuestion:\n\"Cardiogenic shock complicating acute myocardial infarction in elderly patients: does admission to a tertiary center improve survival?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27615402": {
                "source": [
                    "\"Parental drinking has been shown to be associated with offspring drinking. However, the relationship appears to be more complex than often assumed and few studies have tracked it over longer time periods.\nTo explore the long-term (10-year) transmission of familial drinking during adolescence to offspring drinking patterns in young adulthood.\nSwedish longitudinal study, assessing the relationship between familial drinking in 2000 and offspring drinking in 2010 using simultaneous quantile regression analysis (n=744).DATA: Data on familial drinking was gathered from the Swedish level-of-living surveys (LNU) and from partner LNU in 2000 while data on offspring drinking in young adulthood was gathered from LNU 2010. Drinking among offspring, parents and potential stepparents was measured through identical quantity-frequency indices referring to the past 12 months in 2010 and 2000 respectively.\nYoung adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of non-abstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. Supplementary analyses confirmed these patterns.\"\nQuestion:\n\"Does the familial transmission of drinking patterns persist into young adulthood?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21164063": {
                "source": [
                    "\"A possible role for fondaparinux as a bridging agent in the perioperative setting is explored.\nAnticoagulation guidelines provide minimal direction on the perioperative use of fondaparinux. Fondaparinux's extended half-life of 17-21 hours complicates its use as a perioperative bridging therapy. The ideal time for discontinuation before surgery is an issue, particularly in surgeries with a high bleeding risk or in which neuraxial anesthesia is used. Guidance for perioperative bridging with fondaparinux must be derived from pharmacokinetic data, surgical prophylaxis trials, case reports, and anesthesia guidelines. Published trials used fondaparinux sodium 2.5 mg daily for venous thromboembolism prophylaxis in surgical patients, and the majority avoided its use before surgery in patients receiving neuraxial anesthesia. Three case reports cited the use of fondaparinux sodium as perioperative bridge therapy; one used a 2.5-mg dose, and the other two used a full treatment dose of 7.5 mg. Furthermore, professional anesthesia guidelines conflict in their recommendations regarding the timing of drug administration with neuraxial catheter use. For these reasons, it may be optimal to avoid fondaparinux use before surgery. In some instances, the use of low-molecular-weight heparin or inpatient use of i.v. unfractionated heparin is not possible, is contraindicated, or has limited efficacy, such as a patient with history of heparin-induced thrombocytopenia or antithrombin III deficiency. Fondaparinux may have a role in bridge therapy for these patients.\"\nQuestion:\n\"Is there a role for fondaparinux in perioperative bridging?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20577124": {
                "source": [
                    "\"Hyperleptinemia and oxidative stress play a major role in the development of cardiovascular diseases in obesity. This study aimed to investigate whether there is a relationship between plasma levels of leptin and phagocytic nicotinamide adenine dinucleotide phosphate (NADPH) oxidase activity, and its potential relevance in the vascular remodeling in obese patients.\nThe study was performed in 164 obese and 94 normal-weight individuals (controls). NADPH oxidase activity was evaluated by luminescence in phagocytic cells. Levels of leptin were quantified by ELISA in plasma samples. Carotid intima-media thickness (cIMT) was measured by ultrasonography. In addition, we performed in-vitro experiments in human peripheral blood mononuclear cells and murine macrophages.\nPhagocytic NADPH oxidase activity and leptin levels were enhanced (P<0.05) in obese patients compared with controls. NADPH oxidase activity positively correlated with leptin in obese patients. This association remained significant in a multivariate analysis. cIMT was higher (P<0.05) in obese patients compared with controls. In addition, cIMT also correlated positively with leptin and NADPH oxidase activity in obese patients. In-vitro studies showed that leptin induced NADPH oxidase activation. Inhibition of the leptin-induced NADPH oxidase activity by wortmannin and bisindolyl maleimide suggested a direct involvement of the phosphatidylinositol 3-kinase and protein kinase C pathways, respectively. Finally, leptin-induced NADPH oxidase activation promoted macrophage proliferation.\"\nQuestion:\n\"Is leptin involved in phagocytic NADPH oxidase overactivity in obesity?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15539888": {
                "source": [
                    "\"The atopy patch test (APT), namely the patch test with aeroallergens, is regarded as specific for patients with atopic dermatitis (AD), but small numbers of positive APT were reported in the past also in atopic subjects without dermatitis and in healthy persons.\nThe aim of this study was to evaluate the response to the APT with house dust mites (HDM) in subjects nonaffected by AD and to compare the outcomes observed in these cases with those pointed out in AD patients, evaluating also the differences between two allergen extracts manufactured at different purifications and concentrations.\nForty-seven atopic subjects without eczema (AWE), 33 nonatopic (NA) subjects and 77 adult AD patients were patch tested with an extract of purified bodies of HDM at 20% and with another extract of whole bodies of HDM at 30%, the latter corresponding to 300 microg/g of Der p 1. The reproducibility of APT was also tested in 8 AD patients, in 37 AWE subjects and in 19 NA subjects.\nPositive responses with extract at 20% were observed in 29 (37.7%) AD, in 5 (10.6%) AWE and in 4 (12.1%) NA subjects. The APT with HDM at 30% was positive in 32 (41.6%) AD, 9 (19.1%) AWE and 4 (12.1%) NA persons. The rates of positivity and the intensity scores of responses were significantly different between AD and non-AD subjects (p<0.01). The reproducibility of the APT in the three groups was satisfactory.\"\nQuestion:\n\"Is the atopy patch test with house dust mites specific for atopic dermatitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25304452": {
                "source": [
                    "\"The gluten-free diet has traditionally been accepted as a healthy diet, but there are articles advocating that it may have some nutritional deficiencies. The current study assesses whether there was any change in the contributions of calories, essential elements, proportion of fatty acids, vitamins, minerals and fiber in children who were diagnosed with celiac diseases, comparing the diet with gluten prior one year after diagnosis with the diet without gluten to the year of diagnosis. The level of clinical or analytical impact that nutritional deficits could have was also assessed.\nA prospective,descriptive, observational study in which information was collected from a dietary survey, anthropometric and analytical data at pre-diagnosis of celiac disease and following a gluten diet and one year after celiac disease diagnosis, under gluten-free diet.\nA total of 37 patients meet the study criteria. A decrease in the intake of saturated fatty acids was found, with an increase of monounsaturated fatty acids and an increase in the intake of phosphorus in the diet without gluten. A deficient intake of vitamin D was found in both diets. Clinically, at year of gluten-free diet there was an improvement in weight and size. Analytically, there was an improvement in hemoglobin, ferritin, vitamin D, and parathyroid hormone in plasma.\"\nQuestion:\n\"Nutritional assessment of gluten-free diet. Is gluten-free diet deficient in some nutrient?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23806388": {
                "source": [
                    "\"To examine the ability of various postoperative nomograms to predict prostate cancer-specific mortality (PCSM) and to validate that they could predict aggressive biochemical recurrence (BCR). Prostate-specific antigen (PSA), grade, and stage are the classic triad used to predict BCR after radical prostatectomy (RP). Multiple nomograms use these to predict risk of BCR. A previous study showed that several nomograms could predict aggressive BCR (prostate-specific antigen doubling time [PSADT]\u00a0<9 months) more accurately than BCR. However, it remains unknown if they can predict more definitive endpoints, such as PCSM.\nWe performed Cox analyses to examine the ability of 4 postoperative nomograms, the Duke Prostate Center (DPC) nomogram, the Kattan postoperative nomogram, the Johns Hopkins Hospital (JHH) nomogram, and the joint Center for Prostate Disease Research(CPDR)/Cancer of the Prostate Strategic Urologic Research Endeavor (CaPSURE) nomogram to predict BCR and PCSM among 1778 men in the Shared Equal Access Regional Cancer Hospital (SEARCH) database who underwent RP between 1990 and 2009. We also compared their ability to predict BCR and aggressive BCR in a subset of men. We calculated the c-index for each nomogram to determine its predictive accuracy for estimating actual outcomes.\nWe found that each nomogram could predict aggressive BCR and PCSM in a statistically significant manner and that they all predicted PCSM more accurately than they predicted BCR (ie, with higher c-index values).\"\nQuestion:\n\"Do nomograms designed to predict biochemical recurrence (BCR) do a better job of predicting more clinically relevant prostate cancer outcomes than BCR?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18802997": {
                "source": [
                    "\"Assessing the clinical course of inflammatory bowel disease (IBD) patients consists of periodical clinical evaluations and laboratory tests. We aimed to assess the role of calprotectin tests in predicting clinical relapse in IBD patients.\nNinety-seven patients with ulcerative colitis (UC) and 65 with Crohn's disease (CD) in clinical remission were prospectively included in the study. A 10-g stool sample was collected for calprotectin assay. The cutoff level was set at 130 mg/kg of feces. Patients were followed up for 1 yr after the test or until relapse. The cumulative proportion of relapses was estimated by the Kaplan-Meier analysis. Statistics for equality of survival distribution were tested using the log-rank test.\nThe calprotectin test was positive in 44 UC patients and 26 of them relapsed within a year, while 11 of 53 UC patients with a negative calprotectin test relapsed within the same time frame. Thirty CD patients had a positive calprotectin test and 13 of them relapsed within a year, as did 7 of the 35 with a negative test result. A significant correlation emerged between a positive calprotectin test and the probability of relapse in UC patients (P= 0.000). In CD patients, only cases of colonic CD showed a significant correlation between a positive calprotectin test and the probability of relapse, i.e., 6 colonic CD patients were positive for the calprotectin test and 4 relapsed (P= 0.02).\"\nQuestion:\n\"Can calprotectin predict relapse risk in inflammatory bowel disease?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22491528": {
                "source": [
                    "\"To determine if composite measures based on process indicators are consistent with short-term outcome indicators in surgical colorectal cancer care.\nLongitudinal analysis of consistency between composite measures based on process indicators and outcome indicators for 85 Dutch hospitals.\nThe Dutch Surgical Colorectal Audit database, the Netherlands.\n4732 elective patients with colon carcinoma and 2239 with rectum carcinoma treated in 85 hospitals were included in the analyses.\nAll available process indicators were aggregated into five different composite measures. The association of the different composite measures with risk-adjusted postoperative mortality and morbidity was analysed at the patient and hospital level.\nAt the patient level, only one of the composite measures was negatively associated with morbidity for rectum carcinoma. At the hospital level, a strong negative association was found between composite measures and hospital mortality and morbidity rates for rectum carcinoma (p<0.05), and hospital morbidity rates for colon carcinoma.\"\nQuestion:\n\"Combining process indicators to evaluate quality of care for surgical patients with colorectal cancer: are scores consistent with short-term outcome?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17462393": {
                "source": [
                    "\"Beating-heart valve surgery appears to be a promising technique for protection of hypertrophied hearts. Normothermic normokalemic simultaneous antegrade/retrograde perfusion (NNSP) may improve myocardial perfusion. However, its effects on myocardial oxygenation and energy metabolism remain unclear. The present study was to determine whether NNSP improved myocardial oxygenation and energy metabolism of hypertrophied hearts relative to normothermic normokalemic antegrade perfusion (NNAP).\nTwelve hypertrophied pig hearts underwent a protocol consisting of three 20-minute perfusion episodes (10 minutes NNAP and 10 minutes NNSP in a random order) with each conducted at a different blood flow in the left anterior descending coronary artery (LAD [100%, 50%, and 20% of its initial control]). Myocardial oxygenation was assessed using near-infrared spectroscopic imaging. Myocardial energy metabolism was monitored using localized phosphorus-31 magnetic resonance spectroscopy.\nWith 100% LAD flow, both NNAP and NNSP maintained myocardial oxygenation, adenosine triphosphate, phosphocreatine, and inorganic phosphate at normal levels. When LAD flow was reduced to 50% of its control level, NNSP resulted in a small but significant decrease in myocardial oxygenation and phosphocreatine, whereas those measurements did not change significantly during NNAP. With LAD flow further reduced to 20% of its control level, both NNAP and NNSP caused a substantial decrease in myocardial oxygenation, adenosine triphosphate, and phosphocreatine with an increase in inorganic phosphate. However, the changes were significantly greater during NNSP than during NNAP.\"\nQuestion:\n\"Does normothermic normokalemic simultaneous antegrade/retrograde perfusion improve myocardial oxygenation and energy metabolism for hypertrophied hearts?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23149821": {
                "source": [
                    "\"A higher prevalence of cardiovascular risk factors (CRFs) in HIV-infected patients, together with chronic infection and treatments, has resulted in an increased risk of silent myocardial ischaemia (SMI). The objective of this study was to evaluate whether myocardial SPECT should be used for screening HIV-infected patients with no clinical symptoms of coronary artery disease.\nThe prevalence of SMI detected by myocardial SPECT was determined in 94 HIV-infected patients with a normal clinical cardiovascular examination in relation to anthropomorphic parameters, CRFs, inflammatory and HIV infection status, and treatment.\nCoronary artery disease was detected in nine patients (eight with ischaemia, one with myocardial infarction), corresponding to 9.6 % positivity. All but two of the scintigraphic diagnoses of ischaemia were confirmed by coronarography. Univariate analysis revealed that the overall number of CRFs and the combination of gender and age were associated with a diagnosis of SMI (p<0.05). According to multivariate analysis, the only independent parameter significantly associated with the scintigraphic diagnosis of SMI was the combination of gender and age (p = 0.01). All the positive myocardial SPECT scans were in men older than 52 years with at least two other CRFs. In this subpopulation of 47 patients, the prevalence of SMI detected by myocardial SPECT reached 19.2 %.\"\nQuestion:\n\"Should HIV-infected patients be screened for silent myocardial ischaemia using gated myocardial perfusion SPECT?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28056802": {
                "source": [
                    "\"It has recently been shown that non-high density lipoprotein cholesterol (non-HDL-C) may be a better predictor of cardiovascular risk than low density lipoprotein cholesterol (LDL-C). Based on known ethic differences in lipid parameters and cardiovascular risk prediction, we sought to study the predictability of attaining non-HDL-C target and long-term major adverse cardiovascular event (MACE) in Thai patients after acute myocardial infarction (AMI) compared to attaining LDL-C target.\nWe retrospectively obtained the data of all patients who were admitted at Maharaj Nakorn Chiang Mai hospital due to AMI during 2006-2013. The mean non-HDL-C and LDL-C during long-term follow-up were used to predict MACE at each time point. The patients were classified as target attainment if non-HDL-C\u2009<100\u00a0mg/dl and/or LDL-C\u2009<70\u00a0mg/dl. The MACE was defined as combination of all-cause death, nonfatal coronary event and nonfatal stroke.\nDuring mean follow-up of 2.6\u2009\u00b1\u20091.6\u00a0years among 868 patients after AMI, 34.4% achieved non-HDL-C target, 23.7% achieved LDL-C target and 21.2% experienced MACEs. LDL-C and non-HDL-C were directly compared in Cox regression model. Compared with non-HDL-C\u2009<100\u00a0mg/dl, patients with non-HDL-C of>130\u00a0mg/dl had higher incidence of MACEs (HR 3.15, 95% CI 1.46-6.80, P\u2009=\u20090.003). Surprisingly, LDL-C\u2009>100\u00a0mg/dl was associated with reduced risk of MACE as compared to LDL\u2009<70\u00a0mg/dl (HR 0.42, 95% CI 0.18-0.98, p\u2009=\u20090.046) after direct pairwise comparison with non-HDL-C level.\"\nQuestion:\n\"Is non-HDL-cholesterol a better predictor of long-term outcome in patients after acute myocardial infarction compared to LDL-cholesterol?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21658267": {
                "source": [
                    "\"There are three main service delivery channels: clinical services, outreach, and family and community. To determine which delivery channels are associated with the greatest reductions in under-5 mortality rates (U5MR), we used data from sequential population-based surveys to examine the correlation between changes in coverage of clinical, outreach, and family and community services and in U5MR for 27 high-burden countries.\nHousehold survey data were abstracted from serial surveys in 27 countries. Average annual changes (AAC) between the most recent and penultimate survey were calculated for under-five mortality rates and for 22 variables in the domains of clinical, outreach, and family- and community-based services. For all 27 countries and a subset of 19 African countries, we conducted principal component analysis to reduce the variables into a few components in each domain and applied linear regression to assess the correlation between changes in the principal components and changes in under-five mortality rates after controlling for multiple potential confounding factors.\nAAC in under 5-mortality varied from 6.6% in Nepal to -0.9% in Kenya, with six of the 19 African countries all experiencing less than a 1% decline in mortality. The strongest correlation with reductions in U5MR was observed for access to clinical services (all countries: p = 0.02, r\u00b2 = 0.58; 19 African countries p<0.001, r\u00b2 = 0.67). For outreach activities, AAC U5MR was significantly correlated with antenatal care and family planning services, while AAC in immunization services showed no association. In the family- and community services domain, improvements in breastfeeding were associated with significant changes in mortality in the 30 countries but not in the African subset; while in the African countries, nutritional status improvements were associated with a significant decline in mortality.\"\nQuestion:\n\"Do improvements in outreach, clinical, and family and community-based services predict improvements in child survival?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20608141": {
                "source": [
                    "\"Prostate-specific antigen (PSA) levels can show wide fluctuations when repeatedly measured. Here we investigatewd if: (a) biopsy timing influences the prostate cancer (PC) detection rate in patients with fluctuating PSA (flu-PSA) in comparison with patients with steadily increasing PSA (si-PSA); (b) PSA slope estimated in patients with flu-PSA predicts a different risk of cancer detection; (c) flu-PSA and si-PSA patients develop PC in topographically different sites; (d) the behaviour of pre-operative PSA is an expression of a disease with defferent characteristics to the following radical prostatectomy.\nThe study involved 211 patients who underwent at least a second biopsy after a first negative prostate biopsy. PSA Slope, PSA velocity (PSAV) and PSA doubling time (PSADT) were estimated. Flu-PSA level was defined as a PSA series with at least one PSA value lower than the one immediately preceding it.\n82 patients had flu-PSA levels and 129 si-PSA levels. There were no significant differences between the two groups in terms of cancer detection, clinical or pathological stage, but the si-PSA group with cancer had a higher Gleason score. No difference was found for PSA Slope between flu-PSA patients with cancer and those without.\"\nQuestion:\n\"PSA repeatedly fluctuating levels are reassuring enough to avoid biopsy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19351635": {
                "source": [
                    "\"National guidelines and government directives have adopted policies for urgent assessment of patients with a transient ischaemic attack or minor stroke not admitted to hospital. The risk of recurrent stroke increases substantially with age, as does the potential benefit of secondary prevention. In order to develop effective strategies for older patients, it is important to identify how stroke care is currently provided for this patient group.\nBetween 2004 and 2006, older patients (>75 years) referred to a neurovascular clinic were compared with younger patients (<or =75 years). Sociodemographic details, clinical features, resource use and secondary prevention in a neurovascular clinic were collected.\nOf 379 patients referred to the clinic, 129 (34%) were given a non-stroke diagnosis. Of the remaining 250 patients, 149 (60%) were<or =75 years. Median time from symptom onset to clinic appointment was similar for the two groups (24 (IQR 15-42) vs 24 (IQR 14-43) days; p = 0.58). Older patients were more likely to be in atrial fibrillation (10.1% vs 22.8%, p<0.001) and have lacunar stroke (34.7% vs 22.1%; p = 0.04). CT rates were similar in the two groups (27.8% vs 80.0%, p = 0.75). Scans were performed more quickly in younger patients (p<0.01). MRI scan rates were higher in younger patients (26% vs 4%, p<0.01), as was carotid Doppler imaging (92% vs 77%, p<0.01). There were no differences in prescribed secondary preventive treatments. Older patients experienced less delay for carotid endarterectomy (49 vs 90 days, p<0.01). Younger patients were more likely to be given advice on weight reduction (30.2% vs 12.9%, p<0.01) and diet (46.3% vs 31.7%, p = 0.02) than older patients.\"\nQuestion:\n\"Do older patients receive adequate stroke care?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15050326": {
                "source": [
                    "\"To determine whether the risk of secondary breast cancer after radiotherapy (RT) for Hodgkin's disease is greater among women who underwent RT around time of pregnancy.\nThe records of 382 women treated with RT for Hodgkin's disease were reviewed and divided into those who received RT around the time of pregnancy and those who were not pregnant. Comparisons of the overall incidence, actuarial rates, and latency to breast cancer between the two groups were made. Multivariate Cox regression modeling was performed to determine possible contributing factors.\nOf the 382 women, 14 developed breast cancer (3.7%). The increase in the overall incidence (16.0% vs. 2.3%, p = 0.0001) and the actuarial rate of breast cancer among the women in the pregnant group (p = 0.011) was statistically significant. The women treated around the time of pregnancy had a 10- and 15-year actuarial rate of breast cancer of 6.7% and 32.6%, respectively. The 10-year and 15-year actuarial rate for the nonpregnant women was 0.4% and 1.7%, respectively. The median latency from RT to the diagnosis of breast cancer was 13.1 and 18.9 years for women in the pregnant and nonpregnant groups, respectively. In the multivariate analysis, pregnancy around the time of RT was the only variable associated with an increased risk of breast cancer. The risk was dependent on the length of time from pregnancy to RT, with women receiving RT during pregnancy and within 1 month of pregnancy having an increased risk of breast cancer compared with nonpregnant women and women irradiated later than 1 month after pregnancy (hazard ratio, 22.49; 95% confidence interval, 5.56-90.88; p<0.001).\"\nQuestion:\n\"Does radiotherapy around the time of pregnancy for Hodgkin's disease modify the risk of breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17578985": {
                "source": [
                    "\"In this prospective, randomized, double-blind study, we compared the tibial and the peroneal evoked motor response with regard to efficacy of sciatic nerve block using the parasacral approach.\nTwenty-six ASA I-III patients scheduled for elective lower limb surgery were randomized to receive a parasacral sciatic block, using a nerve stimulator technique seeking either a tibial (n = 14) or peroneal (n = 12) motor response. After the evoked motor response was obtained, a solution of 10 mL 2% lidocaine with epinephrine and 10 mL 0.75% ropivacaine (actual final concentration of epinephrine, 1/160,000) was slowly injected through the needle. Sensory and motor blocks were assessed every 5 min for 30 min by an anesthesiologist blinded to the elicited motor response. If the block was not complete 30 min after injection of the local anesthetics, it was considered as failed, and general anesthesia was supplemented.\nTime to perform the block and level of minimal and maximal stimulation were not different between groups. The success rate of complete block was significantly higher in the tibial compared to the peroneal group (11 of 14 vs 2 of 12; P = 0.002).\"\nQuestion:\n\"Parasacral sciatic nerve block: does the elicited motor response predict the success rate?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21276532": {
                "source": [
                    "\"Complications associated with blood transfusions have resulted in widespread acceptance of low hematocrit levels in surgical patients. However, preoperative anemia seems to be a risk factor for adverse postoperative outcomes in certain surgical patients. This study investigated the National Surgical Quality Improvement Program (NSQIP) database to determine if preoperative anemia in patients undergoing open and laparoscopic colectomies is an independent predictor for an adverse composite outcome (CO) consisting of myocardial infarction, stroke, progressive renal insufficiency or death within 30 days of operation, or for an increased hospital length of stay (LOS).\nHematocrit levels were categorized into 4 classes: severe, moderate, mild, and no anemia. From 2005 to 2008, the NSQIP database recorded 23,348 elective open and laparoscopic colectomies that met inclusion criteria. Analyses using multivariable models, controlling for potential confounders and stratifying on propensity score, were performed.\nCompared with nonanemic patients, those with severe, moderate, and mild anemia were more likely to have the adverse CO with odds ratios of 1.83 (95% CI 1.05 to 3.19), 2.19 (95 % CI 1.63 to 2.94), and 1.49 (95% CI 1.20 to 1.86), respectively. Patients with a normal hematocrit had a reduced hospital LOS, compared with those with severe, moderate, and mild anemia (p<0.01). A history of cardiovascular disease did not significantly influence these findings.\"\nQuestion:\n\"Does preoperative anemia adversely affect colon and rectal surgery outcomes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8111516": {
                "source": [
                    "\"To determine whether volunteer family physician reports of the frequency of influenza-like illness (ILI) usefully supplement information from other influenza surveillance systems conducted by the Centers for Disease Control and Prevention.\nEvaluation of physician reports from five influenza surveillance seasons (1987-88 through 1991-92).\nFamily physician office practices in all regions of the United States.\nAn average of 140 physicians during each of five influenza seasons.\nNone.\nAn office visit or hospitalization of a patient for ILI, defined as presence of fever (temperature>or = 37.8 degrees C) and cough, sore throat, or myalgia, along with the physician's clinical judgment of influenza. A subset of physicians collected specimens for confirmation of influenza virus by culture.\nPhysicians attributed 81,408 (5%) of 1,672,542 office visits to ILI; 2754 (3%) patients with ILI were hospitalized. Persons 65 years of age and older accounted for 11% of visits for ILI and 43% of hospitalizations for ILI. In three of five seasons, physicians obtained influenza virus isolates from a greater proportion of specimens compared with those processed by World Health Organization laboratories (36% vs 12%). Influenza virus isolates from sentinel physicians peaked from 1 to 4 weeks earlier than those reported by World Health Organization laboratories. Physicians reported peak morbidity 1 to 4 weeks earlier than state and territorial health departments in four of five seasons and 2 to 5 weeks earlier than peak mortality reported by 121 cities during seasons with excess mortality associated with pneumonia and influenza.\"\nQuestion:\n\"Do family physicians make good sentinels for influenza?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26859535": {
                "source": [
                    "\"Updated guidelines for the screening and management of cervical cancer in the United States recommend starting Papanicolaou (Pap) testing at age 21 and screening less frequently with less aggressive management for abnormalities. We sought to examine updated Pap test screening guidelines and how they may affect the detection of invasive cervical cancer, especially among women<30 years of age.\nPatients diagnosed at Brigham and Women's Hospital with invasive cervical cancer between 2002 and 2012 were retrospectively identified. Prior screening history was obtained and patients were divided into two groups based on age<30 years or age \u226530 years. The two groups were then compared with respect to demographics, pathological findings, and time to diagnosis.\nA total of 288 patients with invasive cervical carcinoma were identified. Among these patients, 109 had adequate information on prior screening history. Invasive adenocarcinoma (IAC) was diagnosed in 37 (33.94%) patients, whereas 64 (58.72%) patients were diagnosed with invasive squamous cell carcinoma (ISCC). The remaining eight patients were diagnosed with other types of cancers of the cervix. A total of 13 patients were younger than 30 while 96 patients were 30 or older. The mean time from normal Pap to diagnosis of IAC was 15 months in patients younger than 30 years of age compared to 56 months in patients aged 30 and older (p\u2009<\u20090.001). The mean time from normal Pap to diagnosis of ISCC was 38 months in patients younger than 30 years of age and 82 months in patients aged 30 and older (p\u2009=\u20090.018).\"\nQuestion:\n\"Screening History Among Women with Invasive Cervical Cancer in an Academic Medical Center: Will We Miss Cancers Following Updated Guidelines?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17279467": {
                "source": [
                    "\"Cyclical pamidronate therapy in a 2-year-old child with skeletal fragility resulted in remodelling of vertebral fractures and improvement in bone mineral density (BMD) at distal radial and spinal sites. The BMD at both sites decreased precipitously within 24 months of stopping treatment, raising the question as to whether bisphosphonates can be stopped in a growing child with skeletal fragility.\nAt age 23 months, a male toddler sustained a low trauma fracture of his right femur. Skeletal radiographs revealed generalised osteopenia with multiple vertebral body fractures. He was diagnosed with type IV osteogenesis imperfecta; however, no mutations were found in COL1A1 or COL1A2 genes.\nThis case report presents bone densitometry data before, during and after bisphosphonate treatment. Axial QCT was main outcome from 2 years of age; DXA and pQCT were taken after age 5.\nQCT confirmed that he had low spinal trabecular volumetric BMD (Z-score -2.4). After 4 years of treatment his vertebral fractures had been remodelled and all bone densitometry values (QCT, DXA and pQCT) were within normal range and therefore treatment was discontinued. Shortly after this he suffered stress fractures of his left mid tibia and at the sclerotic metaphyseal line corresponding to his first APD treatment. He had marked reduction in spinal trabecular and distal radial vBMD; change in BMAD was less marked.\"\nQuestion:\n\"Can bisphosphonate treatment be stopped in a growing child with skeletal fragility?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26113007": {
                "source": [
                    "\"Orthodontic patients show high prevalence of tooth-size discrepancy. This study investigates the possible association between arch form, clinically significant tooth-size discrepancy, and sagittal molar relationship.\nPretreatment orthodontic casts of 230 Saudi patients were classified into one of three arch form types (tapered, ovoid, and square) using digitally scanned images of the mandibular arches. Bolton ratio was calculated, sagittal molar relationship was defined according to Angle classification, and correlations were analyzed using ANOVA, chi-square, and t-tests.\nNo single arch form was significantly more common than the others. Furthermore, no association was observed between the presence of significant Bolton discrepancy and the sagittal molar relationship or arch form. Overall Bolton discrepancy is significantly more prevalent in males.\"\nQuestion:\n\"Is arch form influenced by sagittal molar relationship or Bolton tooth-size discrepancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24507422": {
                "source": [
                    "\"Patients presenting with transient ischemic attack or stroke may have symptom-related lesions on acute computed tomography angiography (CTA) such as free-floating intraluminal thrombus (FFT). It is difficult to distinguish FFT from carotid plaque, but the distinction is critical as management differs. By contouring the shape of these vascular lesions (\"virtual endarterectomy\"), advanced morphometric analysis can be performed. The objective of our study is to determine whether quantitative shape analysis can accurately differentiate FFT from atherosclerotic plaque.\nWe collected 23 consecutive cases of suspected carotid FFT seen on CTA (13 men, 65 \u00b1 10 years; 10 women, 65.5 \u00b1 8.8 years). True-positive FFT cases (FFT+) were defined as filling defects resolving with anticoagulant therapy versus false-positives (FFT-), which remained unchanged. Lesion volumes were extracted from CTA images and quantitative shape descriptors were computed. The five most discriminative features were used to construct receiver operator characteristic (ROC) curves and to generate three machine-learning classifiers. Average classification accuracy was determined by cross-validation.\nFollow-up imaging confirmed sixteen FFT+ and seven FFT- cases. Five shape descriptors delineated FFT+ from FFT- cases. The logistic regression model produced from combining all five shape features demonstrated a sensitivity of 87.5% and a specificity of 71.4% with an area under the ROC curve = 0.85 \u00b1 0.09. Average accuracy for each classifier ranged from 65.2%-76.4%.\"\nQuestion:\n\"Can shape analysis differentiate free-floating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke or transient ischemic attack?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19482903": {
                "source": [
                    "\"Earlier studies have demonstrated low peak oxygen uptake ((.)Vo(2)peak) in children with spina bifida. Low peak heart rate and low peak respiratory exchange ratio in these studies raised questions regarding the true maximal character of (.)Vo(2)peak values obtained with treadmill testing.\nThe aim of this study was to determine whether the Vo(2)peak measured during an incremental treadmill test is a true reflection of the maximum oxygen uptake ((.)Vo(2)max) in children who have spina bifida and are ambulatory.\nA cross-sectional design was used for this study.\nTwenty children who had spina bifida and were ambulatory participated. The (.)Vo(2)peak was measured during a graded treadmill exercise test. The validity of (.)Vo(2)peak measurements was evaluated by use of previously described guidelines for maximum exercise testing in children who are healthy, as well as differences between Vo(2)peak and (.)Vo(2) during a supramaximal protocol ((.)Vo(2)supramaximal).\nThe average values for (.)Vo(2)peak and normalized (.)Vo(2)peak were, respectively, 1.23 L/min (SD=0.6) and 34.1 mL/kg/min (SD=8.3). Fifteen children met at least 2 of the 3 previously described criteria; one child failed to meet any criteria. Although there were no significant differences between (.)Vo(2)peak and Vo(2)supramaximal, 5 children did show improvement during supramaximal testing.\nThese results apply to children who have spina bifida and are at least community ambulatory.\"\nQuestion:\n\"Treadmill testing of children who have spina bifida and are ambulatory: does peak oxygen uptake reflect maximum oxygen uptake?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21904069": {
                "source": [
                    "\"Knowing the collaterals is essential for a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nTo ascertain the sources of the blood supply to the spleen after a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nPerfusion of the cadaveric left gastric and right gastroepiploic arteries with methylene blue after occlusion of all the arteries except the short gastric arteries (n=10). Intraoperative color Doppler ultrasound was used for the evaluation of the hilar arterial blood flow at distal pancreatectomy (n=23) after 1) clamping of the splenic artery alone, 2) clamping of the splenic and left gastroepiploic arteries and 3) clamping of the splenic and short gastric arteries. CT angiography of the gastric and splenic vessels before and after a spleen-preserving distal pancreatectomy (n=10).\nPerfusion of the cadaveric arteries revealed no effective direct or indirect (through the submucous gastric arterial network) communication between the left gastric and the branches of the short gastric arteries. In no case did intraoperative color Doppler ultrasound detect any hilar arterial blood flow after the clamping of the splenic and left gastroepiploic arteries. The clamping of the short gastric arteries did not change the flow parameters. In none of the cases did a post-spleen-preserving distal pancreatectomy with resection of the splenic vessels CT angiography delineate the short gastric vessels supplying the spleen. In all cases, the gastroepiploic arcade was the main arterial pathway feeding the spleen.\"\nQuestion:\n\"Spleen-preserving distal pancreatectomy with resection of the splenic vessels. Should one rely on the short gastric arteries?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11334578": {
                "source": [
                    "\"To evaluate the effectiveness of feeding information on pharmacy back to primary care doctors in order to create awareness (knowledge) of pharmaceutical expenditure (PE).\nRetrospective cross-sectional study, through personal interview.\nReformed PC, Sabadell, Barcelona.\nThe 80 PC doctors working with primary care teams.\nAs the personal feed-back on PE, each doctor was asked for the PE generated during 1997 and the mean cost of prescriptions to active and pensioner patients. The statistical test used was the t test to compare means for paired data, with p<0.05 the required level of significance.\nOut of the total doctors interviewed (80), 71 replies were obtained for the annual PE and 76 for the mean cost of prescriptions, for both active and pensioner patients. Significant differences were found between the annual PE in reality and doctors' estimates: around twelve million pesetas. The differences between the real mean costs of prescription and the estimates were also significant.\"\nQuestion:\n\"Is there awareness of pharmaceutical expenditure in the reformed primary care system?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10593212": {
                "source": [
                    "\"To investigate the importance of loss of consciousness (LOC) in predicting neuropsychological test performance in a large sample of patients with head injury.\nRetrospective comparison of neuropsychological test results for patients who suffered traumatic LOC, no LOC, or uncertain LOC.\nAllegheny General Hospital, Pittsburgh, Pennsylvania.\nThe total number of patients included in this study was 383.\nNeuropsychological test measures, including the visual reproduction, digit span, and logical memory subtests of the Wechsler memory scale (revised), the Trail Making test, Wisconsin Card Sorting test, Hopkins Verbal Learning test, Controlled Oral Word Association, and the Galveston Orientation and Amnesia test (GOAT).\nNo significant differences were found between the LOC, no LOC, or uncertain LOC groups for any of the neuropsychological measures used. Patients who had experienced traumatic LOC did not perform more poorly on neuropsychological testing than those with no LOC or uncertain LOC. All three groups demonstrated mildly decreased performance on formal tests of speed of information processing, attentional process, and memory.\"\nQuestion:\n\"Does loss of consciousness predict neuropsychological decrements after concussion?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27338535": {
                "source": [
                    "\"Current risk assessment models for surgical site occurrence (SSO) and surgical site infection (SSI) after open ventral hernia repair (VHR) have limited external validation. Our aim was to determine (1) whether existing models stratify patients into groups by risk and (2) which model best predicts the rate of SSO and SSI.\nPatients who underwent open VHR and were followed for at least 1\u00a0mo were included. Using two data sets-a retrospective multicenter database (Ventral Hernia Outcomes Collaborative) and a single-center prospective database (Prospective)-each patient was assigned a predicted risk with each of the following models: Ventral Hernia Risk Score (VHRS), Ventral Hernia Working Group (VHWG), Centers for Disease Control and Prevention Wound Class, and Hernia Wound Risk Assessment Tool (HW-RAT). Patients in the Prospective database were also assigned a predicted risk from the American College of Surgeons National Surgical Quality Improvement Program (ACS-NSQIP). Areas under the receiver operating characteristic curve (area under the curve [AUC]) were compared to assess the predictive accuracy of the models for SSO and SSI. Pearson's chi-square was used to determine which models were able to risk-stratify patients into groups with significantly differing rates of actual SSO and SSI.\nThe Ventral Hernia Outcomes Collaborative database (n\u00a0=\u00a0795) had an overall SSO and SSI rate of 23% and 17%, respectively. The AUCs were low for SSO (0.56, 0.54, 0.52, and 0.60) and SSI (0.55, 0.53, 0.50, and 0.58). The VHRS (P\u00a0=\u00a00.01) and HW-RAT (P\u00a0<\u00a00.01) significantly stratified patients into tiers for SSO, whereas the VHWG (P\u00a0<\u00a00.05) and HW-RAT (P\u00a0<\u00a00.05) stratified for SSI. In the Prospective database (n\u00a0=\u00a088), 14% and 8% developed an SSO and SSI, respectively. The AUCs were low for SSO (0.63, 0.54, 0.50, 0.57, and 0.69) and modest for SSI (0.81, 0.64, 0.55, 0.62, and 0.73). The ACS-NSQIP (P\u00a0<\u00a00.01) stratified for SSO, whereas the VHRS (P\u00a0<\u00a00.01) and ACS-NSQIP (P\u00a0<\u00a00.05) stratified for SSI. In both databases, VHRS, VHWG, and Centers for Disease Control and Prevention overestimated risk of SSO and SSI, whereas HW-RAT and ACS-NSQIP underestimated risk for all groups.\"\nQuestion:\n\"Do risk calculators accurately predict surgical site\u00a0occurrences?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17691856": {
                "source": [
                    "\"Rising health care costs and the need to consolidate expertise in tertiary services have led to the centralisation of services. In the UK, the result has been that many rural maternity units have become midwife-led. A key consideration is that midwives have the skills to competently and confidently provide maternity services in rural areas, which may be geographically isolated and where the midwife may only see a small number of pregnant women each year. Our objective was to compare the views of midwives in rural and urban settings, regarding their competence and confidence with respect to 'competencies' identified as being those which all professionals should have in order to provide effective and safe care for low-risk women.\nThis was a comparative questionnaire survey involving a stratified sample of remote and rural maternity units and an ad hoc comparison group of three urban maternity units in Scotland. Questionnaires were sent to 82 midwives working in remote and rural areas and 107 midwives working in urban hospitals with midwife-led units.\nThe response rate from midwives in rural settings was considerably higher (85%) than from midwives in the urban areas (60%). Although the proportion of midwives who reported that they were competent was broadly similar in the two groups, there were some significant differences regarding specific competencies. Midwives in the rural group were more likely to report competence for breech delivery (p = 0.001), while more urban midwives reported competence in skills such as intravenous fluid replacement (p<0.001) and initial and discharge examination of the newborn (p<0.001). Both groups reported facing barriers to continuing professional development; however, more of the rural group had attended an educational event within the last month (p<0.001). Lack of time was a greater barrier for urban midwives (p = 0.02), whereas distance to training was greater for rural midwives (p = 0.009). Lack of motivation or interest was significantly higher in urban units (p = 0.006).\"\nQuestion:\n\"Midwives' competence: is it affected by working in a rural location?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10401824": {
                "source": [
                    "\"Laparoscopic techniques can be used to treat patients whose antireflux surgery has failed.\nCase series.\nTwo academic medical centers.\nForty-six consecutive patients, of whom 21 were male and 25 were female (mean age, 55.6 years; range, 15-80 years). Previous antireflux procedures were laparoscopic (21 patients), laparotomy (21 patients), thoracotomy (3 patients), and thoracoscopy (1 patient).\nThe cause of failure, operative and postoperative morbidity, and the level of follow-up satisfaction were determined for all patients.\nThe causes of failure were hiatal herniation (31 patients [67%]), fundoplication breakdown (20 patients [43%]), fundoplication slippage (9 patients [20%]), tight fundoplication (5 patients [11%]), misdiagnosed achalasia (2 patients [4%]), and displaced Angelchik prosthesis (2 patients [4%]). Twenty-two patients (48%) had more than 1 cause. Laparoscopic reoperative procedures were Nissen fundoplication (n = 22), Toupet fundoplication (n = 13), paraesophageal hernia repair (n = 4), Dor procedure (n = 2), Angelchik prosthesis removal (n = 2), Heller myotomy (n = 2), and the takedown of a wrap (n = 1). In addition, 18 patients required crural repair and 13 required paraesophageal hernia repair. The mean +/- SEM duration of surgery was 3.5+/-1.1 hours. Operative complications were fundus tear (n = 8), significant bleeding (n = 4), bougie perforation (n = 1), small bowel enterotomy (n = 1), and tension pneumothorax (n = 1). The conversion rate (from laparoscopic to an open procedure) was 20% overall (9 patients) but 0% in the last 10 patients. Mortality was 0%. The mean +/- SEM hospital stay was 2.3+/-0.9 days for operations completed laparoscopically. Follow-up was possible in 35 patients (76%) at 17.2+/-11.8 months. The well-being score (1 best; 10, worst) was 8.6+/-2.1 before and 2.9+/-2.4 after surgery (P<.001). Thirty-one (89%) of 35 patients were satisfied with their decision to have reoperation.\"\nQuestion:\n\"Is laparoscopic reoperation for failed antireflux surgery feasible?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22233470": {
                "source": [
                    "\"The 58th World Health Assembly called for all health systems to move towards universal coverage where everyone has access to key promotive, preventive, curative and rehabilitative health interventions at an affordable cost. Universal coverage involves ensuring that health care benefits are distributed on the basis of need for care and not on ability to pay. The distribution of health care benefits is therefore an important policy question, which health systems should address. The aim of this study is to assess the distribution of health care benefits in the Kenyan health system, compare changes over two time periods and demonstrate the extent to which the distribution meets the principles of universal coverage.\nTwo nationally representative cross-sectional households surveys conducted in 2003 and 2007 were the main sources of data. A comprehensive analysis of the entire health system is conducted including the public sector, private-not-for-profit and private-for-profit sectors. Standard benefit incidence analysis techniques were applied and adopted to allow application to private sector services.\nThe three sectors recorded similar levels of pro-rich distribution in 2003, but in 2007, the private-not-for-profit sector was pro-poor, public sector benefits showed an equal distribution, while the private-for-profit sector remained pro-rich. Larger pro-rich disparities were recorded for inpatient compared to outpatient benefits at the hospital level, but primary health care services were pro-poor. Benefits were distributed on the basis of ability to pay and not on need for care.\"\nQuestion:\n\"Does the distribution of health care benefits in Kenya meet the principles of universal coverage?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9542484": {
                "source": [
                    "\"To determine whether successful completion of the Perinatal Education Programme (PEP) improves obstetric practice.\nThe three midwife obstetric units (MOUs) in a health district of Mpumalanga were included in the study. Two MOUs enrolled in the PEP and the third did not. A 'before-and-after' study design was used to assess any changes in practice, and to monitor whether any changes occurred in the district during the time of the study; data were also collected at the third MOU. Data were collected by scoring of the obstetric files after the patient had delivered.\nWe ascertained whether the obstetric history, syphilis testing, blood group testing, haemoglobin measurement and uterine growth assessment were performed during antenatal care along with whether appropriate action was taken. For intrapartum care, estimation of fetal weight, the performance of pelvimetry, blood pressure monitoring, urine testing, evaluation of head above pelvis, fetal heart rate monitoring, monitoring of contractions and plotting of cervical dilatation, and whether the appropriate actions were taken, were assessed.\nEight of the 13 midwives at the two MOUs completed the PEP and all demonstrated an improvement in knowledge. Case notes of 303 patients from the various clinics were studied. There was no change in the referral patterns of any of the clinics during the study period. The obstetric history was well documented, but in no group was there a satisfactory response to a detected problem; appropriate action was taken in between 0% and 12% of cases. Syphilis testing was performed in 56-82% of cases, with no difference between the groups. The haemoglobin level was measured in only 4-15% of patients, with no difference before or after completion of the PEP. Where a problem in uterine growth was detected, an appropriate response occurred in 0-8% of patients and no difference before or after completion of the PEP was ascertained. In all groups, estimation of fetal weight and pelvimetry were seldom performed, the urine and fetal heart rate documentation were moderately well done and the blood pressure monitoring, assessment of head above pelvis, monitoring of contractions and plotting of cervical dilatation were usually performed. No differences before or after the PEP were detected. Where problems were detected, appropriate actions taken during labour improved, but not significantly.\"\nQuestion:\n\"Does successful completion of the Perinatal Education Programme result in improved obstetric practice?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20971618": {
                "source": [
                    "\"Cutaneous infections such as impetigo contagiosum (IC), molluscum contagiosum (MC) and herpes virus infection (HI) appear to be associated with atopic dermatitis (AD), but there are no reports of concrete epidemiological evidence.\nWe evaluated the association of childhood AD with these infections by conducting a population-based cross-sectional study.\nEnrolled in this study were 1117 children aged 0-6 years old attending nursery schools in Ishigaki City, Okinawa Prefecture, Japan. Physical examination was performed by dermatologists, and a questionnaire was completed on each child's history of allergic diseases including AD, asthma, allergic rhinitis and egg allergy, and that of skin infections including IC, MC and HI, as well as familial history of AD.\nIn 913 children (AD; 132), a history of IC, MC or HI was observed in 45.1%, 19.7%, and 2.5%, respectively. Multiple logistic regression analysis revealed that the odds of having a history of IC were 1.8 times higher in AD children than in non-AD children. Meanwhile, a history of MC was significantly correlated to the male gender, but not to a personal history of AD. As for HI, we found no correlated factors in this study.\"\nQuestion:\n\"Are lifetime prevalence of impetigo, molluscum and herpes infection really increased in children having atopic dermatitis?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19712912": {
                "source": [
                    "\"To evaluate the effect of the 80-hour workweek restrictions on resident education within surgical programs in the New England area.\nWeb-based survey.\nAll Accreditation Council for Graduate Medical Education (ACGME) accredited surgical residency programs in New England (n = 20).\nProgram directors/coordinators in each surgical residency program in New England.\nFirst, American Board of Surgery In-Training Examination (ABSITE) scores and the passing rate of the ABS certifying examination were recorded for the years 2001, 2002, 2005, and 2006. Second, the changes in the curriculum of surgical education were documented as perceived by program coordinators and directors.\nIn all, 85% (17/20) of surgical programs in New England responded to the survey. The programs began to implement the 80-hour workweek from 2002 to 2004. An equal distribution of community (n = 8) and university programs (n = 9) was sampled. Prior to the initiation of the 80-hour workweek, residency programs emphasized weekly didactic sessions given by attending physicians (88%), mock orals (88%), and conventional journal club (76%). After the 80-hour workweek was implemented, the education curriculum most often consisted of didactic sessions by attending (100%), mock orals (88%), and simulation laboratories (75%). No difference was observed in ABSITE scores and first-time pass rates of the ABS examination before or after the introduction of the 80-hour workweek (20% response). Only 25% of programs felt that surgical education was improved after the implementation of the 80-hour workweek, whereas 31% felt education was worse. Overall, 44% of respondents believed that there was no difference in surgical education.\"\nQuestion:\n\"Has the 80-hour workweek improved surgical resident education in New England?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19156007": {
                "source": [
                    "\"To investigate whether the Patient Health Questionnaire-9 (PHQ-9) possesses the essential psychometric characteristics to measure depressive symptoms in people with visual impairment.\nThe PHQ-9 scale was completed by 103 participants with low vision. These data were then assessed for fit to the Rasch model.\nThe participants' mean +/- standard deviation (SD) age was 74.7 +/- 12.2 years. Almost one half of them (n = 46; 44.7%) were considered to have severe vision impairment (presenting visual acuity<6/60 in the better eye). Disordered thresholds were evident initially. Collapsing the two middle categories produced ordered thresholds and fit to the Rasch model (chi = 10.1; degrees of freedom = 9; p = 0.34). The mean (SD) items and persons Fit Residual values were -0.31 (1.12) and -0.25 (0.78), respectively, where optimal fit of data to the Rasch model would have a mean = 0 and SD = 1. Unidimensionality was demonstrated confirming the construct validity of the PHQ-9 and there was no evidence of differential item functioning on a number of factors including visual disability. The person separation reliability value was 0.80 indicating that the PHQ-9 has satisfactory precision. There was a degree of mistargeting as expected in this largely non-clinically depressed sample.\"\nQuestion:\n\"Can clinicians use the PHQ-9 to assess depression in people with vision loss?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24614789": {
                "source": [
                    "\"Postoperative CSF fistulas were described in 16 of 198 patients (8%) who underwent spine surgery between 2009 and 2010. The choice of the therapeutic strategy was based on the clinical condition of the patients, taking into account the possibility to maintain the prone position continuously and the risk of morbidity due to prolonged bed rest. Six patients were treated conservatively (position prone for three weeks), ten patients were treated by positioning an external CSF lumbar drainage for ten days. The mean follow-up period was ten months.\nAll patients healed their wound properly and no adverse events were recorded. Patients treated conservatively were cured in a mean period of 30 days, while patients treated with CSF drainage were cured in a mean period of 10 days.\"\nQuestion:\n\"Is lumbar drainage of postoperative cerebrospinal fluid fistula after spine surgery effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22266735": {
                "source": [
                    "\"The International Association of the Diabetes and Pregnancy Study Groups (IADPSG) recently recommended new criteria for diagnosing gestational diabetes mellitus (GDM). This study was undertaken to determine whether adopting the IADPSG criteria would be cost-effective, compared with the current standard of care.\nWe developed a decision analysis model comparing the cost-utility of three strategies to identify GDM: 1) no screening, 2) current screening practice (1-h 50-g glucose challenge test between 24 and 28 weeks followed by 3-h 100-g glucose tolerance test when indicated), or 3) screening practice proposed by the IADPSG. Assumptions included that 1) women diagnosed with GDM received additional prenatal monitoring, mitigating the risks of preeclampsia, shoulder dystocia, and birth injury; and 2) GDM women had opportunity for intensive postdelivery counseling and behavior modification to reduce future diabetes risks. The primary outcome measure was the incremental cost-effectiveness ratio (ICER).\nOur model demonstrates that the IADPSG recommendations are cost-effective only when postdelivery care reduces diabetes incidence. For every 100,000 women screened, 6,178 quality-adjusted life-years (QALYs) are gained, at a cost of $125,633,826. The ICER for the IADPSG strategy compared with the current standard was $20,336 per QALY gained. When postdelivery care was not accomplished, the IADPSG strategy was no longer cost-effective. These results were robust in sensitivity analyses.\"\nQuestion:\n\"Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22302658": {
                "source": [
                    "\"Patients with aggressive lower extremity musculoskeletal tumors may be candidates for either above-knee amputation or limb-salvage surgery. However, the subjective and objective benefits of limb-salvage surgery compared with amputation are not fully clear.QUESTIONS/\nWe therefore compared functional status and quality of life for patients treated with above-knee amputation versus limb-salvage surgery.\nWe reviewed 20 of 51 patients aged 15 years and older treated with above-knee amputation or limb-salvage surgery for aggressive musculoskeletal tumors around the knee between 1994 and 2004 as a retrospective cohort study. At last followup we obtained the Physiological Cost Index, the Reintegration to Normal Living Index, SF-36, and the Toronto Extremity Salvage Score questionnaires. The minimum followup was 12 months (median, 56 months; range, 12-108 months).\nCompared with patients having above-knee amputation, patients undergoing limb-salvage surgery had superior Physiological Cost Index scores and Reintegration to Normal Living Index. The Toronto Extremity Salvage scores and SF-36 scores were similar in the two groups.\"\nQuestion:\n\"Does limb-salvage surgery offer patients better quality of life and functional capacity than amputation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14599616": {
                "source": [
                    "\"Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume.\nNinety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard.\nThirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.\"\nQuestion:\n\"Can a practicing surgeon detect early lymphedema reliably?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11567820": {
                "source": [
                    "\"To test the hypothesis that increasing the nerve length within the treatment volume for trigeminal neuralgia radiosurgery would improve pain relief.\nEighty-seven patients with typical trigeminal neuralgia were randomized to undergo retrogasserian gamma knife radiosurgery (75 Gy maximal dose with 4-mm diameter collimators) using either one (n = 44) or two (n = 43) isocenters. The median follow-up was 26 months (range 1-36).\nPain relief was complete in 57 patients (45 without medication and 12 with low-dose medication), partial in 15, and minimal in another 15 patients. The actuarial rate of obtaining complete pain relief (with or without medication) was 67.7% +/- 5.1%. The pain relief was identical for one- and two-isocenter radiosurgery. Pain relapsed in 30 of 72 responding patients. Facial numbness and mild and severe paresthesias developed in 8, 5, and 1 two-isocenter patients vs. 3, 4, and 0 one-isocenter patients, respectively (p = 0.23). Improved pain relief correlated with younger age (p = 0.025) and fewer prior procedures (p = 0.039) and complications (numbness or paresthesias) correlated with the nerve length irradiated (p = 0.018).\"\nQuestion:\n\"Does increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26222664": {
                "source": [
                    "\"A retrospective analysis.\nThe purpose of this study was to determine whether the deformity angular ratio (DAR) can reliably assess the neurological risks of patients undergoing deformity correction.\nIdentifying high-risk patients and procedures can help ensure that appropriate measures are taken to minimize neurological complications during spinal deformity corrections. Subjectively, surgeons look at radiographs and evaluate the riskiness of the procedure. However, 2 curves of similar magnitude and location can have significantly different risks of neurological deficit during surgery. Whether the curve spans many levels or just a few can significantly influence surgical strategies. Lenke et al have proposed the DAR, which is a measure of curve magnitude per level of deformity.\nThe data from 35 pediatric spinal deformity correction procedures with thoracic 3-column osteotomies were reviewed. Measurements from preoperative radiographs were used to calculate the DAR. Binary logistic regression was used to model the relationship between DARs (independent variables) and presence or absence of an intraoperative alert (dependent variable).\nIn patients undergoing 3-column osteotomies, sagittal curve magnitude and total curve magnitude were associated with increased incidence of transcranial motor evoked potential changes. Total DAR greater than 45\u00b0 per level and sagittal DAR greater than 22\u00b0 per level were associated with a 75% incidence of a motor evoked potential alert, with the incidence increasing to 90% with sagittal DAR of 28\u00b0 per level.\"\nQuestion:\n\"The Deformity Angular Ratio: Does It Correlate With High-Risk Cases for Potential Spinal Cord Monitoring Alerts in Pediatric 3-Column Thoracic Spinal Deformity Corrective Surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26578404": {
                "source": [
                    "\"Breathlessness is one of the most distressing symptoms experienced by patients with advanced cancer and noncancer diagnoses alike. Often, severity of breathlessness increases quickly, calling for rapid symptom control. Oral, buccal, and parenteral routes of provider-controlled drug administration have been described. It is unclear whether patient-controlled therapy (PCT) systems would be an additional treatment option.\nTo investigate whether intravenous opioid PCT can be an effective therapeutic method to reduce breathlessness in patients with advanced disease. Secondary aims were to study the feasibility and acceptance of opioid PCT in patients with refractory breathlessness.\nThis was a pilot observational study with 18 inpatients with advanced disease and refractory breathlessness receiving opioid PCT. Breathlessness was measured on a self-reported numeric rating scale. Richmond Agitation Sedation Scale scores, Palliative Performance Scale scores, vital signs, and a self-developed patient satisfaction questionnaire were used for measuring secondary outcomes. Descriptive and interference analyses (Friedman test) and post hoc analyses (Wilcoxon tests and Bonferroni corrections) were performed.\nEighteen of 815 patients (advanced cancer; median age\u00a0=\u00a057.5\u00a0years [range 36-81]; 77.8% female) received breathlessness symptom control with opioid PCT; daily morphine equivalent dose at Day 1 was median\u00a0=\u00a020.3\u00a0mg (5.0-49.6\u00a0mg); Day 2: 13.0\u00a0mg (1.0-78.5\u00a0mg); Day 3: 16.0\u00a0mg (8.3-47.0\u00a0mg). Numeric rating scale of current breathlessness decreased (baseline: median\u00a0=\u00a05 [range 1-10]; Day 1: median\u00a0=\u00a04 [range 0-8], P\u00a0<\u00a00.01; Day 2: median\u00a0=\u00a04 [range 0-5], P\u00a0<\u00a00.01). Physiological parameters were stable over time. On Day 3, 12/12 patients confirmed that this mode of application provided relief of breathlessness.\"\nQuestion:\n\"Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23588461": {
                "source": [
                    "\"Ascitis and undernutrition are frequent complications of cirrhosis, however ascitis volume and anthropometric assessment are not routinely documented or considered in prognostic evaluation. In a homogeneous cohort followed during two years these variables were scrutinized, aiming to ascertain relevance for longterm outcome.\nPopulation (N = 25, all males with alcoholic cirrhosis) was recruited among patients hospitalized for uncomplicated ascitis. Exclusion criteria were refractory or tense ascitis, cancer, spontaneous bacterial peritonitis, bleeding varices and critical illness. Measurements included ultrasonographically estimated ascitis volume, dry body mass index/BMI , upper arm anthropometrics, hematologic counts and liver function tests.\nPopulation (age 48.3 \u00b1 11.3 years, BMI 21.1 \u00b1 3.5 kg/m\u00b2, serum albumin 2.5 \u00b1 0.8 g/dL) was mostly in the Child-Pugh C category (77.8%) but clinically stable. During the follow-up period of 22.6 \u00b1 3.8 months, additional hospitalizations numbered 1.7 \u00b1 1.0 and more than one quarter succumbed. Admission ascitis volume corresponded to 7.1 \u00b1 3.6 L and dry BMI to 18.3 \u00b1 3.5 kg/m\u00b2. Child Pugh index was relevant for both mortality and rehospitalization. Nevertheless, similar matches for mortality were documented with ascitis volume and dry BMI, and arm circumference below the 5th percentile was highly significantly associated with rehospitalization.\"\nQuestion:\n\"Should ascitis volume and anthropometric measurements be estimated in hospitalized alcoholic cirrotics?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21889895": {
                "source": [
                    "\"The aim of this study was to determine if educating residents about the potential effects of radiation exposure from computed tomographic (CT) imaging alters ordering patterns. This study also explored whether referring physicians are interested in radiation education and was an initial effort to address their CT ordering behavior.\nTwo to four months after a radiologist's lecture on the potential effects of radiation exposure related to CT scans, urology and orthopedic residents were surveyed regarding the number and types of CT scans they ordered, the use of alternative imaging modalities, and whether they used the lecture information to educate patients.\nTwenty-one resident lecture attendants completed the survey. The number of CT scans ordered after the lecture stayed constant for 90% (19 of 21) and decreased for 10% (two of 21). The types of CT scans ordered changed after the lecture for 14% (three of 21). Thirty-three percent (seven of 21) reported increases in alternative imaging after the lecture, including 24% (five of 21) reporting increases in magnetic resonance imaging and 19% (four of 21) reporting increases in ultrasound. Patients directed questions about radiation exposure to 57% (12 of 21); 38% (eight of 21) used the lecture information to educate patients. Referring physicians were interested in the topic, and afterward, other physician groups requested radiation education lectures.\"\nQuestion:\n\"Will CT ordering practices change if we educate residents about the potential effects of radiation exposure?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12377809": {
                "source": [
                    "\"Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter and/or the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography.\nTwenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborectalis were measured at rest, and during voluntary squeezing and straining. Statistical analysis was performed within and between the two groups.\nThe anal sphincter became paradoxically shorter and/or thicker during straining (versus the resting state) in 85% of patients but in only 35% of control subjects. Changes in sphincter length were statistically significantly different (p<0.01, chi(2) test) in patients compared with control subjects. The m. puborectalis became paradoxically shorter and/or thicker during straining in 80% of patients but in only 30% of controls. Both the changes in length and thickness of the m. puborectalis were significantly different (p<0.01, chi(2) test) in patients versus control subjects.\"\nQuestion:\n\"Is anorectal endosonography valuable in dyschesia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26419377": {
                "source": [
                    "\"The purpose of this study was to evaluate safe depth for suture anchor insertion during acetabular labral repair and to determine the neighbouring structures at risk during drilling and anchor insertion.\nTen human cadaveric hips (six males and four females) were obtained. Acetabular labral surface was prepared and marked for right hips as 12, 1 and 3 o'clock positions, for left hips 12, 11 and 9 o'clock positions. Those were defined as anterior, anterior-superior and superior zones, respectively. These labral positions were drilled at defined zones. After measurements, depth of the bone at 10\u00b0 and 20\u00b0 drill angles on zones was compared statistically.\nAcetabular bone widths at investigated labral insertion points did not statistically differ. A total of 14 injuries in 60 penetrations occurred (23.3\u00a0%) with free drill penetrations, and no injuries occurred with stopped drill penetrations. The bone depth was gradually decreasing from 10\u00b0 to 20\u00b0 drill angles and from anterior to superior inserting zones without significant importance. The risk of perforation to the pelvic cavity started with 20\u00a0mm drill depth, and the mean depth for all insertions was calculated as 31.7\u00a0mm (SD 2.6).\"\nQuestion:\n\"Are pelvic anatomical structures in danger during arthroscopic acetabular labral repair?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16414216": {
                "source": [
                    "\"Do endometrial polyps from pre- and post-menopausal women have similar immunohistochemical expression of oestrogen and progesterone receptors (ER, PR) and markers of cellular proliferation/apoptosis (Ki67 and Bcl-2).\nProspective cohort study. Non-parametric statistical analysis was used.\nPolyps recruited from women attending an out-patient hysteroscopy clinic in a UK district general hospital.\nFourteen pre-menopausal and 16 post-menopausal women who presented with abnormal bleeding with endometrial polyps.\nImmunohistochemical staining was performed on endometrial polyps.\nSignificant differences or correlations between hormone receptor expression (oestrogen and progesterone) and cell growth indices (Ki67 and Bcl-2).\nEndometrial polyps from pre- and post-menopausal women had significant differences in their expression of hormone receptors and Ki67. However, polyps from both groups of women had similarly increased levels of Bcl-2, an inhibitor of apoptosis.\"\nQuestion:\n\"Are endometrial polyps from pre-menopausal women similar to post-menopausal women?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9427037": {
                "source": [
                    "\"The most common primary brain tumors in children and adults are of astrocytic origin. Classic histologic grading schemes for astrocytomas have included evaluating the presence or absence of nuclear abnormalities, mitoses, vascular endothelial proliferation, and tumor necrosis.\nWe evaluated the vascular pattern of 17 astrocytoma surgical specimens (seven from children and 10 from adults), and four normal brains obtained at autopsy, utilizing antibody to glial fibrillary acidic protein (GFAP) and von Willebrand factor (vWF) utilizing confocal microscopy. A modified WHO classification was used.\nAll tumor cases showed cells positive for GFAP. Control tissues showed a few, widely separated vessels. Pilocytic astrocytomas (four cases) showed lacy clusters of small-to-medium sized vessels, with intact vessel wall integrity. Diffuse, low grade astrocytoma (three cases) showed a staining pattern similar to control tissue; intermediate grade (one case), anaplastic astrocytoma (three cases) and gliobastoma multiforme (six cases) showed an increased vessel density with multiple small vessels (glomeruloid clusters), some with prominent intimal hyperplasia, loss of vessel wall integrity, and with numerous vWF-positive single cells/microvessels within the tumor substance.\"\nQuestion:\n\"Are endothelial cell patterns of astrocytomas indicative of grade?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23096188": {
                "source": [
                    "\"The primary physis is responsible for longitudinal bone growth. Similarly, epiphysial growth relies on endochondral ossification from the circumferential secondary physeal [corrected]. injury can result in disruption of normal ossification. The cause of juvenile osteochondritis dissecans (OCD) remains elusive. We hypothesized that juvenile OCD results from an insult affecting endochondral ossification from the secondary physis. The purpose of our study was to evaluate the MRI appearance of the distal femoral epiphysis-particularly the secondary physis-of children with juvenile OCD and to compare these findings with the MRI findings of unaffected children.\nKnee MRI examinations of 30 children (age range, 8 years 8 months to 13 years 4 months) with OCD and 30 matched control patients were evaluated for skeletal maturity; location of the OCD lesion, if present; secondary physeal [corrected] continuity; overlying chondroepiphysial integrity, contour, and width; signal intensity of subchondral bone; and secondary physeal [corrected]conspicuity. Variables were compared using chi-square tests.\nAll children were skeletally immature. Condylar lesions were medial in 24 knees and lateral in six knees. All were in the middle one third, posterior one third, or middle and posterior thirds in the sagittal plane. The majority of lesions spanned the intercondylar and middle one third of the femoral condyle in the coronal plane (73%). There was a significant difference between secondary physeal [corrected] disruption in juvenile OCD condyles compared with unaffected condyles (p<0.001) and control condyles (p<0.001). Compared with unaffected and control condyles, the OCD group showed chondroepiphysial widening (p<0.001) and subchondral bone edema (p<0.001) on MRI. Neither chondroepiphysial integrity nor chondroepiphysial contour was significantly different between groups (p = 0.21, p = 0.31, respectively).\"\nQuestion:\n\"Juvenile osteochondritis dissecans: is it a growth disturbance of the secondary physis of the epiphysis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15995461": {
                "source": [
                    "\"This article examines the hypothesis that the six U.S. states with the highest rates of road traffic deaths (group 1 states) also had above-average rates of other forms of injury such as falling, poisoning, drowning, fire, suffocation, homicide, and suicide, and also for the retail trade and construction industries. The converse, second hypothesis, for the six states with the lowest rates of road traffic deaths (group 2 states) is also examined.\nData for these 12 states for the period 1983 to 1995 included nine categories of unintentional and four categories of intentional injury. Seventy-four percent of the group 1 states conformed to the first hypothesis, and 85% of the group 2 states conformed to the second hypothesis.\"\nQuestion:\n\"Do some U.S. states have higher/lower injury mortality rates than others?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18719011": {
                "source": [
                    "\"To compare growth curves of body mass index from children to adolescents, and then to young adults, in Japanese girls and women in birth cohorts born from 1930 to 1999.\nRetrospective repeated cross sectional annual nationwide surveys (national nutrition survey, Japan) carried out from 1948 to 2005.\nJapan.\n76,635 females from 1 to 25 years of age.\nBody mass index.\nGenerally, body mass index decreased in preschool children (2-5 years), increased in children (6-12 years) and adolescents (13-18 years), and slightly decreased in young adults (19-25 years) in these Japanese females. However, the curves differed among birth cohorts. More recent cohorts were more overweight as children but thinner as young women. The increments in body mass index in early childhood were larger in more recent cohorts than in older cohorts. However, the increments in body mass index in adolescents were smaller and the decrease in body mass index in young adults started earlier, with lower peak values in more recent cohorts than in older cohorts. The decrements in body mass index in young adults were similar in all birth cohorts.\"\nQuestion:\n\"Do overweight children necessarily make overweight adults?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12153648": {
                "source": [
                    "\"Women's experiences of childbirth may affect their future reproduction, and the model of care affects their experiences, suggesting that a causal link may exist between model of care and future reproduction. The study objective was to examine whether the birth center model of care during a woman's first pregnancy affects whether or not she has a second baby, and on the spacing to the next birth.\nBetween October 1989 and July 1993, a total of 1860 women at low medical risk in early pregnancy, who participated in a randomized controlled trial of in-hospital birth center care versus standard care, gave birth. The 1063 primiparas in the trial, 543 in the birth center group and 520 in the standard care group, were included in a secondary analysis in which women's personal identification codes were linked to the Swedish National Birth Register, which included information about their subsequent birth during the following 7 to 10 years. Time to an event curves were constructed by means of the Kaplan Meier method.\nThe observation period after the first birth was on average 8.8 years in the birth center group and 8.7 years in the standard care group. No statistical difference was found between the groups in time to second birth, which was 2.85 and 2.82 years, respectively (median; log-rank 1.26; p=0.26).\"\nQuestion:\n\"Does birth center care during a woman's first pregnancy have any impact on her future reproduction?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8199520": {
                "source": [
                    "\"To explore expressed needs, both formal and informal, of family caregivers of frail elderly. To evaluate roles of physicians.\nQuestionnaire survey of members of the Montreal Jewish community providing care for frail elderly family members.\nJewish community of Montreal.\nVolunteer caregivers who were caring for a family member or friend 60 years or older, who had greatest responsibility for providing physical or emotional support to an elderly person, who saw themselves as caregivers, and who could speak English or French were studied. Of 118 volunteers, 32 were excluded because they withdrew for personal reasons or because they did not meet study criteria.\nDemographic variables, functional status of the care receiver, use of home care services, and needs assessment to identify additional services.\nAn average of 75.4% respondents did not use formal support services. Just under half of caregivers were dissatisfied with the attention they received from the health care system, and more than one third expressed feelings of stress, depression, guilt, and isolation.\"\nQuestion:\n\"Are physicians meeting the needs of family caregivers of the frail elderly?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19578820": {
                "source": [
                    "\"Opioid-dependent patients often have co-occurring chronic illnesses requiring medications that interact with methadone. Methadone maintenance treatment (MMT) is typically provided separately from medical care. Hence, coordination of medical care and substance use treatment is important to preserve patient safety.\nTo identify potential safety risks among MMT patients engaged in medical care by evaluating the frequency that opioid dependence and MMT documentation are missing in medical records and characterizing potential medication-methadone interactions.\nAmong patients from a methadone clinic who received primary care from an affiliated, but separate, medical center, we reviewed electronic medical records for documentation of methadone, opioid dependence, and potential drug-methadone interactions. The proportions of medical records without opioid dependence and methadone documentation were estimated and potential medication-methadone interactions were identified.\nAmong the study subjects (n = 84), opioid dependence documentation was missing from the medical record in 30% (95% CI, 20%-41%) and MMT documentation was missing from either the last primary care note or the last hospital discharge summary in 11% (95% CI, 5%-19%). Sixty-nine percent of the study subjects had at least 1 medication that potentially interacted with methadone; 19% had 3 or more potentially interacting medications.\"\nQuestion:\n\"Are opioid dependence and methadone maintenance treatment (MMT) documented in the medical record?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27040842": {
                "source": [
                    "\"To measure the dimensions of compensatory hypertrophy of the middle turbinate in patients with nasal septal deviation, before and after septoplasty.\nThe mucosal and bony structures of the middle turbinate and the angle of the septum were measured using radiological analysis before septoplasty and at least one year after septoplasty. All pre- and post-operative measurements of the middle turbinate were compared using the paired sample t-test and Wilcoxon rank sum test.\nThe dimensions of bony and mucosal components of the middle turbinate on concave and convex sides of the septum were not significantly changed by septoplasty. There was a significant negative correlation after septoplasty between the angle of the septum and the middle turbinate total area on the deviated side (p = 0.033).\"\nQuestion:\n\"Does septoplasty change the dimensions of compensatory hypertrophy of the middle turbinate?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9444542": {
                "source": [
                    "\"To investigate whether the presence of hippocampal atrophy (HCA) on MRI in Alzheimer's disease (AD) leads to a more rapid decline in cognitive function. To investigate whether cognitively unimpaired controls and depressed subjects with HCA are at higher risk than those without HCA of developing dementia.\nA prospective follow-up of subjects from a previously reported MRI study.\nMelbourne, Australia.\nFive controls with HCA and five age-matched controls without HCA, seven depressed subjects with HCA and seven without HCA, and 12 subjects with clinically diagnosed probable AD with HCA and 12 without HCA were studied. They were followed up at approximately 2 years with repeat cognitive testing, blind to initial diagnosis and MRI result.\nHCA was rated by two radiologists blind to cognitive test score results. Cognitive assessment was by the Cambridge Cognitive Examination (CAMCOG).\nNo significant differences in rate of cognitive decline, mortality or progression to dementia were found between subjects with or without HCA.\"\nQuestion:\n\"Does hippocampal atrophy on MRI predict cognitive decline?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15489384": {
                "source": [
                    "\"Spasticity and loss of function in an affected arm are common after stroke. Although botulinum toxin is used to reduce spasticity, its functional benefits are less easily demonstrated. This paper reports an exploratory meta-analysis to investigate the relationship between reduced arm spasticity and improved arm function.\nIndividual data from stroke patients in two randomised controlled trials of intra-muscular botulinum toxin were pooled. The Modified Ashworth Scale (elbow, wrist, fingers) was used to calculate a \"Composite Spasticity Index\". Data from the arm section of the Barthel Activities of Daily Living Index (dressing, grooming, and feeding) and three subjective measures (putting arm through sleeve, cleaning palm, cutting fingernails) were summed to give a \"Composite Functional Index\". Change scores and the time of maximum change were also calculated.\nMaximum changes in both composite measures occurred concurrently in 47 patients. In 26 patients the improvement in spasticity preceded the improvement in function with 18 showing the reverse. There was a definite relationship between the maximum change in spasticity and the maximum change in arm function, independent of treatment (rho = -0.2822, p = 0.0008, n = 137). There was a clear relationship between the changes in spasticity and in arm function in patients treated with botulinum toxin (Dysport) at 500 or 1000 units (rho = -0.5679, p = 0.0090, n = 22; rho = -0.4430, p = 0.0018, n = 47), but not in those treated with placebo or 1500 units.\"\nQuestion:\n\"Does reducing spasticity translate into functional benefit?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25885219": {
                "source": [
                    "\"Pregnancy induces adaptations in maternal metabolism to meet the increased need for nutrients by the placenta and fetus. Creatine is an important intracellular metabolite obtained from the diet and also synthesised endogenously. Experimental evidence suggests that the fetus relies on a maternal supply of creatine for much of gestation. However, the impact of pregnancy on maternal creatine homeostasis is unclear. We hypothesise that alteration of maternal creatine homeostasis occurs during pregnancy to ensure adequate levels of this essential substrate are available for maternal tissues, the placenta and fetus. This study aimed to describe maternal creatine homeostasis from mid to late gestation in the precocial spiny mouse.\nPlasma creatine concentration and urinary excretion were measured from mid to late gestation in pregnant (n = 8) and age-matched virgin female spiny mice (n = 6). At term, body composition and organ weights were assessed and tissue total creatine content determined. mRNA expression of the creatine synthesising enzymes arginine:glycine amidinotransferase (AGAT) and guanidinoacetate methyltransferase (GAMT), and the creatine transporter (CrT1) were assessed by RT-qPCR. Protein expression of AGAT and GAMT was also assessed by western blot analysis.\nPlasma creatine and renal creatine excretion decreased significantly from mid to late gestation (P<0.001, P<0.05, respectively). Pregnancy resulted in increased lean tissue (P<0.01), kidney (P<0.01), liver (P<0.01) and heart (P<0.05) mass at term. CrT1 expression was increased in the heart (P<0.05) and skeletal muscle (P<0.05) at term compared to non-pregnant tissues, and creatine content of the heart (P<0.05) and kidney (P<0.001) were also increased at this time. CrT1 mRNA expression was down-regulated in the liver (<0.01) and brain (<0.01) of pregnant spiny mice at term. Renal AGAT mRNA (P<0.01) and protein (P<0.05) expression were both significantly up-regulated at term, with decreased expression of AGAT mRNA (<0.01) and GAMT protein (<0.05) observed in the term pregnant heart. Brain AGAT (<0.01) and GAMT (<0.001) mRNA expression were also decreased at term.\"\nQuestion:\n\"Maternal creatine homeostasis is altered during gestation in the spiny mouse: is this a metabolic adaptation to pregnancy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16465002": {
                "source": [
                    "\"To study the correlation and agreement between end-tidal carbon dioxide (EtCO2) and arterial carbon dioxide (PaCO(2)) in ventilated extremely low birth weight (ELBW) infants in the first week of life.\nRetrospective chart review of all ELBW (<1,000 g) infants admitted to a level III NICU from January 2003 to December 2003. Data collected included demographic details and simultaneous EtCO(2) (mainstream capnography) and arterial blood gas values (pH, PaCO(2), PaO(2)).\nThe correlation coefficient, degree of bias with 95% confidence interval between the EtCO(2) and PaCO(2).\nThere were 754 end-tidal and arterial CO(2) pairs from 31 ELBW infants (21 male and 10 female). The overall EtCO(2) values were significantly lower than PaCO(2) value. In only 89/754(11.8%) pairs, the EtCO(2) was higher than the PaCO(2). The overall bias was 5.6 +/- 6.9 mmHg (95% C.I. 5.11-6.09). The intraclass correlation coefficient was 0.81. Using EtCO2 ranges of 30 to 50 mmHg, the capnographic method was able to identify 84% of instances where PaCO(2) was between 35 (<35 = hypocarbia) and 55 mmHg (>55= hypercapnia).\"\nQuestion:\n\"Dose end-tidal carbon dioxide measurement correlate with arterial carbon dioxide in extremely low birth weight infants in the first week of life?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24183388": {
                "source": [
                    "\"To assess if the Hawkins sign can predict whether or not astragalus fractures of the neck will develop avascular necrosis. It is also assessed whether the occurrence of this complication is related to the displacement of the fracture, soft tissue injury, or delay in the reduction or surgery. The results were compared with those found in the literature.\nA retrospective study was conducted on 23 talar neck fractures recorded over a a period of thirteen years. The following variables were analysed: displacement of the fracture, soft tissue injury, delay and type of treatment, complications, observation of the Hawkins sign, and functional outcome.\nThere were 7 type I Hawkins fractures, 11 type II, and 4 type III and 1 type IV. Four cases developed avascular necrosis (2 Hawkins type II and 2 type III). Hawkins sign was observed in 12 cases, of which none developed necrosis. Four cases with negative Hawkins sign developed necrosis. No statistically significant differences were found when comparing the development of avascular necrosis with the displacement of the fracture, soft tissue injury, or delay in treatment. Differences were found when comparing the development of avascular necrosis with the Hawkins sign (P=.03).\"\nQuestion:\n\"Is the Hawkins sign able to predict necrosis in fractures of the neck of the astragalus?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21431987": {
                "source": [
                    "\"This study was designed to determine prospectively whether the systematic use of PET/CT associated with conventional techniques could improve the accuracy of staging in patients with liver metastases of colorectal carcinoma. We also assessed the impact on the therapeutic strategy.\nBetween 2006 and 2008, 97 patients who were evaluated for resection of LMCRC were prospectively enrolled. Preoperative workup included multidetector-CT (MDCT) and PET/CT. In 11 patients with liver steatosis or iodinated contrast allergy, MR also was performed. Sixty-eight patients underwent laparotomy. Sensitivity, specificity, positive predictive value (PPV), and negative predictive values for hepatic and extrahepatic staging of MDCT and PET-CT were calculated.\nIn a lesion-by-lesion analysis of the hepatic staging, the sensitivity of MDCT/RM was superior to PET/CT (89.2 vs. 55%, p\u00a0<\u00a00.001). On the extrahepatic staging, PET/CT was superior to MDCT/MR only for the detection of locoregional recurrence (p\u00a0=\u00a00.03) and recurrence in uncommon sites (p\u00a0=\u00a00.016). New findings in PET/CT resulted in a change in therapeutic strategy in 17 patients. However, additional information was correct only in eight cases and wrong in nine patients.\"\nQuestion:\n\"Preoperative staging of patients with liver metastases of colorectal carcinoma. Does PET/CT really add something to multidetector CT?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17306983": {
                "source": [
                    "\"Enlargement of the ascending aorta is often combined with valvular, coronary, or other cardiac diseases. Reduction aortoplasty can be an optional therapy; however, indications regarding the diameter of aorta, the history of dilatation (poststenosis, bicuspid aortic valve), or the intraoperative management (wall excision, reduction suture, external reinforcement) are not established.\nIn a retrospective study between 1997 and 2005, we investigated 531 patients operated for aneurysm or ectasia of the ascending aorta (diameter: 45-76mm). Of these, in 50 patients, size-reducing ascending aortoplasty was performed. External reinforcement with a non-coated dacron prosthesis was added in order to stabilize the aortic wall.\nAortoplasty was associated with aortic valve replacement in 47 cases (35 mechanical vs 12 biological), subvalvular myectomy in 29 cases, and CABG in 13 cases. The procedure was performed with low hospital mortality (2%) and a low postoperative morbidity. Computertomographic and echocardiographic diameters were significantly smaller after reduction (55.8+/-9mm down to 40.51+/-6.2mm (CT), p<0.002; 54.1+/-6.7mm preoperatively down to 38.7+/-7.1mm (echocardiography), p<0.002), with stable performance in long-term follow-up (mean follow-up time: 70 months).\"\nQuestion:\n\"Is size-reducing ascending aortoplasty with external reinforcement an option in modern aortic surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15841770": {
                "source": [
                    "\"Annual data on children aged under 16 y treated for asthma, including consumption of regular medication for asthma, numbers of hospital periods, lengths of hospitalizations and annual proportions of readmissions, were collected using patient-specific medical records from 1995 to 1999. In the Kuopio province, on average, 35.6-36.7/1000 children were on maintenance for asthma, of which 23% were receiving cromones, 51% were taking inhaled steroids and 26% were treated with cromones plus intermittent steroids. In the Oulu province, the respective prevalence was 32.7-34.9/1000, and the respective proportions were 5%, 93% and 2%.\nTotal and first admissions, as well as hospital days were clearly less in the Oulu province. In the children aged>or = 6y, the average annual total admissions were 0.3/1000 (Oulu) vs 1.2/1000 (Kuopio) (p<0.001). Similarly, the first admissions were 0.2/1000 vs 1.0/1000 (p<0.001), proportions of readmissions 6.3% vs 19.3% (p<0.05), and numbers of hospital days 0.7/1000 vs 3.8/1000 (p<0.001). The differences were in the same direction, though less prominent, also among children 2-5 y of age.\"\nQuestion:\n\"Do inhaled steroids differ from cromones in terms of hospital admission rates for asthma in children?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20187289": {
                "source": [
                    "\"Stock et al. (Eur Respir J 25:47-53, 2005) recently estimated asthma prevalence in Germany using claims data on prescriptions and hospital diagnoses and found high prevalence peaks in infants. Our objective was to critically assess and discuss various aspects of identifying children with asthma using prescription data.\nWe replicated the selection procedure of Stock et al. using data on 290,919 children aged 0-17 years insured in the Gm\u00fcnder ErsatzKasse (GEK) in 2005. Asthma prevalence was also estimated in a sample of 17,641 children aged 0-17 years participating in the German Health Interview and Examination Survey for Children and Adolescents (KiGGS) from 2003 to 2006.\nIn children aged 0-4 years insured in the GEK, prevalences were found to range from 11.7 to 17.7% for boys and from 7.2 to 11.1% for girls when the criteria of Stock et al. were applied. A steady decline in prevalences was observed in older age groups. Asthma prevalence estimated in the KiGGS data showed a quite different distribution. In the age group 0-4 years, prevalences were found to range from 0 to 2.6% in boys and from 0 to 1.0% in girls; in children>4 years, prevalences were found to increase with increasing age.\"\nQuestion:\n\"Prescriptions as a proxy for asthma in children: a good choice?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20297950": {
                "source": [
                    "\"To investigate the effect of fenofibrate on sleep apnoea indices.\nProof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period.\nNCT00816829.\nAs this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour.\nFenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate.\nExploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.\"\nQuestion:\n\"Proof of concept study: does fenofibrate have a role in sleep apnoea syndrome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20602784": {
                "source": [
                    "\"This paper investigates the impact of geographic scale (census tract, zip code, and county) on the detection of disparities in breast cancer mortality among three ethnic groups in Texas (period 1995-2005). Racial disparities were quantified using both relative (RR) and absolute (RD) statistics that account for the population size and correct for unreliable rates typically observed for minority groups and smaller geographic units. Results were then correlated with socio-economic status measured by the percentage of habitants living below the poverty level.\nAfrican-American and Hispanic women generally experience higher mortality than White non-Hispanics, and these differences are especially significant in the southeast metropolitan areas and southwest border of Texas. The proportion and location of significant racial disparities however changed depending on the type of statistic (RR versus RD) and the geographic level. The largest proportion of significant results was observed for the RD statistic and census tract data. Geographic regions with significant racial disparities for African-Americans and Hispanics frequently had a poverty rate above 10.00%.\"\nQuestion:\n\"Identification of racial disparities in breast cancer mortality: does scale matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27405146": {
                "source": [
                    "\"The neonatal infection by Streptococcus group B is one of the main causes of neonatal morbi-mortality rate. For this reason a screening is made to each pregnant woman in order to detect its presence, and if it was the case, to apply an antibiotic treatment during labour. The aim of this study was to know the prevalence of this Streptococcus in the pregnant women from Melilla, as well as the differences according to culture and age.\nA descriptive cross-sectional study located in the Hospital Comarcal from Melilla.\nThe sample is taken from 280 women: 194 are from Muslim culture (69.3%), 68 are from Christian culture (24.3%) and 18 women from unknown cultures (6.4%). Also it is known that 78 of them are 25 years old or less (27.85%), 158 are between 26 and 34 years old (56.42%) and 44 are 35 years old or more (15.71%).\"\nQuestion:\n\"PREVALENCE OF THE STREPTOCOCUS AGALACTIAE IN THE PREGNANT WOMAN FROM THE AUTONOMIC CITY OF MELILLA: IS CULTURE A DETERMINANT FACTOR?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17323047": {
                "source": [
                    "\"We examined whether invasive lung-specimen collection-to-treatment times for intensive care unit patients with suspected ventilator-associated pneumonia (VAP) differ with to the work shift during which specimens were collected. We compared weekday day shifts and off-hours (from 6:30 p.m. to 8:29 a.m. the next day for night shifts, from Saturday 1:00 p.m. to Monday 8:29 a.m. for weekends, and from 8:30 a.m. to 8:29 a.m. the following morning for public holidays).\nSingle-center, observational study in the intensive care unit in an academic teaching hospital.\n101 patients who developed 152 episodes of bacteriologically confirmed VAP.\nOf the 152 VAP episodes 66 were diagnosed during off-hours. Neither more bronchoscopy complications nor more inappropriate initial antimicrobial treatments for patients were observed between day and off-hour shifts. Indeed, the overall time from brochoalveolar lavage to antibiotic administration was shorter for off-hours than day-shifts due to shorter specimen collection-to-antibiotic prescription times, but antibiotic prescription-to-administration times were the same.\"\nQuestion:\n\"Does invasive diagnosis of nosocomial pneumonia during off-hours delay treatment?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15065192": {
                "source": [
                    "\"The use of three-dimensional (3D) ultrasound may help to determine the exact position of the needle during breast biopsy, thereby reducing the number of core samples that are needed to achieve a reliable histological diagnosis. The aim of this study was to demonstrate the efficacy of 3D ultrasound-validated large-core needle biopsy (LCNB) of the breast.\nA total of 360 core needle biopsies was obtained from 169 breast lesions in 146 patients. Additional open breast biopsy was performed in 111 women (127/169 breast lesions); the remaining 42 lesions were followed up for at least 24 months. 3D ultrasound visualization of the needle in the postfiring position was used to classify the biopsy as central, marginal or outside the lesion. Based on this classification it was decided whether another sample had to be obtained.\nA median of two core samples per lesion provided for all the lesions a sensitivity for malignancy of 96.9%, specificity of 100%, false-positive rate of 0% and false-negative rate of 3.1%, and for the excised lesions a sensitivity of 96.5%, specificity of 100%, false-positive rate of 0%, false-negative rate of 3.5% and an underestimation rate of 3.4%.\"\nQuestion:\n\"Three-dimensional ultrasound-validated large-core needle biopsy: is it a reliable method for the histological assessment of breast lesions?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23240452": {
                "source": [
                    "\"In recent years the role of trace elements in lithogenesis has received steadily increasing attention.\nThis study was aimed to attempt to find the correlations between the chemical content of the stones and the concentration of chosen elements in the urine and hair of stone formers.\nThe proposal for the study was approved by the local ethics committee. Specimens were taken from 219 consecutive stone-formers. The content of the stone was evaluated using atomic absorption spectrometry, spectrophotometry, and colorimetric methods. An analysis of 29 elements in hair and 21 elements in urine was performed using inductively coupled plasma-atomic emission spectrometry.\nOnly a few correlations between the composition of stones and the distribution of elements in urine and in hair were found. All were considered incidental.\"\nQuestion:\n\"Can we predict urinary stone composition based on an analysis of microelement concentration in the hair and urine?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17559449": {
                "source": [
                    "\"The reduced use of sugars-containing (SC) liquid medicines has increased the use of other dose forms, potentially resulting in more widespread dental effects, including tooth wear. The aim of this study was to assess the erosive potential of 97 paediatric medicines in vitro.\nThe study took the form of in vitro measurement of endogenous pH and titratable acidity (mmol). Endogenous pH was measured using a pH meter, followed by titration to pH 7.0 with 0.1-M NaOH.\nOverall, 55 (57%) formulations had an endogenous pH of<5.5. The mean (+/- SD) endogenous pH and titratable acidity for 41 SC formulations were 5.26 +/- 1.30 and 0.139 +/- 0.133 mmol, respectively; for 56 sugars-free (SF) formulations, these figures were 5.73 +/- 1.53 and 0.413 +/- 1.50 mmol (P>0.05). Compared with their SC bioequivalents, eight SF medicines showed no significant differences for pH or titratable acidity, while 15 higher-strength medicines showed lower pH (P = 0.035) and greater titratable acidity (P = 0.016) than their lower-strength equivalents. Chewable and dispersible tablets (P<0.001), gastrointestinal medicines (P = 0.002) and antibiotics (P = 0.007) were significant predictors of higher pH. In contrast, effervescent tablets (P<0.001), and nutrition and blood preparations (P = 0.021) were significant predictors of higher titratable acidity.\"\nQuestion:\n\"Are sugars-free medicines more erosive than sugars-containing medicines?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18507507": {
                "source": [
                    "\"Specialty pharmaceuticals have evolved beyond their status as niche drugs designed to treat rare conditions and are now poised to become the standard of care in a wide variety of common chronic illnesses. Due in part to the cost of these therapies, payers are increasingly demanding evidence of their value. Determining the value of these medications is hampered by a lack of robust pharmacoeconomic data.\nTo outline emerging strategies and case study examples for the medical and pharmacy benefits management of specialty pharmaceuticals.\nThe promise of specialty pharmaceuticals: increased life expectancy, improved quality of life, enhanced workplace productivity, decreased burden of disease, and reduced health care spending comes at a significant cost. These agents require special handling, administration, patient education, clinical support, and risk mitigation. Additionally, specialty drugs require distribution systems that ensure appropriate patient selection and data collection. With the specialty pharmaceutical pipeline overflowing with new medicines and an aging population increasingly relying on these novel treatments to treat common diseases, the challenge of managing the costs associated with these agents can be daunting. Aided by sophisticated pharmacoeconomic models to assess value, the cost impacts of these specialty drugs can be appropriately controlled.\"\nQuestion:\n\"The promise of specialty pharmaceuticals: are they worth the price?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26965932": {
                "source": [
                    "\"This study sought to investigate the ischemic and bleeding outcomes of patients fulfilling high bleeding risk (HBR) criteria who were randomized to zotarolimus-eluting Endeavor Sprint stent (E-ZES) or bare-metal stent (BMS) implantation followed by an abbreviated dual antiplatelet therapy (DAPT) duration for stable or unstable coronary artery disease.\nDES instead of BMS use remains controversial in HBR patients, in whom long-term DAPT poses safety concerns.\nThe ZEUS (Zotarolimus-Eluting Endeavor Sprint Stent in Uncertain DES Candidates) is a multinational, randomized single-blinded trial that randomized among others, in a stratified manner, 828 patients fulfilling pre-defined clinical or biochemical HBR criteria-including advanced age, indication to oral anticoagulants or other pro-hemorrhagic medications, history of bleeding and known anemia-to receive E-ZES or BMS followed by a protocol-mandated 30-day DAPT regimen. The primary endpoint of the study was the 12-month major adverse cardiovascular event rate, consisting of death, myocardial infarction, or target vessel revascularization.\nCompared with patients without, those with 1 or more HBR criteria had worse outcomes, owing to higher ischemic and bleeding risks. Among HBR patients, major adverse cardiovascular events occurred in 22.6% of the E-ZES and 29% of the BMS patients (hazard ratio: 0.75; 95% confidence interval: 0.57 to 0.98; p = 0.033), driven by lower myocardial infarction (3.5% vs. 10.4%; p<0.001) and target vessel revascularization (5.9% vs. 11.4%; p = 0.005) rates in the E-ZES arm. The composite of definite or probable stent thrombosis was significantly reduced in E-ZES recipients, whereas bleeding events did not differ between stent groups.\"\nQuestion:\n\"Is Bare-Metal Stent Implantation Still Justifiable in High Bleeding Risk Patients Undergoing Percutaneous Coronary Intervention?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16319544": {
                "source": [
                    "\"There is a positive association between chronic inflammation and the risk of cardiovascular disease, but whether there is an association between C-reactive protein (CRP) and carotid atherosclerosis is controversial. We investigated the relationship between high-sensitivity CRP (hsCRP) levels and carotid intima-media thickness (IMT) in healthy Koreans.\nWe measured hsCRP levels, the carotid IMT, and conventional cardiovascular risk factors including obesity parameters, blood pressure, lipid profiles, insulin resistance, and smoking habits in 820 volunteers (35-79 years old) in a cross-sectional study.\nHigher hsCRP quartile groups had higher mean IMTs, as compared with the lowest quartile (P<0.001 for the trend across quartiles). However, after adjustment for age, the relationship between hsCRP level and IMT was substantially weaker (P = 0.018). After additional adjustments for conventional cardiovascular risk factors, no significant association was observed (P = 0.548). The unadjusted risk for a high carotid IMT value (>or = 1.0 mm) was also positively related to hsCRP quartile, but this relationship was not significant after adjustment for age and other cardiovascular risk factors.\"\nQuestion:\n\"Is high-sensitivity C-reactive protein associated with carotid atherosclerosis in healthy Koreans?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18243752": {
                "source": [
                    "\"This study aims to evaluate local failure patterns in node negative breast cancer patients treated with post-mastectomy radiotherapy including internal mammary chain only.\nRetrospective analysis of 92 internal or central-breast node-negative tumours with mastectomy and external irradiation of the internal mammary chain at the dose of 50 Gy, from 1994 to 1998.\nLocal recurrence rate was 5 % (five cases). Recurrence sites were the operative scare and chest wall. Factors associated with increased risk of local failure were age<or = 40 years and tumour size greater than 20mm, without statistical significance.\"\nQuestion:\n\"Should chest wall irradiation be included after mastectomy and negative node breast cancer?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19419587": {
                "source": [
                    "\"Sternal instability with mediastinitis is a very serious complication after median sternotomy. Biomechanical studies have suggested superiority of rigid plate fixation over wire cerclage for sternal fixation. This study tests the hypothesis that sternal closure stability can be improved by adding plate fixation in a human cadaver model.\nMidline sternotomy was performed in 18 human cadavers. Four sternal closure techniques were tested: (1) approximation with six interrupted steel wires; (2) approximation with six interrupted cables; (3) closure 1 (wires) or 2 (cables) reinforced with a transverse sternal plate at the sixth rib; (4) Closure using 4 sternal plates alone. Intrathoracic pressure was increased in all techniques while sternal separation was measured by three pairs of sonomicrometry crystals fixed at the upper, middle and lower parts of the sternum until 2.0 mm separation was detected. Differences in displacement pressures were analyzed using repeated measures ANOVA and Regression Coefficients.\nIntrathoracic pressure required to cause 2.0 mm separation increased significantly from 183.3 +/- 123.9 to 301.4 +/- 204.5 in wires/cables alone vs. wires/cables plus one plate respectively, and to 355.0 +/- 210.4 in the 4 plates group (p<0.05). Regression Coefficients (95% CI) were 120 (47-194) and 142 (66-219) respectively for the plate groups.\"\nQuestion:\n\"Sternal plating for primary and secondary sternal closure; can it improve sternal stability?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19931500": {
                "source": [
                    "\"The aim of this study was to analyze the properties of the immune cell microenvironment of regional lymph nodes (LNs) positive for lung cancer.\nTwenty-four patients operated on for stages T1 and T2 of the NSCLC, were enrolled in the study. Peripheral blood and LN tissue were obtained from different lymph node sites and levels. As a control, LN tissue was taken from patients diagnosed with emphysema or pneumothorax. The cells from randomly chosen LN were tested by multi-color flow cytometry. Separate portions of LN were snap-frozen and examined for the presence of cytokeratin positive cells (CK). Propensity for apoptosis, level of TCR zeta chain expression of T cells and the number and maturation status of dendritic cells were confronted with the presence of CK-positive cells.\nThe presence of metastases correlated with the downregulation of TCR zeta, especially CD8(+) T cells. The most striking feature was the reduction in the number of myeloid CD11c(+) dendritic cells in the LN of patients with LN metastases. This could be a reflection of the immunodeficient state observed in lung cancer patients. Even in the absence of metastases in the regional LN, the same type of changes in the LN microenvironment were observed in those LN located nearer the primary tumor.\"\nQuestion:\n\"Can the condition of the cell microenvironment of mediastinal lymph nodes help predict the risk of metastases in non-small cell lung cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28027677": {
                "source": [
                    "\"There is increasing concern amongst educators that the provision of recorded lectures may reduce student attendance of live lectures. We therefore sought to determine if the provision of prerecorded lecture video podcasts (VODcasts) to first-year Graduate Entry to Medicine (GEM) students, affected attendance at 21 Physiology lectures within three separate pre-clinical modules.\nData on lecture attendance, utilization of VODcasts, and whether VODcasts should replace live lectures were drawn from three surveys conducted in academic years 2014-2015 and 2015-2016 on all first-year GEM students in two first-year pre-clinical modules where prerecorded Physiology VODcasts were available for viewing or downloading prior to scheduled live lectures.\nA total of 191/214 (89%) students responded to the three surveys, with 84.3% of students attending all 21 lectures in the study. Only 4% of students missed more than one lecture in each of the three lecture series, with 79% indicating that VODcasts should not replace lectures.\"\nQuestion:\n\"Do prerecorded lecture VODcasts affect lecture attendance of first-yearpre-clinical Graduate Entry to Medicine students?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21166749": {
                "source": [
                    "\"\u2022 Robot-assisted radical cystectomy (RARC) remains controversial in terms of oncologic outcomes, especially during the initial experience. The purpose of this study was to evaluate the impact of initial experience of robotic cystectomy programs on oncologic outcomes and overall survival.\n\u2022 Utilizing a prospectively maintained, single institution robotic cystectomy database, we identified 164 consecutive patients who underwent RARC since November 2005. \u2022 After stratification by age group, gender, pathologic T stage, lymph node status, surgical margin status, and sequential case number; we used chi-squared analyses to correlate sequential case number to operative time, surgical blood loss, lymph node yield, and surgical margin status. \u2022 We also addressed the relationship between complications and sequential case number. We then utilized Cox proportional hazard modeling and Kaplan-Meier survival analyses to correlate variables to overall mortality.\n\u2022 Sequential case number was not significantly associated with increased incidence of complications, surgical blood loss, or positive surgical margins (P= 0.780, P= 0.548, P= 0.545). Case number was, however, significantly associated with shorter operative time and mean number of lymph nodes retrieved (P<0.001, P<0.001). \u2022 Sequential case number was not significantly associated with survival; however, tumour stage, the presence of lymph node metastases, and positive surgical margins were significantly associated with death. \u2022 Although being the largest of its kind, this was a small study with short follow-up when compared to open cystectomy series.\"\nQuestion:\n\"Is patient outcome compromised during the initial experience with robot-assisted radical cystectomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15141797": {
                "source": [
                    "\"Arterial catheters are routinely used to sample blood for clotting studies in most cardiothoracic intensive care units. The clotting profile in surgical bleeding after cardiac surgery influences further management. Aspiration and discard of a certain amount of blood from the line, prior to sampling, are assumed to clear heparin contamination. We have investigated this assumption through analysis of the clotting profile by simultaneous arterial line and peripheral venous samples.\nThe morning following cardiac surgery, simultaneous arterial line and peripheral venous blood samples were taken for activated plasma thromboplastin time (APTT) ratio and international normalized ratio (INR) in 49 randomly selected patients. Also, a thromboelastogram analysis (TEG) (n = 7) was made. A survey of 22 UK cardiothoracic intensive care units was carried out to determine the practice for the withdrawal of blood for clotting studies.\nThe median arterial APTT ratio value was 1.32 +/- 0.52 as compared to the median peripheral APTT ratio value which was 1.1 +/- 0.24 (P<0.001). INR values were statistically similar by both routes. Heparin contamination was confirmed by TEG which revealed that the R-value for arterial catheter blood samples without heparinase in the cup was higher (406.00 +/- 64.44 s) compared with the value for arterial samples with heparinase in the cup (318.28 +/- 47.26s, P<0.05). The survey of 22 UK cardiothoracic intensive care units showed that heparinized arterial lines were by far the commonest ports used for blood withdrawal for the measurement of APTT ratio results.\"\nQuestion:\n\"Can we rely on arterial line sampling in performing activated plasma thromboplastin time after cardiac surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25779009": {
                "source": [
                    "\"There is no standard protocol for the evaluation of antiseptics used for skin and mucous membranes in the presence of interfering substances. Our objective was to suggest trial conditions adapted from the NF EN 13727 standard, for the evaluation of antiseptics used in gynecology and dermatology.\nThree antiseptic solutions were tested in vitro: a chlorhexidine-benzalkonium (CB) combination, a hexamidine-chlorhexidine-chlorocresol (HCC) combination, and povidone iodine (P). The adaptation of trial conditions to the standard involved choosing dilutions, solvent, and interfering substances. The activity of solutions was assessed on the recommended strains at concentrations of 97% (pure solution), 50%, and 10% (diluted solution), and 1%. A logarithmic reduction \u2265 5 was expected after 60seconds of contact, to meet requirements of bactericidal activity.\nHCC did not present any bactericidal activity except on P. aeruginosa at a concentration of 97%. P was not bactericidal on E. hirae at any concentration and on S. aureus at 97%. CB had the most homogeneous bactericidal activity with a reduction>5 log on the 4 bacterial strains at concentrations of 97%, 50% and 10%.\"\nQuestion:\n\"Bactericidal activity of 3 cutaneous/mucosal antiseptic solutions in the presence of interfering substances: Improvement of the NF EN 13727 European Standard?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20306735": {
                "source": [
                    "\"To ascertain the perspectives of Trainee Ophthalmologist Diplomats (TOD) on the Ophthalmic Diploma Training (ODT) in West Africa with a view to improving the programme.\nA survey of set 2005 TOD on ODT was carried out in Ghana, 2006.\nThe trainees included 10 (83.35%) males and two (16.7%) females whose ages ranged between thirty-two and fifty-one years. The sponsors of the trainees included Sight Savers International, five (41.7%); Christian Blind Mission International, three (25.0%); Eye Foundation, Lagos, Nigeria two (16.7%); Ministry of Defence Nigeria, one (8.3%); and Health Authority Ghana, one (8.3%). Nine trainees (75.0%) felt the programme was well structured, training allowances were adequate eight (66.7%) and inadequate four (33.3%). Eleven (91.7%) trainees would work wherever they were posted; ten (83.3%) trainees had sense of fulfillment and three (25%) would like to proceed for residency training. All trainees were at least good in chalazion surgery and treatment of common medical eye conditions. Majority were at least good in eye surgery like cataract, eleven (91.7%); trabeculectomy nine (75.0%); pterygium 10 (83.3%); eyelid, eight (66.7%); destructive 11 (91.6%) and refraction 9 (75.0%). Some trainees' perceived problems included inadequate sponsorship (33.3%), short duration of the course four (33.3%) and poor accommodation facility two (16.7%). However, trainees' suggested increase in training posts, four (33.3); training allowance three (25.0%); and incentives for trainers/training hospitals two (16.7%).\"\nQuestion:\n\"Fulfilling human resources development goal in West Africa: can the training of ophthalmologist diplomates be improved?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20842006": {
                "source": [
                    "\"Acupuncture has been successfully used in myofascial pain syndromes. However, the number of needles used, that is, the dose of acupuncture stimulation, to obtain the best antinociceptive efficacy is still a matter of debate. The question was addressed comparing the clinical efficacy of two different therapeutic schemes, characterized by a different number of needles used on 36 patients between 29-60 years of age with by a painful cervical myofascial syndrome.\nPatients were divided into two groups; the first group of 18 patients were treated with 5 needles and the second group of 18 patients were treated with 11 needles, the time of needle stimulation was the same in both groups: 100 seconds. Each group underwent six cycles of somatic acupuncture. Pain intensity was evaluated before, immediately after and 1 and 3 months after the treatment by means of both the Mc Gill Pain Questionnaire and the Visual Analogue Scale (VAS). In both groups, the needles were fixed superficially excluding the two most painful trigger points where they were deeply inserted.\nBoth groups, independently from the number of needles used, obtained a good therapeutic effect without clinically relevant differences.\"\nQuestion:\n\"Neck pain treatment with acupuncture: does the number of needles matter?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20073599": {
                "source": [
                    "\"The cytomorphology of liquid-based preparations in urine cytology is different than classic slide preparations.\nTo compare the performance of liquid-based preparation specimens to classically prepared urine specimens with a malignant diagnosis in the College of American Pathologists Interlaboratory Comparison Program in Nongynecologic Cytology.\nParticipant responses between 2000 and 2007 for urine specimens with a reference diagnosis of high-grade urothelial carcinoma/carcinoma in situ/dysplasia (HGUCA), squamous cell carcinoma, or adenocarcinoma were evaluated. ThinPrep and SurePath challenges were compared with classic preparations (smears, cytospins) for discordant responses.\nThere were 18 288 pathologist, 11 957 cytotechnologist, and 8086 \"laboratory\" responses available. Classic preparations comprised 90% (n = 34 551) of urine challenges; 9% (n = 3295) were ThinPrep and 1% (n = 485) were SurePath. Concordance to the general category of \"positive-malignant\" was seen in 92% of classic preparations, 96.5% of ThinPrep, and 94.6% of SurePath challenges (P<.001). These results were statistically different for the exact reference interpretation of HGUCA (P<.001) but not for adenocarcinoma (P = .22). Cytotechnologists demonstrate statistically better performance for the general category of \"positive-malignant\" compared with pathologists for all urinary slide types and for the exact reference interpretation of HGUCA (94% versus 91.1%; P<.001) but not adenocarcinoma (96.3% versus 95.8%; P = .77) or squamous cell carcinoma (93.6% versus 87.7%; P = .07).\"\nQuestion:\n\"Do liquid-based preparations of urinary cytology perform differently than classically prepared cases?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22301406": {
                "source": [
                    "\"CYP2D6 is an important cytochrome P450 enzyme. These enzymes catalyse the oxidative biotransformation of about 25% of clinically important drugs as well as the metabolism of numerous environmental chemical carcinogens. The most frequent null allele of CYP2D6 in European populations, CYP2D6*4, has been studied here in order to elucidate whether a relationship exists between this allele and the risk of developing breast cancer in a Spanish population.\nNinety-six breast cancer Spanish patients and one hundred healthy female volunteers were genotyped for the CYP2D6*4 allele using AmpliChip CYP450 Test technology.\nHomozygous CYP2D6*4 frequency was significant lower in breast cancer patients than in the control group (OR=0.22, p=0.04). The heterozygous CYP2D6*4 group also displayed lower values in patients than in controls but the difference was not significant (OR=0.698, p=0.28). Therefore, the presence of the CYP2D6*4 allele seems to decrease susceptibility to breast carcinoma in the selected population.\"\nQuestion:\n\"CYP2D6*4 allele and breast cancer risk: is there any association?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17704864": {
                "source": [
                    "\"Laparoscopic adrenalectomy (LA) has become the gold standard treatment for small (less than 6 cm) adrenal masses. However, the role of LA for large-volume (more than 6 cm) masses has not been well defined. Our aim was to evaluate, retrospectively, the outcome of LA for adrenal lesions larger than 7 cm.\n18 consecutive laparoscopic adrenalectomies were performed from 1996 to 2005 on patients with adrenal lesions larger than 7 cm.\nThe mean tumor size was 8.3 cm (range 7-13 cm), the mean operative time was 137 min, the mean blood loss was 182 mL (range 100-550 mL), the rate of intraoperative complications was 16%, and in three cases we switched from laparoscopic procedure to open surgery.\"\nQuestion:\n\"Is laparoscopic adrenalectomy safe and effective for adrenal masses larger than 7 cm?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24827404": {
                "source": [
                    "\"Laparoscopic sleeve gastrectomy (LSG) was initially performed as the first stage of biliopancreatic diversion with duodenal switch for the treatment of super-obese or high-risk obese patients but is now most commonly performed as a standalone operation. The aim of this prospective study was to investigate outcomes after LSG according to resected stomach volume.\nBetween May 2011 and April 2013, LSG was performed in 102 consecutive patients undergoing bariatric surgery. Two patients were excluded, and data from the remaining 100 patients were analyzed in this study. Patients were divided into three groups according to the following resected stomach volume: 700-1,200 mL (group A, n\u2009=\u200921), 1,200-1,700 mL (group B, n\u2009=\u200962), and>1,700 mL (group C, n\u2009=\u200917). Mean values were compared among the groups by analysis of variance.\nThe mean percentage excess body weight loss (%EBWL) at 3, 6, 12, and 24 months after surgery was 37.68\u2009\u00b1\u200910.97, 50.97\u2009\u00b1\u200913.59, 62.35\u2009\u00b1\u200911.31, and 67.59\u2009\u00b1\u20099.02 %, respectively. There were no significant differences in mean %EBWL among the three groups. Resected stomach volume was greater in patients with higher preoperative body mass index and was positively associated with resected stomach weight.\"\nQuestion:\n\"Is resected stomach volume related to weight loss after laparoscopic sleeve gastrectomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18435678": {
                "source": [
                    "\"Kell haemolytic disease in pregnancies has been suggested to be associated with decreased fetal platelet counts. The aim of this study was to evaluate the incidence and clinical significance of fetal thrombocytopenia in pregnancies complicated by Kell alloimmunization.\nIn this retrospective cohort study, fetal platelet counts were performed in 42 pregnancies with severe Kell alloimmunization prior to the first intrauterine blood transfusion. Platelet counts from 318 first intrauterine transfusions in RhD alloimmunized pregnancies were used as controls.\nFetal thrombocytopenia (platelet count<150 x 10(9)/l) was found in 4/42 (10%) in the Kell group and in 84/318 (26%) in the RhD group. None of the fetuses in the Kell alloimmunized pregnancies, including 15 with severe hydrops, had a clinically significant thrombocytopenia defined as a platelet count<50 x 10(9)/l. In the RhD alloimmunized pregnancies, 2/230 (1%) of the non-hydropic fetuses and 7/30 (23%) of the severely hydropic fetuses had a clinically significant thrombocytopenia.\"\nQuestion:\n\"Kell alloimmunization in pregnancy: associated with fetal thrombocytopenia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23621776": {
                "source": [
                    "\"Unintended pregnancy has been significantly associated with subsequent female sterilization. Whether women who are sterilized after experiencing an unintended pregnancy are less likely to express desire for sterilization reversal is unknown.\nThis study used national, cross-sectional data collected by the 2006-2010 National Survey of Family Growth. The study sample included women ages 15-44 who were surgically sterile from a tubal sterilization at the time of interview. Multivariable logistic regression was used to examine the relationship between a history of unintended pregnancy and desire for sterilization reversal while controlling for potential confounders.\nIn this nationally representative sample of 1,418 women who were sterile from a tubal sterilization, 78% had a history of at least one unintended pregnancy and 28% expressed a desire to have their sterilization reversed. In unadjusted analysis, having a prior unintended pregnancy was associated with higher odds of expressing desire for sterilization reversal (odds ratio [OR]: 1.80; 95% confidence interval [CI]: 1.15-2.79). In adjusted analysis controlling for sociodemographic factors, unintended pregnancy was no longer significantly associated with desire for reversal (OR: 1.46; 95% CI: 0.91-2.34).\"\nQuestion:\n\"Does a history of unintended pregnancy lessen the likelihood of desire for sterilization reversal?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18284441": {
                "source": [
                    "\"Paraffin-embedded tissues in Cukurova University Faculty of Medicine Department of Pathology between January 2002 and February 2006 were searched restrospectively to investigate this issue. We performed immunohistochemistry on biopsies of 125 patients with HBV infection, grouped as: mild, moderate and severe hepatitis, cirrhosis and HCC, 25 patients in each of them, using anti c-kit monoclonal antibody. The severity of parenchymal inflammation and of interface hepatitis was semiquantitatively graded on a haematoxylin and eosin stained paraffin sections. Additionally, 50 more HCC, formed on HBV basis, were studied to determine the prevalence of c-kit overexpression.\nIn cirrhotic liver, lower intensity of staining and rarely c-kit positivity were present. The greatest number of the c-kit positivity and higher intensity of staining was found in the livers of patients with severe hepatitis and HCC. In chronic hepatitis B infection, the staining intensity was parallel with the grade and stage of the disease. In the areas where fibrosis was seen, c-kit positivity was rare or absent. In the HCC specimens, c-kit positivity appeared both inside and around the cancerous nodes. C-kit expression was observed in 62 of 75 HCC tissue specimens (82%) (p<0.001).\"\nQuestion:\n\"Expression of c-kit protooncogen in hepatitis B virus-induced chronic hepatitis, cirrhosis and hepatocellular carcinoma: has it a diagnostic role?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15954832": {
                "source": [
                    "\"Laparoscopy has rapidly emerged as the preferred surgical approach to a number of different diseases because it allows for a correct diagnosis and proper treatment. It seems to be moving toward the use of mini-instruments (5 mm or less in diameter). The aim of this paper is to illustrate retrospectively the results of an initial experience of minilaparoscopic transabdominal preperitoneal (miniTAPP) repair of groin hernia defects performed at two institutions.\nBetween February 2000 and December 2003, a total of 303 patients (mean age, 45 years) underwent a miniTAPP procedure: 213 patients (70.2%) were operated on bilaterally and 90 (28.7%) for a unilateral defect, with a total of 516 hernia defects repaired. The primary endpoint was the feasibility rate for miniTAPP. The secondary endpoint was the incidence of mini-TAPP-related complications.\nNo conversions to laparoscopy or an anterior open approach were required. There were no major complications, while minor complications ranged as high as 0.3%.\"\nQuestion:\n\"Is minilaparoscopic inguinal hernia repair feasible?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21198823": {
                "source": [
                    "\"Elevation of cardiac troponin (cTn) is considered specific for myocardial damage. Elevated cTn and echocardiogrpahic documentation of wall motion abnormalities (WMAs) that were recorded after extreme physical effort raise the question whether dobutamine stress echo (DSE), can also induce elevation of troponin.\nwe prospective enrolled stable patients (age>18 years) referred to DSE. The exam was performed under standardized conditions. Blood samples for cTnI were obtained at baseline and 18-24 hours after the test. We aimed to compare between the clinical and echocardiographic features of patients with elevated cTnI and those without cTnI elevations.\nFifty-seven consecutive patients were included. The average age was 64.4 \u00b1 10.7, 73% of the patients were males, and nearly half of the patients were known to have ischemic heart disease. Two of the patients were excluded due to technical difficulty. No signs of ischemia were recorded in 25 (45.4%). Among the patients with established ischemia on DSE, 12 (22%) had mild ischemia, 13 (23.6%) had moderate and 5 (9%) had severe ischemia. Angiography was performed in 13 (26%) of the patients, of which 7 had PCI and one was referred to bypass surgery. None of the patients had elevated cTnI 18-24 hours after the DSE.\"\nQuestion:\n\"Can dobutamine stress echocardiography induce cardiac troponin elevation?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23449952": {
                "source": [
                    "\"To investigate the diagnostic value of a half dose compared with a full dose of gadobenate dimeglumine in the assessment of synovitis or tenosynovitis in the wrist and finger joints in patients with early rheumatoid arthritis (RA) and a disease activity score greater than 3.2.\nWith institutional review board approval and informed consent, 57 patients with early RA underwent 3-T magnetic resonance (MR) imaging with two different doses of contrast media. The contrast enhancement was measured in inflamed synovial tissue at half dose (0.05 mmol per kilogram of body weight) and at full dose (0.1 mmol/kg) by using T1-weighted sequences with fat saturation. The differences and the correlation of signal intensities (SIs) at half- and full-dose sequences were compared by using the paired t test and Pearson correlations. Image quality, Rheumatoid Arthritis MRI Score (RAMRIS), and tenosynovitis score on half- and full-dose images were compared by two observers using the Wilcoxon test. Interrater agreement was assessed by using \u03ba statistics.\nA significant difference in SI was found between half-dose and full-dose gadobenate dimeglumine-enhanced synovial tissue (mean: 914.35 \u00b1 251.1 vs 1022 \u00b1 244.5, P<.001). Because the SI showed high correlation between the ratio at half dose and full dose (r = 0.875), the formula, ratio of synovial enhancement to saline syringe at full dose = 0.337 + 1.070 \u00d7 ratio of synovial enhancement to saline syringe at half dose, can be used to convert the normalized value of half dose to full dose. However, no difference in RAMRIS (score 0 in 490 of 1026 joints; score 1 in 344; score 2 in 158; and score 3 in 34) or tenosynovitis scores in grading synovitis or tenosynovitis in image quality and in assessment of synovial enhancement was detected between half-dose and full-dose images (P = 1).\"\nQuestion:\n\"Contrast-enhanced MR imaging of hand and finger joints in patients with early rheumatoid arthritis: do we really need a full dose of gadobenate dimeglumine for assessing synovial enhancement at 3 T?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9483814": {
                "source": [
                    "\"Uterus-specific synthetic Prostaglandin analogues (gemeprost, sulproston etc.) have been widely employed for termination of pregnancy in the second trimester. Since paracervical anaesthesia may be useful during this procedure, we investigated in this prospective randomised study its impact on the clinical course of abortion and pain especially in the late first and second stage of labour.\n20 women scheduled for elective abortion (fetal reasons) between the 16th and 23rd week of gestation were to be given 1 mg gemeprost vaginally every 6 hours. They were allocated at random: 10 women received only Pethidin intravenously and Butylscopolamine rectally, another 10 women were additionally treated by paracervical anaesthesia (2 x 10 ml 0.5% Bupivacain solution) at a cervical dilatation of 2-3 cm.\nA median of 3 gemeprost applications were administered in both groups. In the group without paracervical anaesthesia the median induction to abortion interval was 20 hours (range: 8-44 hours), 13 hours (range: 8-36 hours, NS) resulting for the paracervical anaesthesia group. The intervals from the last application of prostaglandin until abortion and from 3 cm cervical dilatation to abortion were slightly, but not significantly shorter in the paracervical anaesthesia group. The requirement of Butylscopolamine was higher in the latter group (p<0.05). The requirement of Pethidin and the intensity of pain (measured by pain scale according to Huskisson) especially in the late first stage of labour were not statistically different between both groups. Side effects of paracervical anaesthesia did not occur.\"\nQuestion:\n\"Does para-cervical block offer additional advantages in abortion induction with gemeprost in the 2nd trimester?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22532370": {
                "source": [
                    "\"To provide insight into the factors by which obesity in itself may directly lead to early arterial damage, we aimed to determine early sonographic markers of obesity-related vascular dysfunction in young obese males.\nThirty-five young obese males and 23 age-matched healthy male volunteers were recruited into the study. Common carotid artery pulsatility index and resistance index were calculated from blood flow velocities curves obtained by pulsed Doppler ultrasonography.\nThe mean pulsatility index, resistance index, body mass index, waist circumference, systolic and diastolic blood pressure, homeostasis model assessment for insulin resistance, plasma fasting glucose, insulin, C-peptide, triglycerides, low-density lipoprotein cholesterol, and high-sensitivity C-reactive protein were statistically higher in obese subjects than in healthy controls.\"\nQuestion:\n\"Are increased carotid artery pulsatility and resistance indexes early signs of vascular abnormalities in young obese males?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22497340": {
                "source": [
                    "\"To clarify whether horizontal canal ocular reflex is influenced by otolith organs input.\nThe subjects were seven healthy humans. The right ear was stimulated using ice-water. Each subject was kept in a left-ear-down position for 20 s and then repositioned to a prone position, a right-ear-down position and a supine position with 20 s intervals. Nystagmus was analysed using three-dimensional video-oculography.\nEye movements in the supine position and the prone position were not in a symmetric fashion. Nystagmus in the left-ear-down position and the right-ear-down position were not symmetric either. These phenomena indicate that the axis of the eyeball rotation was affected by the shift of the direction of gravity exerted on the head.\"\nQuestion:\n\"Is horizontal semicircular canal ocular reflex influenced by otolith organs input?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22440363": {
                "source": [
                    "\"This was a study to compare the results of mitral valve (MV) repair and MV replacement for the treatment of functional mitral regurgitation (MR) in advanced dilated and ischemic cardiomyopathy (DCM).\nOne-hundred and thirty-two patients with severe functional MR and systolic dysfunction (mean ejection fraction 0.32 \u00b1 0.078) underwent mitral surgery in the same time frame. The decision to replace rather than repair the MV was taken when 1 or more echocardiographic predictors of repair failure were identified at the preoperative echocardiogram. Eighty-five patients (64.4%) received MV repair and 47 patients (35.6%) received MV replacement. Preoperative characteristics were comparable between the 2 groups. Only ejection fraction was significantly lower in the MV repair group (0.308 \u00b1 0.077 vs 0.336 \u00b1 0.076, p = 0.04).\nHospital mortality was 2.3% for MV repair and 12.5% for MV replacement (p = 0.03). Actuarial survival at 2.5 years was 92 \u00b1 3.2% for MV repair and 73 \u00b1 7.9% for MV replacement (p = 0.02). At a mean follow-up of 2.3 years (median, 1.6 years), in the MV repair group LVEF significantly increased (from 0.308 \u00b1 0.077 to 0.382 \u00b1 0.095, p<0.0001) and LV dimensions significantly decreased (p = 0.0001). On the other hand, in the MV replacement group LVEF did not significantly change (from 0.336 \u00b1 0.076 to 0.31 \u00b1 0.11, p = 0.56) and the reduction of LV dimensions was not significant. Mitral valve replacement was identified as the only predictor of hospital (odds ratio, 6; 95% confidence interval, 1.1 to 31; p = 0.03) and overall mortality (hazard ratio, 3.1; 95% confidence interval, 1.1 to 8.9; p = 0.02).\"\nQuestion:\n\"Mitral replacement or repair for functional mitral regurgitation in dilated and ischemic cardiomyopathy: is it really the same?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16249670": {
                "source": [
                    "\"The placement of the superficial cervical plexus block has been the subject of controversy. Although the investing cervical fascia has been considered as an impenetrable barrier, clinically, the placement of the block deep or superficial to the fascia provides the same effective anesthesia. The underlying mechanism is unclear. The aim of this study was to investigate the three-dimensional organization of connective tissues in the anterior region of the neck.\nUsing a combination of dissection, E12 sheet plastination, and confocal microscopy, fascial structures in the anterior cervical triangle were examined in 10 adult human cadavers.\nIn the upper cervical region, the fascia of strap muscles in the middle and the fasciae of the submandibular glands on both sides formed a dumbbell-like fascia sheet that had free lateral margins and did not continue with the sternocleidomastoid fascia. In the lower cervical region, no single connective tissue sheet extended directly between the sternocleidomastoid muscles. The fascial structure deep to platysma in the anterior cervical triangle comprised the strap fascia.\"\nQuestion:\n\"Does the investing layer of the deep cervical fascia exist?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27989969": {
                "source": [
                    "\"The relationship between the use of an endoscope during ventriculoperitoneal shunt (VPS) procedures and infection remains poorly defined. In this study, we sought to analyze whether the simultaneous use of an endoscope could in fact increase the infection rate associated with VPS procedures.\nThis study included 438 VPS procedures, 49 in which an endoscope was used (11.2%) and 389 in which an endoscope was not used (88.8%). The infection rates in these 2 main groups were calculated and compared. Subsequently, 4 new groups were created, composed of patients with a shunt inserted for the first time (groups 1A and 1B) and patients with a shunt reviewed or inserted for a second time (groups 2A and 2B). Groups 1A and 2A comprised patients in whom an endoscope was used simultaneously with VPS surgery, and groups 1B and 2B comprised patients in whom an endoscope was not used. These groups were compared to determine the infection rate.\nThe overall infection rate was 18.5%, including 22.4% in the groups in which an endoscope was used and 18% in those in which an endoscope was not used (P\u00a0=\u00a00.449). Groups 1A and 1B and groups 2A and 2B were matched for possible intervening risk factors. The infection rate was 28.6% in group 1A and 16.2% in group 1B (P\u00a0= 0.27), and 20% in group 2A and 19.8% in group 2B (P\u00a0= 0.977).\"\nQuestion:\n\"Does the Simultaneous Use of a Neuroendoscope Influence the Incidence of Ventriculoperitoneal Shunt Infection?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24495711": {
                "source": [
                    "\"More than 50,000 new HIV infections occur annually in the United States. Injection drug users represent twelve percent of incident HIV infections each year. Pharmacy sales of over-the-counter (OTC) syringes have helped prevent HIV transmission among injection drug users in many states throughout the United States. However, concerns exist among some law enforcement officials, policymakers, pharmacists, and community members about potential links between OTC syringe sales and crime.\nWe used a geographic information system and novel spatial and longitudinal analyses to determine whether implementation of pharmacy-based OTC syringe sales were associated with reported crime between January 2006 and December 2008 in Los Angeles Police Department Reporting Districts. We assessed reported crime pre- and post-OTC syringe sales initiation as well as longitudinal associations between crime and OTC syringe-selling pharmacies.\nBy December 2008, 9.3% (94/1010) of Los Angeles Police Department Reporting Districts had at least one OTC syringe-selling pharmacy. Overall reported crime counts and reported crime rates decreased between 2006 and 2008 in all 1010 Reporting Districts. Using generalized estimating equations and adjusting for potential confounders, reported crime rates were negatively associated with OTC syringe sales (adjusted rate ratio: 0.89; 95% confidence interval: 0.81, 0.99).\"\nQuestion:\n\"Is crime associated with over-the-counter pharmacy syringe sales?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11833948": {
                "source": [
                    "\"To detemine the relationship between delay in transfer to rehabilitation wards and outcome for patients aged over 75 years with fracture of the proximal femur.\nAn observational study in a district general hospital of all patients admitted to hospital aged over 75 years with fracture of the proximal femur over 3 1/2 years. Outcome data collected included the number of patients discharged back to their usual residence and total hospital length of stay related to age, gender, usual residence and delay in transfer to a rehabilitation ward.\n58% of 455 patients were transferred to a rehabilitation ward. For those patients who were transferred to a rehabilitation ward only age predicted discharge to a more dependent residence. The relative risk for discharge to a more dependent residence for people aged over 85 years compared to younger people was 1.47 (95% CI 1.15-1.88). Delay in transfer to rehabilitation was associated with a longer total hospital length of stay of 0.64 (95% CI 0.23-1.05) days per day of delay in transfer.\"\nQuestion:\n\"Does a delay in transfer to a rehabilitation unit for older people affect outcome after fracture of the proximal femur?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16046584": {
                "source": [
                    "\"Irregular bleeding affects many users of combined menopausal hormone therapy (HT) and commonly leads to invasive and expensive investigations to exclude underlying malignancy. In most cases no abnormality is found.\nThe main objective of this study was to explore the role of uterine natural killer (uNK) cells and their regulatory cytokine IL-15 in irregular bleeding in HT users.\nThis was a prospective observational study conducted between 2002 and 2004.\nThe study was conducted in a tertiary referral menopause clinic at King Edward Memorial Hospital, Western Australia.\nPatients included 117 postmenopausal women taking combined HT.\nOutpatient endometrial biopsies were taken during and outside bleeding episodes.\nThe relationship between endometrial uNK cells (CD56+) and bleeding patterns was measured. We also addressed the impact of HT exposure on uNK cell populations, the relationship between endometrial IL-15 expression and uNK cell populations, and killer Ig like receptor genotype in subjects with irregular bleeding.\nEndometrial CD56+ uNK cells were significantly increased in biopsies obtained during bleeding episodes (P<0.001), compared with HT users with no bleeding. The highest level of IL-15 expression was also seen in biopsies taken during bleeding. No clear relationship between killer Ig like receptor genotype and bleeding on HT was observed.\"\nQuestion:\n\"Menopausal hormone therapy and irregular endometrial bleeding: a potential role for uterine natural killer cells?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22849512": {
                "source": [
                    "\"The aim of this study is to explore whether availability of sports facilities, parks, and neighbourhood social capital (NSC) and their interaction are associated with leisure time sports participation among Dutch adolescents.\nCross-sectional analyses were conducted on complete data from the last wave of the YouRAction evaluation trial. Adolescents (n\u2009=\u2009852) completed a questionnaire asking for sports participation, perceived NSC and demographics. Ecometric methods were used to aggregate perceived NSC to zip code level. Availability of sports facilities and parks was assessed by means of geographic information systems within the zip-code area and within a 1600 meter buffer. Multilevel logistic regression analyses, with neighborhood and individual as levels, were conducted to examine associations between physical and social environmental factors and leisure time sports participation. Simple slopes analysis was conducted to decompose interaction effects.\nNSC was significantly associated with sports participation (OR: 3.51 (95%CI: 1.18;10.41)) after adjustment for potential confounders. Availability of sports facilities and availability of parks were not associated with sports participation. A significant interaction between NSC and density of parks within the neighbourhood area (OR: 1.22 (90%CI: 1.01;1.34)) was found. Decomposition of the interaction term showed that adolescents were most likely to engage in leisure time sports when both availability of parks and NSC were highest.\"\nQuestion:\n\"Are neighbourhood social capital and availability of sports facilities related to sports participation among Dutch adolescents?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10548670": {
                "source": [
                    "\"The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect.\nWe examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume.\nThe volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.\"\nQuestion:\n\"Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24098953": {
                "source": [
                    "\"All VLBW infants from January 2008 to December 2012 with positive blood culture beyond 72 hours of life were enrolled in a retrospective cohort study. Newborns born after June 2010 were treated with IgM-eIVIG, 250 mg/kg/day iv for three days in addition to standard antibiotic regimen and compared to an historical cohort born before June 2010, receiving antimicrobial regimen alone. Short-term mortality (i.e. death within 7 and 21 days from treatment) was the primary outcome. Secondary outcomes were: total mortality, intraventricular hemorrhage, necrotizing enterocolitis, periventricular leukomalacia, bronchopulmonary dysplasia at discharge.\n79 neonates (40 cases) were enrolled. No difference in birth weight, gestational age or SNAP II score (disease severity score) were found. Significantly reduced short-term mortality was found in treated infants (22% vs 46%; p = 0.005) considering all microbial aetiologies and the subgroup affected by Candida spp. Secondary outcomes were not different between groups.\"\nQuestion:\n\"Are IgM-enriched immunoglobulins an effective adjuvant in septic VLBW infants?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28177278": {
                "source": [
                    "\"Polyarteritis nodosa (PAN) is a systemic vasculitis involving mainly medium-sized arteries and, rarely, small-sized arteries. The diagnosis is principally based on clinical exams, biopsy of an affected organ, and/or arteriography of renal or mesenteric arteries. Once diagnosed, immunosuppressive agents, such as glucocorticoids and cyclophosphamide, are generally introduced as soon as possible. Whether spontaneous remission of PAN occurs is therefore largely unknown.\nWe describe the case of a 51-year-old woman who presented with a 4-day-history of intense pain in her left flank, hypertension, fever, microscopic hematuria, and acute renal failure. Contrast-enhanced renal ultrasound strongly suggested bilateral renal infarction. Medical history and an extensive workup allowed to exclude systemic embolism, recreational drug abuse, cardiac arrhythmias, and thrombophilia. A possible diagnosis of PAN was considered; however, within 2 weeks of admission, spontaneous remission of her clinical and biological symptoms occurred without the use of any immunosuppressive treatment. Finally, 3 months later, renal arteriography confirmed the diagnosis of PAN. The patient remains free of symptoms 1 year after initial presentation.\"\nQuestion:\n\"Does spontaneous remission occur in polyarteritis nodosa?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12690589": {
                "source": [
                    "\"To compare the myoelectric onset of muscle fatigue in physically active trained young skiers with respect to elderly skiers and to test whether continuous training can counteract the selective loss of type II muscle fibers usually observed with aging.\nAn observational, cross-sectional study of the myoelectric onset of muscle fatigue in the left tibialis anterior muscles.\nSurface electromyography recorded with portable devices at a downhill ski rescue lodge in the Italian Alps.\nFifty-four physically trained, active skiers (43 men, 11 women; age range, 24-85y).\nQuestionnaire on physical activity and 2 sustained isometric voluntary contractions at 20% and 2 at 80% of the maximal voluntary contraction level.\nIsometric contractions and mean and median spectral frequencies calculated to monitor the myoelectric manifestations of muscle fatigue.\nFatigue indices did not differ significantly between younger and older subjects and, thus, did not show a correlation between myoelectric manifestations of muscle fatigue and age in physically active subjects.\"\nQuestion:\n\"Can continuous physical training counteract aging effect on myoelectric fatigue?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18388848": {
                "source": [
                    "\"This study examines whether having a regular clinician for preventive care is associated with quality of care for young children, as measured by interpersonal quality ratings and content of anticipatory guidance.\nThe National Survey of Early Childhood Health (NSECH), a nationally representative parent survey of health care quality for 2068 young US children fielded by the National Center for Health Statistics (NCHS).\nBivariate and multivariate analyses evaluate associations between having a regular clinician for well child care and interpersonal quality, the content of anticipatory guidance, and timely access to care.\nIn bivariate analysis, parents of children with a regular clinician for preventive care reported slightly higher interpersonal quality (69 vs. 65 on a 0-100 scale, P = 0.01). Content of anticipatory guidance received was slightly greater for children with a regular clinician (82 vs. 80 on a 0-100 scale, P = 0.03). In bivariate analysis, a regular clinician was associated with interpersonal quality only among African American and Hispanic children. In multivariate analyses, controlling for factors that could independently influence self-reports of experiences with care, interpersonal quality but not anticipatory guidance content was higher for children with a regular clinician.\"\nQuestion:\n\"Does having a regular primary care clinician improve quality of preventive care for young children?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14697414": {
                "source": [
                    "\"To analyze, retrospectively, the patterns and behavior of metastatic lesions in prostate cancer patients treated with external beam radiotherapy and to investigate whether patients with<or =5 lesions had an improved outcome relative to patients with>5 lesions.\nThe treatment and outcome of 369 eligible patients with Stage T1-T3aN0-NXM0 prostate cancer were analyzed during a minimal 10-year follow-up period. All patients were treated with curative intent to a mean dose of 65 Gy. The full history of any metastatic disease was documented for each subject, including the initial site of involvement, any progression over time, and patient survival.\nThe overall survival rate for the 369 patients was 75% at 5 years and 45% at 10 years. The overall survival rate of patients who never developed metastases was 90% and 81% at 5 and 10 years, respectively. However, among the 74 patients (20%) who developed metastases, the survival rate at both 5 and 10 years was significantly reduced (p<0.0001). The overall survival rate for patients who developed bone metastases was 58% and 27% at 5 and 10 years, respectively, and patients with bone metastases to the pelvis fared worse compared with those with vertebral metastases. With regard to the metastatic number, patients with<or =5 metastatic lesions had superior survival rates relative to those with>5 lesions (73% and 36% at 5 and 10 years vs. 45% and 18% at 5 and 10 years, respectively; p = 0.02). In addition, both the metastasis-free survival rate and the interval measured from the date of the initial diagnosis of prostate cancer to the development of bone metastasis were statistically superior for patients with<or =5 lesions compared with patients with>5 lesions (p = 0.01 and 0.02, respectively). However, the survival rate and the interval from the date of diagnosis of bone metastasis to the time of death for patients in both groups were not significantly different, statistically (p = 0.17 and 0.27, respectively).\"\nQuestion:\n\"Is there a favorable subset of patients with prostate cancer who develop oligometastases?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17276801": {
                "source": [
                    "\"The purpose of this study was to evaluate the value of elevated cardiac troponin I (cTnI) for prediction of complicated clinical course and in-hospital mortality in patients with confirmed acute pulmonary embolism (PE).\nThis study was a retrospective chart review of patients diagnosed as having PE, in whom cTnI testing was obtained at emergency department (ED) presentation between January 2002 and April 2006. Clinical characteristics; echocardiographic right ventricular dysfunction; inhospital mortality; and adverse clinical events including need for inotropic support, mechanical ventilation, and thrombolysis were compared in patients with elevated cTnI levels vs patients with normal cTnI levels. One hundred sixteen patients with PE were identified, and 77 of them (66%) were included in the study. Thirty-three patients (42%) had elevated cTnI levels. Elevated cTnI levels were associated with inhospital mortality (P = .02), complicated clinical course (P<.001), and right ventricular dysfunction (P<.001). In patients with elevated cTnI levels, inhospital mortality (odds ratio [OR], 3.31; 95% confidence interval [CI], 1.82-9.29), hypotension (OR, 7.37; 95% CI, 2.31-23.28), thrombolysis (OR, 5.71; 95% CI, 1.63-19.92), need for mechanical ventilation (OR, 5.00; 95% CI, 1.42-17.57), and need for inotropic support (OR, 3.02; 95% CI, 1.03-8.85) were more prevalent. The patients with elevated cTnI levels had more serious vital parameters (systolic blood pressure, pulse, and oxygen saturation) at ED presentation.\"\nQuestion:\n\"Can elevated troponin I levels predict complicated clinical course and inhospital mortality in patients with acute pulmonary embolism?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19608436": {
                "source": [
                    "\"To investigate the contribution of chemical shift magnetic resonance imaging for assessment of the margins of solid breast masses by benefiting from India ink artifact.\nEighty-eight masses in 64 patients were evaluated in T1- and T2-weighted images, dynamic contrast and chemical shift studies according to Breast Imaging Reporting and Data System magnetic resonance lexicon. Subtraction images were automatically obtained by chemical shift imaging and dynamic studies. Each sequence was scored using a scale of 1 to 5 according to its ability to demonstrate margins separate from surrounding parenchyma. Breast parenchyma was evaluated as fatty and dense. The results were compared with the histopathologic results.\nTwenty-eight (31.8%) of the lesions were localized in fatty breast, and the remaining 60 (68.2%) lesions were localized in dense breast. There were 34 (38.6%) benign and 54 (61.4%) malignant masses. In fatty breast, chemical shift subtraction and T1-weighted images were valuable both for the demonstration and differentiation of benign lesions (P<.05). None of the sequence was valuable for both the demonstration and differentiation of malignant lesions in fatty breasts (P>.05). In dense breasts, chemical shift subtraction and dynamic contrast subtraction images were valuable for both the demonstration and differentiation of benign and malignant lesions. Additional to these sequences, T2-weighted images was also valuable for benign lesions (P<.05).\"\nQuestion:\n\"Chemical shift MRI: is there any contribution to morphologic evaluation of solid breast masses?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24996865": {
                "source": [
                    "\"Reconstructing the natural joint line in knee revision surgery improves clinical and functional outcome but may be challenging when both cartilage and bone were removed during previous operations. Assessing joint lines (JLs) by means of bony landmarks is inadvisable because of large variations in human anatomy. Because of the inherent symmetry of the human body, we hypothesised that JLs may be directly assessed by measuring the distances from the bony landmarks to the JL of the contralateral knee by means of radiographic images.\nUsing scaled weight-bearing radiographs in anteroposterior view of both knees, two independent observers measured the distances from the fibular head, the medial and lateral epicondyle, and the adductor tubercle to the JL. A two-sided p value of \u22640.05 was considered statistically significant.\nTwo hundred knees of 100 patients (50 men and 50 women) were examined. For the fibular head, the mean difference between the treated and the control knee was 0.0 mm with narrow confidence limits ranging from -1.1 to 1.1.\"\nQuestion:\n\"Assessing joint line positions by means of the contralateral knee: a new approach for planning knee revision surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24630333": {
                "source": [
                    "\"As with some procedures, trauma fellowship training and greater surgeon experience may result in better outcomes following intramedullary nailing (IMN) of diaphyseal femur fractures. However, surgeons with such training and experience may not always be available to all patients. The purpose of this study is to determine whether trauma training affects the post-operative difference in femoral version (DFV) following IMN.\nBetween 2000 and 2009, 417 consecutive patients with diaphyseal femur fractures (AO/OTA 32A-C) were treated via IMN. Inclusion criteria for this study included complete baseline and demographic documentation as well as pre-operative films for fracture classification and post-operative CT scanogram (per institutional protocol) for version and length measurement of both the nailed and uninjured femurs. Exclusion criteria included bilateral injuries, multiple ipsilateral lower extremity fractures, previous injury, and previous deformity. Of the initial 417 subjects, 355 patients met our inclusion criteria. Other data included in our analysis were age, sex, injury mechanism, open vs. closed fracture, daytime vs. nighttime surgery, mechanism of injury, and AO and Winquist classifications. Post-operative femoral version of both lower extremities was measured on CT scanogram by an orthopaedic trauma fellowship trained surgeon. Standard univariate and multivariate analyses were performed to determine statistically significant risk factors for malrotation between the two cohorts.\nOverall, 80.3% (288/355) of all fractures were fixed by trauma-trained surgeons. The mean post-operative DFV was 8.7\u00b0 in these patients, compared to 10.7\u00b0 in those treated by surgeons of other subspecialties. This difference was not statistically significant when accounting for other factors in a multivariate model (p>0.05). The same statistical trend was true when analyzing outcomes of only the more severe Winquist type III and IV fractures. Additionally, surgeon experience was not significantly predictive of post-operative version for either trauma or non-trauma surgeons (p>0.05 for both).\"\nQuestion:\n\"Comparing femoral version after intramedullary nailing performed by trauma-trained and non-trauma trained surgeons: is there a difference?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26820719": {
                "source": [
                    "\"The incidence of colorectal cancer in young patients is increasing. It remains unclear if the disease has unique features in this age group.\nThis was a single-center, retrospective cohort study which included patients diagnosed with colorectal cancer at age \u226440\u00a0years in 1997-2013 matched 1:2 by year of diagnosis with consecutive colorectal cancer patients diagnosed at age>50\u00a0years during the same period. Patients aged 41-50\u00a0years were not included in the study, to accentuate potential age-related differences. Clinicopathological characteristics, treatment, and outcome were compared between groups.\nThe cohort included 330 patients, followed for a median time of 65.9\u00a0months (range 4.7-211). Several significant differences were noted. The younger group had a different ethnic composition. They had higher rates of family history of colorectal cancer (p\u00a0=\u00a00.003), hereditary colorectal cancer syndromes (p\u00a0<\u00a00.0001), and inflammatory bowel disease (p\u00a0=\u00a00.007), and a lower rate of polyps (p\u00a0<\u00a00.0001). They were more likely to present with stage III or IV disease (p\u00a0=\u00a00.001), angiolymphatic invasion, signet cell ring adenocarcinoma, and rectal tumors (p\u00a0=\u00a00.02). Younger patients more frequently received treatment. Young patients had a worse estimated 5-year disease-free survival rate (57.6\u00a0 vs. 70\u00a0%, p\u00a0=\u00a00.039), but this did not retain significance when analyzed by stage (p\u00a0=\u00a00.092). Estimated 5-year overall survival rates were 59.1 and 62.1\u00a0% in the younger and the control group, respectively (p\u00a0=\u00a00.565).\"\nQuestion:\n\"Colorectal cancer in young patients: is it a distinct clinical entity?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21669959": {
                "source": [
                    "\"Secondhand smoke exposure (SHSe) threatens fragile infants discharged from a neonatal intensive care unit (NICU). Smoking practices were examined in families with a high respiratory risk infant (born at very low birth weight; ventilated>12 hr) in a Houston, Texas, NICU. Socioeconomic status, race, and mental health status were hypothesized to be related to SHSe and household smoking bans.\nData were collected as part of The Baby's Breath Project, a hospital-based SHSe intervention trial targeting parents with a high-risk infant in the NICU who reported a smoker in the household (N = 99). Measures of sociodemographics, smoking, home and car smoking bans, and depression were collected.\nOverall, 26% of all families with a high-risk infant in the NICU reported a household smoker. Almost half of the families with a smoker reported an annual income of less than $25,000. 46.2% of families reported having a total smoking ban in place in both their homes and cars. Only 27.8% families earning less than $25,000 reported having a total smoking ban in place relative to almost 60% of families earning more (p<.01). African American and Caucasian families were less likely to have a smoking ban compared with Hispanics (p<.05). Mothers who reported no smoking ban were more depressed than those who had a household smoking ban (p<.02).\"\nQuestion:\n\"Secondhand smoke risk in infants discharged from an NICU: potential for significant health disparities?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14976655": {
                "source": [
                    "\"The aim of this study was to evaluate poststapedectomy-delayed facial palsy etiopathogenesis, risk factors, evolution, and prevention.\nSeven hundred six stapedectomies performed in 580 patients were reviewed. In all patients who developed delayed facial palsy, the dates of onset and subside of facial palsy, the anatomic and pathologic predisposing factors, and a possible history for recurrent labial herpetic lesions were considered. The House-Brackmann (H-B) grading system was used to evaluate the facial function. Virus-specific immunoglobulin (Ig) G and IgM antibodies against herpes simplex virus type 1 (HSV-1) were determined by enzyme-linked immunosorbent assay (ELISA) 3 weeks after the onset of the paralysis. The results were compared with a control group without a history of recurrent herpes labialis.\nPoststapedectomy facial palsy developed in 7 out of 706 procedures. All 7 patients referred a history of recurrent labial herpetic lesions. One patient showed a facial palsy H-B grade II, 2 a grade III, and 3 a grade IV. After acyclovir therapy, 6 subjects recovered completely, whereas 1 maintained an H-B grade II. An increased IgG antibody titer was found in 6 of the patients with delayed facial palsy and in 1 out of 7 controls. Mean IgG titer was 1:14,050 in the subjects with delayed facial palsy and 1:2,300 in controls (P<.001).\"\nQuestion:\n\"Delayed peripheral facial palsy in the stapes surgery: can it be prevented?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17329379": {
                "source": [
                    "\"To evaluate feasibility of the guidelines of the Groupe Francophone de R\u00e9animation et Urgence P\u00e9diatriques (French-speaking group of paediatric intensive and emergency care; GFRUP) for limitation of treatments in the paediatric intensive care unit (PICU).\nA 2-year prospective survey.\nA 12-bed PICU at the H\u00f4pital Jeanne de Flandre, Lille, France.\nWere included when limitation of treatments was expected.\nOf 967 children admitted, 55 were included with a 2-day median delay. They were younger than others (24 v 60 months), had a higher paediatric risk of mortality (PRISM) score (14 v 4), and a higher paediatric overall performance category (POPC) score at admission (2 v 1); all p<0.002. 34 (50% of total deaths) children died. A limitation decision was made without meeting for 7 children who died: 6 received do-not-resuscitate orders (DNROs) and 1 received withholding decision. Decision-making meetings were organised for 31 children, and the following decisions were made: 12 DNROs (6 deaths and 6 survivals), 4 withholding (1 death and 3 survivals), with 14 withdrawing (14 deaths) and 1 continuing treatment (survival). After limitation, 21 (31% of total deaths) children died and 10 survived (POPC score 4). 13 procedures were interrupted because of death and 11 because of clinical improvement (POPC score 4). Parents' opinions were obtained after 4 family conferences (for a total of 110 min), 3 days after inclusion. The first meeting was planned for 6 days after inclusion and held on the 7th day after inclusion; 80% of parents were immediately informed of the decision, which was implemented after half a day.\"\nQuestion:\n\"Are the GFRUP's recommendations for withholding or withdrawing treatments in critically ill children applicable?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26778755": {
                "source": [
                    "\"Although dose-volume parameters in image-guided brachytherapy have become a standard, the use of posterior-inferior border of the pubic symphysis (PIBS) points has been recently proposed in the reporting of vaginal doses. The aim was to evaluate their pertinence.\nNineteen patients who received image-guided brachytherapy after concurrent radiochemotherapy were included. Per treatment, CT scans were performed at Days 2 and 3, with reporting of the initial dwell positions and times. Doses delivered to the PIBS points were evaluated on each plan, considering that they were representative of one-third of the treatment. The movements of the applicator according to the PIBS point were analysed.\nMean prescribed doses at PIBS -2, PIBS, PIBS +2 were, respectively, 2.23 \u00b1 1.4, 6.39 \u00b1 6.6, and 31.85 \u00b1 36.06 Gy. Significant differences were observed between the 5 patients with vaginal involvement and the remaining 14 at the level of PIBS +2 and PIBS: +47.60 Gy and +7.46 Gy, respectively (p = 0.023 and 0.03). The variations between delivered and prescribed doses at PIBS points were not significant. However, at International commission on radiation units and measurements rectovaginal point, the delivered dose was decreased by 1.43 \u00b1 2.49 Gy from the planned dose (p = 0.019). The delivered doses at the four points were strongly correlated with the prescribed doses with R(2) ranging from 0.93 to 0.95. The movements of the applicator in regard of the PIBS point assessed with the Digital Imaging and Communications in Medicine coordinates were insignificant.\"\nQuestion:\n\"Vaginal dose assessment in image-guided brachytherapy for cervical cancer: Can we really rely on dose-point evaluation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8262881": {
                "source": [
                    "\"Although body dysmorphic disorder (BDD) is classified in DSM-III-R as a nonpsychotic somatoform disorder, controversy exists as to whether BDD can present with psychotic features. If it can, this raises the possibility that its DSM-III-R psychotic counterpart-delusional disorder, somatic type--may not be a separate disorder. The purpose of this study was to determine whether patients with nonpsychotic BDD (defined according to DSM-III-R criteria, i.e., with maintenance of some insight) were different from patients with psychotic BDD (those whose preoccupation was without insight and of delusional intensity).\nFifty consecutive patients meeting DSM-III-R criteria A and C for BDD were assessed with a semistructured interview and the Structured Clinical Interview for DSM-III-R (SCID). Family histories of psychiatric disorders were blindly assessed. The 24 patients with nonpsychotic BDD were compared with the 26 patients with psychotic BDD with respect to demographics, phenomenology, course of illness, associated features, comorbid psychiatric disorders, family history, and treatment response.\nPatients with psychotic BDD displayed a significantly higher rate of lifetime DSM-III-R psychotic disorder diagnoses than patients with nonpsychotic BDD. However, the two groups did not differ significantly on most other variables examined. For instance, both psychotic and nonpsychotic patients displayed significant morbidity; high comorbidity with mood, anxiety, and psychoactive substance use disorders; and apparent preferential response to serotonin reuptake inhibitors rather than to non-serotonin reuptake blocking antidepressants or antipsychotics.\"\nQuestion:\n\"Body dysmorphic disorder: does it have a psychotic subtype?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11799314": {
                "source": [
                    "\"Gallbladder carcinoma is characterized by delayed diagnosis, ineffective treatment and poor prognosis. Surgical resection has been thought to be the treatment of choice, while the role of radiotherapy as adjuvant or palliative treatment has not been fully clarified in the literature.\nWe present the case of a 45-year-old female, with unresectable gallbladder carcinoma, grade IV, histologically diagnosed during laparotomy. The patient was treated with palliative intent with percutaneous transhepatic biliary drainage. Furthermore, she received external radiotherapy by (60)Co, using a three-field technique (anterior-posterior and right lateral). The total dose was 3,000 cGy in 10 fractions, with 300 cGy per fraction, 5 days weekly.\nThe patient showed clinico-laboratory improvement and was discharged with a permanent percutaneous transhepatic endoprosthesis. During follow-up (10 and 12 months postirradiation), abdominal CTs showed no local extension of the tumor, while the patient had a good performance status. So far, 1 year after the diagnosis of gallbladder cancer she is still alive.\"\nQuestion:\n\"Is external palliative radiotherapy for gallbladder carcinoma effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23076787": {
                "source": [
                    "\"To explain China's cigarette pricing mechanism and the role of the Chinese State Tobacco Monopoly Administration (STMA) on cigarette pricing and taxation.\nPublished government tobacco tax documentation and statistics published by the Chinese STMA are used to analyse the interrelations among industry profits, taxes and retail price of cigarettes in China.\nThe 2009 excise tax increase on cigarettes in China has not translated into higher retail prices because the Chinese STMA used its policy authority to ensure that retail cigarette prices did not change. The government tax increase is being collected at both the producer and wholesale levels. As a result, the 2009 excise tax increase in China has resulted in higher tax revenue for the government and lower profits for the tobacco industry, with no increase in the retail price of cigarettes for consumers.\"\nQuestion:\n\"Can increases in the cigarette tax rate be linked to cigarette retail prices?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21190419": {
                "source": [
                    "\"This study was planned to evaluate whether increased nuchal translucency (NT) thickness in the first trimester of gestation can be related to onset of gestational diabetes mellitus (GDM) during pregnancy.\nFrom January 2006 to August 2008, a group of 678 singleton pregnancies who had developed GDM has been selected as a study group among a total of 3966 pregnant women who had undergone first trimester screening for aneuploidies at 11-14 weeks of gestation. A group of 420 single pregnant women with physiological pregnancy were enrolled as control group. Both fetal structural and karyotype's anomalies were excluded in the two groups. NT was mesured by a Fetal Medicine Foundation certificated operator; GDM was diagnosed at 24-28 weeks of gestation following Carpenter and Coustan criteria. In the analyses of continuos variables, study and control group were compared by Student's t-test and Anova test.\nThere was no significative difference (p = 0.585) between NT values in the study (mean = 1.56) and control group (mean = 1.54).\"\nQuestion:\n\"Does nuchal translucency thickness in the first trimester predict GDM onset during pregnancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27216167": {
                "source": [
                    "\"The aim of this study was to determine the proportion of patients who were referred to specialist care after reporting gynecological cancer alarm symptoms to their general practitioner. We sought to investigate whether contact with specialist care was associated with lifestyle factors or socioeconomic status.\nNationwide population-based prospective cohort study in Denmark, based on a random sample of 51 090 women aged 20 years or older from the general population. A web-based questionnaire regarding gynecological alarm symptoms and lifestyle was distributed to the invited individuals. Data about contact with specialist care were obtained from the National Patient Register and the National Health Insurance Service Registry, whereas information about socioeconomic status was collected from Statistics Denmark. Main outcome measures were percentages of patients having contact with specialist care and odds ratios (ORs) for associations between specialist care contact, lifestyle factors and socioeconomic status.\nThe study included 25 866 nonpregnant women; 2957 reported the onset of at least one gynecological cancer alarm symptom, and 683 of these (23.1%) reported symptoms to their general practitioner. The proportion of individuals having contact with specialist care ranged from 39.3% (pain during intercourse) to 47.8% (bleeding during intercourse). Individuals with higher educational level had significantly higher odds of contact with a specialist (OR 1.86, 95% CI 1.17-2.95).\"\nQuestion:\n\"Gynecological cancer alarm symptoms: is contact with specialist care associated with lifestyle and socioeconomic status?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23321509": {
                "source": [
                    "\"To evaluate surgical outcome and survival benefit after quaternary cytoreduction (QC) in epithelial ovarian cancer (EOC) relapse.\nWe systematically evaluated all consecutive patients undergoing QC in our institution over a 12-year period (October 2000-January 2012). All relevant surgical and clinical outcome parameters were systematically assessed.\nForty-nine EOC patients (median age: 57; range: 28-76) underwent QC; in a median of 16 months (range:2-142) after previous chemotherapy. The majority of the patients had an initial FIGO stage III (67.3%), peritoneal carcinomatosis (77.6%) and no ascites (67.3%). At QC, patients presented following tumour pattern: lower abdomen 85.7%; middle abdomen 79.6% and upper abdomen 42.9%. Median duration of surgery was 292 min (range: a total macroscopic tumour clearance could be achieved. Rates of major operative morbidity and 30-day mortality were 28.6% and 2%, respectively.Mean follow-up from QC was 18.41 months (95% confidence interval (CI):12.64-24.18) and mean overall survival (OS) 23.05 months (95% CI: 15.5-30.6). Mean OS for patients without vs any tumour residuals was 43 months (95% CI: 26.4-59.5) vs 13.4 months (95% CI: 7.42-19.4); P=0.001. Mean OS for patients who received postoperative chemotherapy (n=18; 36.7%) vs those who did not was 40.5 months (95% CI: 27.4-53.6) vs 12.03 months (95% CI: 5.9-18.18); P<0.001.Multivariate analysis indentified multifocal tumour dissemination to be of predictive significance for incomplete tumour resection, higher operative morbidity and lower survival, while systemic chemotherapy subsequent to QC had a protective significant impact on OS. No prognostic impact had ascites, platinum resistance, high grading and advanced age.\"\nQuestion:\n\"Quaternary cytoreductive surgery in ovarian cancer: does surgical effort still matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17502203": {
                "source": [
                    "\"Cholestasis occurs frequently in patients with small bowel atresia (SBA) and is often attributed to prolonged parental nutrition. When severe or prolonged, patients may undergo unnecessary intensive or invasive investigation. We characterized cholestasis and analyzed the pertinence of investigating this patient population.\nWith Research Ethics Board approval, patients with SBA between 1996 and 2005 were retrospectively reviewed. Demographics, location of atresia, operative findings, complications, investigations, resumption of feeding, duration of prolonged parental nutrition, and follow-up information were examined. Cholestasis was evaluated for incidence, severity, and evolution.\nFifty-five patients (29 male, 26 female), with a median gestational age and birth weight of 36 weeks and 2025 g, respectively, were reviewed. Care was withdrawn for 2 patients before repair. For the remaining 53 patients, SBA were duodenal atresia in 18, jejunoileal atresia in 32, and multiple atresia in 3. Of 53, 24 (45%) patients developed cholestasis postoperatively (direct/total bilirubin>20%). All patients with short bowel (4) and 60% (6/10) of patients with a delay of enteral feeding more than 14 days postoperatively had cholestasis. Ten patients (36%) proceeded with in-depth evaluations for cholestasis, with 8 (28%) undergoing liver biopsy. No patient had biliary atresia. No deaths were related to isolated cholestasis/cirrhosis. Cholestasis resolved spontaneously in all the survivors.\"\nQuestion:\n\"Cholestasis associated with small bowel atresia: do we always need to investigate?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17910536": {
                "source": [
                    "\"Adults with a mild intellectual disability (ID) often show poor decoding and reading comprehension skills. The goal of this study was to investigate the effects of teaching text comprehension strategies to these adults. Specific research goals were to determine (1) the effects of two instruction conditions, i.e. strategy instruction to individuals and strategy instruction in small groups in a reciprocal teaching context; (2) intervention programme effects on specific strategy tests (so-called direct effects), and possible differences between strategies; (3) (long-term) transfer effects of the programme on general reading comprehension ability; and (4) the regression of general text comprehension by the variables of technical reading, IQ, reading comprehension of sentences (RCS), and pretest and posttest scores on the strategies taught.\nIn total, 38 adults (age range 20-72 years; mean age of 36 years) with ID participated in the study. IQs ranged from 45 to 69 with a mean IQ of 58. The intervention programme involved 15 weekly lessons of 1 h each, taught during 3 months. Blocks of lessons included each of Brown and Palincsar's strategies of summarizing, questioning, clarifying and predicting, as participants read and studied narrative and expository texts.\nResults indicated no significant difference between group and individual instruction conditions. Second, direct programme effects - as determined by posttest-pretest contrasts for strategy tests - were substantial, except for the questioning strategy. Third, even more substantial was the transfer effect to general text comprehension. Moreover, the results on this test were well maintained at a follow-up test. Finally, the variance of general reading comprehension ability was best explained by the test of RCS, and only moderately by the strategies trained.\"\nQuestion:\n\"Adults with mild intellectual disabilities: can their reading comprehension ability be improved?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14978612": {
                "source": [
                    "\"The influence of positron emission tomography in the management of recurrent rectal cancer is well established but its role in primary rectal cancer remains uncertain. This study therefore prospectively assesses the impact of position emission tomography scanning on the management of primary rectal cancer.\nForty-six patients with advanced primary rectal cancer referred for consideration of adjuvant preoperative therapy underwent position emission tomography scanning. The referring physicians prospectively recorded each patient's stage following conventional imaging and the proposed treatment plan prior to position emission tomography scanning. This was then compared with subsequent stage and actual management implemented, and the appropriateness of position emission tomography-induced changes was noted by subsequent clinical follow-up.\nThe surgical management of 36 of 46 patients (78 percent) was unchanged as a result of position emission tomography, even though position emission tomography upstaged disease in 3 of 36 cases (8 percent) and downstaged disease in 5 of 36 cases (14 percent). In 8 of 46 cases (17 percent), management was altered because of the position emission tomography scan findings, including 6 cases (13 percent) in which surgery was cancelled and 2 other cases (4 percent) in which the radiotherapy field was changed. Where available, follow-up confirmed the appropriateness of position emission tomography-induced management change in each case. Two patients had a change in therapy independent of the position emission tomography scan due to clinical circumstances. Overall tumor stage was changed following position emission tomography in 18 of 46 patients (39 percent).\"\nQuestion:\n\"Does positron emission tomography change management in primary rectal cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23992109": {
                "source": [
                    "\"The solitary kidney (SK) is currently debated in the literature, as living kidney donation is extensively used and the diagnosis of congenital SK is frequent. Tubulointerstitial lesions associated with adaptive phenomena may occur early within the SK.\nAnalysis of the significance of urinary biomarkers in the assessment of tubulointerstitial lesions of the SK.\nA cross-sectional study of 37 patients with SK included 18 patients-acquired SK (mean age 56.44\u2009\u00b1\u200912.20 years, interval from nephrectomy 10.94\u2009\u00b1\u20099.37 years), 19 patients-congenital SK (mean age 41.52\u2009\u00b1\u200910.54 years). Urinary NAG, urinary alpha-1-microglobulin, albuminuria, eGFR (CKD-EPI equation) were measured.\nIn acquired SK, NAG increased in 60.66%, urinary alpha 1-microglobulin in 16.66%, albuminuria in 55.55% of patients. Inverse correlation with eGFR presented NAG (R(2\u2009)=\u20090.537, p\u2009=\u20090.022), urinary alpha 1-microglobulin (R(2\u2009)=\u20090.702, p\u2009=\u20090.001), albuminuria (R(2\u2009)=\u20090.655, p\u2009=\u20090.003). In congenital SK, NAG increased in 52.63%, urinary alpha 1-microglobulin in 5.26%, albuminuria in 47.36% of patients. In this group, urinary biomarkers correlated inversely with eGFR: NAG (R(2\u2009)=\u20090.743, p\u2009<\u20090.001), urinary alpha 1-microglobulin (R(2\u2009)=\u20090.701, p\u2009=\u20090.001), albuminuria (R(2\u2009)=\u20090.821, p\u2009<\u20090.001). Significant correlations were found between the urinary biomarkers in both groups.\"\nQuestion:\n\"Is the urinary biomarkers assessment a non-invasive approach to tubular lesions of the solitary kidney?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24315783": {
                "source": [
                    "\"It is commonly accepted that pathological gambling results from the interaction of multiple risk factors. Among these, dopamine replacement therapy (DRT) prescribed for Parkinson disease can be cited. Another dopamine agonist, aripiprazole, could be a new risk factor. We decided to explore this potential adverse drug reaction (ADR).\nBased on a cohort of 166 pathological gamblers starting treatment in our department, data of each of the 8 patients treated by aripiprazole at inclusion were analyzed.\nThe patients involved were schizophrenic or bipolar, mostly young men with a history of addictive disorders and regular gambling prior to the prescription of aripiprazole. For each one of them, the causality of aripiprazole was considered, using an algorithm. The probability that pathological gambling is actually due to aripiprazole is \"possible\" in 7 cases out of 8, and \"doubtful\" in one.\"\nQuestion:\n\"Aripiprazole: a new risk factor for pathological gambling?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10201555": {
                "source": [
                    "\"Serum chloride level is routinely assayed in clinical laboratories in the management of patients with kidney disorders and with metabolic diseases. It is a biological parameter that is easily, precisely and relatively cheaply measured. The epidemiological features of serum chloride levels have not been studied before.\nFor the random sample of men and women from the Belgian Interuniversity Research on Nutrition and Health aged 25-74 years, free of symptomatic coronary heart disease at baseline, serum chloride concentrations were measured, among those of other electrolytes. The cohort was followed up for 10 years with respect to subsequent cause-specific mortality.\nThe results are based on observations of 4793 men and 4313 women. According to Cox regression analysis serum chloride level was one of the strongest predictors of total, cardiovascular disease (CVD) and non-CVD mortalities independently of age, body mass index, sex, smoking, systolic blood pressure, levels of total and high-density lipoprotein cholesterol, uric acid, serum creatinine and serum total proteins and intake of diuretics. This relation was proved to be independent of levels of other serum electrolytes and similar for men and women. The estimated adjusted risk ratio for CVD death for subjects with a serum chloride level<or =100 mmol/l compared with those with levels above that limit was 1.65 (95% confidence interval 1.06-2.57) for men and 2.16 (95% confidence interval 1.11-4.22) for women. The study of adjusted risk ratios for four groups of subjects defined on the basis of their baseline serum chloride levels revealed a decreasing log-linear 'dose-response' relation to total and cardiovascular mortalities.\"\nQuestion:\n\"Is low serum chloride level a risk factor for cardiovascular mortality?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15530261": {
                "source": [
                    "\"The purpose of this paper is to evaluate the efficacy of ultrasonographic screening for primary hepatocellular carcinoma.\nA total of 680 eligible cases were classified into three groups (surveillance, opportunistic, and symptomatic groups) according to their initial exposure. We used survival time, tumor morphology, and T staging as prognostic outcomes. The outcomes of screened/unscreened and sur veillance/nonsur veillance were compared with the use of the logistic regression model.\nThe adjusted odds ratios for the screened group versus the unscreened group, with 1-, 2-, and 3-year survival time being used as outcomes, were 0.33 (95% confidence interval [CI], 0.21-0.52), 0.33 (95% CI, 0.21-0.53), and 0.37 (95% CI, 0.23-0.61), respectively. The adjusted odds ratios for surveillance versus nonsurveillance were 0.58 (95% CI, 0.35-0.97), 0.45 (95% CI, 0.27-0.74), and 0.44 (95% CI, 0.26-0.73). The odds ratios were even smaller when tumor morphology or T stage was taken as the main outcome. All these results were statistically significant. There were significant gradient relationships between prognostic outcomes and extent of screening history.\"\nQuestion:\n\"Does screening or surveillance for primary hepatocellular carcinoma with ultrasonography improve the prognosis of patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22644412": {
                "source": [
                    "\"To evaluate retrospectively whether technical factors of hepatic arterial embolization affect the prognosis of patients with hepatocellular carcinoma (HCC).\nInclusion criteria of this study were the following: (1) patients received embolization as the initial treatment during 2003-2004, (2) Child A or B liver profile, (3) five or fewer HCCs with maximum diameter of 7 cm or smaller, and (4) no extrahepatic metastasis. Patient data were gathered from 43 centers. Prognostic factors were evaluated using univariate and multivariate analyses.\nEight hundred fifteen patients were enrolled. The 1-, 3-, 5-, and 7-year overall survival rates were 92.0 % (95 % CI 90.1-93.9), 62.9 % (95 % CI 59.3-66.6), 39.0 % (95 % CI 35.1-43.0), and 26.7 % (95 % CI 22.6-30.8) in all patients. Univariate analysis showed a Child-Pugh class-A, alpha-fetoprotein level lower than 100 ng/ml, tumor size of 3 cm or smaller, tumor number of 3 or fewer, one-lobe tumor distribution, nodular tumor type, within the Milan criteria, stage I or II, no portal venous invasion, use of iodized oil, and selective embolization were significantly better prognostic factors. In the multivariate Cox model, the benefit to survival of selective embolization remained significant (hazard ratio 0.68; 95 % CI 0.48-0.97; p = 0.033).\"\nQuestion:\n\"Hepatic arterial embolization for unresectable hepatocellular carcinomas: do technical factors affect prognosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26717802": {
                "source": [
                    "\"The CLASS Act, which was part of the Affordable Care Act of 2010, established a voluntary personal assistance services (PAS) insurance program. However, concerns about enrollment and adverse selection led to repeal of the CLASS Act in\u00a02013.\nTo estimate the number of middle-aged adults interested in purchasing PAS insurance, the sociodemographic, socioeconomic and disability attributes of this population, and the maximum monthly premium they would be willing to pay for such coverage.\nA total of 13,384 adults aged 40-65 answered questions about their interest in PAS insurance in the 2011 Sample Adult National Health Interview Survey. We applied survey weights for the U.S. population and conducted logistic regression analyses to identify personal factors associated with interest in paying for the CLASS program.\nAn estimated 25.8 million adults aged 40-65 (26.7%) said they would be interested in paying for a public insurance program to cover PAS benefits. However, interest in PAS insurance varied by age, race, ethnicity, region, income, disability status, and family experience with ADL assistance. Only 1.6 million adults aged 40-65 (1.8%) said they would be willing to pay $100 per month or more for coverage.\"\nQuestion:\n\"After CLASS--Is a voluntary public insurance program a realistic way to meet the long-term support and service needs of adults with disabilities?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20684175": {
                "source": [
                    "\"Epidemiological data show significant associations of vitamin D deficiency and autoimmune diseases. Vitamin D may prevent autoimmunity by stimulating naturally occurring regulatory T cells.\nTo elucidate whether vitamin D supplementation increases Tregs frequency (%Tregs) within circulating CD4+ T cells.\nWe performed an uncontrolled vitamin D supplementation trial among 50 apparently healthy subjects including supplementation of 140,000 IU at baseline and after 4 weeks (visit 1). The final follow-up visit was performed 8 weeks after the baseline examination (visit 2). Blood was drawn at each study visit to determine 25-hydroxyvitamin D levels and %Tregs. Tregs were characterized as CD4+CD25++ T cells with expression of the transcription factor forkhead box P3 and low or absent expression of CD127.\nForty-six study participants (65% females, mean age +/- SD 31 +/- 8 years) completed the trial. 25(OH)D levels increased from 23.9 +/- 12.9 ng/ml at baseline to 45.9 +/- 14.0 ng/ml at visit 1 and 58.0 +/- 15.1 ng/ml at visit 2. %Tregs at baseline were 4.8 +/- 1.4. Compared to baseline levels we noticed a significant increase of %Tregs at study visit 1 (5.9 +/- 1.7, P<0.001) and 2 (5.6 +/- 1.6, P<0.001).\"\nQuestion:\n\"Vitamin D supplementation and regulatory T cells in apparently healthy subjects: vitamin D treatment for autoimmune diseases?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23412195": {
                "source": [
                    "\"This study was designed to compare clinical effectiveness of operative with nonoperative treatment for displaced midshaft clavicular fractures (DMCF).\nWe systematically searched electronic databases (MEDILINE, EMBASE, CLINICAL, OVID, BIOSIS and Cochrane registry of controlled clinical trials) to identify randomized controlled trials (RCTs) in which operative treatment was compared with nonoperative treatment for DMCF from 1980 to 2012. The methodologic quality of trials was assessed. Data from chosen studies were pooled with using of fixed-effects and random-effects models with mean differences and risk ratios for continuous and dichotomous variables, respectively.\nFour RCTs with a total of 321 patients were screened for the present study. Results showed that the operative treatment was superior to the nonoperative treatment regarding the rate of nonunion [95\u00a0% confidence interval (CI) (0.05, 0.43), P\u00a0=\u00a00.0004], malunion [95\u00a0% CI (0.06, 0.34), P\u00a0<\u00a00.00001] and overall complication [95\u00a0% CI (0.43-0.76), P\u00a0=\u00a00.0001]. Subgroup analyses of complications revealed that significant differences were existed in the incidence of neurologic symptoms [95\u00a0% CI (0.20, 0.74), P\u00a0=\u00a00.004] and dissatisfaction with appearance [95\u00a0% CI (0.19, 0.65), P\u00a0=\u00a00.001]. Lack of consistent and standardized assessment data, insufficiency analysis that carried out showed improved functional outcomes (P\u00a0<\u00a00.05) in operative treatment.\"\nQuestion:\n\"Should displaced midshaft clavicular fractures be treated surgically?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15502995": {
                "source": [
                    "\"To analyse associations between indicators for adoption of new drugs and to test the hypothesis that physicians' early adoption of new drugs is a personal trait independent of drug groups.\nIn a population-based cohort study using register data, we analysed the prescribing of new drugs by Danish general practitioners. Angiotensin-II antagonists, triptans, selective cyclo-oxygenase-2 antagonists and esomeprazol were used in the assessment. As indicators of new drug uptake, we used adoption time, cumulative incidence, preference proportion, incidence rate and prescription cost and volume. For each measure, we ranked the general practices. Ranks were pair-wise plotted, and Pearson's correlation coefficient ( r) was calculated. Next, we analysed the correlation between ranks across different drug classes.\nFor all indicators, the general practitioners' adoption of one group of drugs was poorly associated with adoption of others ( r</=0.49), indicating that early adoption of one type of drugs is not associated with early adoption of another. For all drug groups, adoption time adjusted for practice size was only weakly associated with other indicators ( r: -0.56 to -0.27). Indicators, based on cost and volume of drugs, were highly correlated ( r: 0.96-0.99), and the others correlated reasonably well ( r: 0.51-0.91).\"\nQuestion:\n\"Does the early adopter of drugs exist?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18607272": {
                "source": [
                    "\"To compare children's, parents' and physicians' perceptions of children's body size.\nWe administered a structured questionnaire of body size perception using a descriptive Likert scale keyed to body image figures to children ages 12 to 18 years. The same scale was given to parents of children ages 5 to 18 years. The sample consisted of 91 children and their parents being seen in the Pediatric Gastroenterology Clinic for concerns unrelated to overweight. Weight and height of the children were measured, and body mass index (BMI) was calculated. The children's BMI percentiles were categorized as underweight (<15th), normal (15th-85th), overweight (85th-95th), and obese (95th and above). The attending physician independently completed the body image and description scale and indicated the figure that most accurately represented the patient without reference to BMI standards. Accuracy of the patients', parents', and doctors' estimates were statistically compared.\nThe sample population consisted of 6.4% underweight, 70.5% normal weight, 7.7% overweight, and 15.4% obese. Forty-four percent of parents underestimated children's body size using word descriptions and 47% underestimated using figures. Forty percent of the children underestimated their own body size using descriptions and 43% underestimated using figures. The physicians in this study had a higher percentage of correct estimates; however, they underestimated 33% of the patients using both word descriptions and figures. Some obese children were not recognized, and several average children were perceived as underweight.\"\nQuestion:\n\"Body perception: do parents, their children, and their children's physicians perceive body image differently?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25432938": {
                "source": [
                    "\"The objective of the current study is to determine to what extent the reduction of Chile's traffic fatalities and injuries during 2000-2012 was related to the police traffic enforcement increment registered after the introduction of its 2005 traffic law reform.\nA unique dataset with assembled information from public institutions and analyses based on ordinary least square and robust random effects models was carried out. Dependent variables were traffic fatality and severe injury rates per population and vehicle fleet. Independent variables were: (1) presence of new national traffic law; (2) police officers per population; (3) number of traffic tickets per police officer; and (4) interaction effect of number of traffic tickets per police officer with traffic law reform. Oil prices, alcohol consumption, proportion of male population 15-24 years old, unemployment, road infrastructure investment, years' effects and regions' effects represented control variables.\nEmpirical estimates from instrumental variables suggest that the enactment of the traffic law reform in interaction with number of traffic tickets per police officer is significantly associated with a decrease of 8% in traffic fatalities and 7% in severe injuries. Piecewise regression model results for the 2007-2012 period suggest that police traffic enforcement reduced traffic fatalities by 59% and severe injuries by 37%.\"\nQuestion:\n\"Did Chile's traffic law reform push police enforcement?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9142039": {
                "source": [
                    "\"To assess the relationship between the experience of pediatric housestaff and tests ordered on infants in the neonatal intensive care unit (ICU).\nProspective, cohort study over one full academic year.\nOne academic Level III neonatal intensive care nursery.\nData were collected prospectively on all 785 infants admitted to the neonatal ICU from July 1993 to June 1994. These infants were cared for by 14 different categorical pediatric housestaff.\nOur neonatal ICU has either a resident or an intern on-call by himself/herself at night, affording us a natural setting to compare intern vs. resident test ordering. The outcomes of interest were number of arterial blood gases, radiographs, and electrolytes ordered per infant by the on-call pediatric houseofficer, as tabulated the morning after the call night. Control variables included the severity-of-illness of the individual infant (using the Neonatal Therapeutic Intervention Scoring System), the workload of the houseofficer (number of patients, number of admissions), and supervision (rounding frequency and on-call attending). Controlling for the severity-of-illness of the infant, the workload on the call night, and supervision with multiple linear regression, we found that interns ordered significantly (p = .02) greater numbers of arterial blood gases per infant than residents, amounting to some 0.33 blood gases per infant per call night (3.22 vs. 2.89 arterial blood gases per infant per night). This increase of 0.33 blood gases per infant amounts to interns ordering $169 more arterial blood gases per call night at our institution. There was no difference between interns and residents in ordering radiographs or electrolytes.\"\nQuestion:\n\"Does pediatric housestaff experience influence tests ordered for infants in the neonatal intensive care unit?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10811329": {
                "source": [
                    "\"The goal of this retrospective study was to assess whether 99mTc-white blood cell (WBC) scintigraphy and upper gastrointestinal small bowel follow-through (UGI-SBFT) could exclude inflammation in children suspected of having inflammatory bowel disease (IBD).\nOf a population of 313 children who had a 99mTc-WBC scan, 130 children were studied exclusively to rule out IBD. Sixty-nine colonoscopies with biopsies were done within a short time interval of the 99mTc-WBC scans. There were also 51 controls studied with 99mTc-WBC scintigraphy.\nOf the 130 children studied to exclude IBD, the final diagnosis was Crohn's disease in 27, ulcerative colitis in nine, miscellaneous colitis in 13, probably normal in 42, and normal in 39. The 99mTc-WBC scans were positive in all but three newly diagnosed Crohn's disease, ulcerative colitis, or miscellaneous colitis children. The false-negative 99mTc-WBC studies were seen in children with mild inflammation on biopsies and normal UGI-SBFT studies. In the 46 children with a true-positive 99mTc-WBC scan, 81% (17/21) of UGI-SBFT studies were normal. In five children with equivocal UGI-SBFT studies, the 99mTc-WBC scan correctly predicted if inflammation was present in the terminal ileum.\"\nQuestion:\n\"Are 99mTc leukocyte scintigraphy and SBFT studies useful in children suspected of having inflammatory bowel disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17062234": {
                "source": [
                    "\"Occlusion of the atherosclerotic ascending aorta by an endoaortic inflatable balloon has been proposed as an alternative to conventional cross-clamping to prevent injury to the vessel and distal embolization of debris. The safety and the effectiveness of endoaortic occlusion have not been documented in this setting.\nEndoaortic occlusion was employed in 52 of 2,172 consecutive patients. Surgeon's choice was based on preoperative identification of aortic calcifications or intraoperative epiaortic ultrasonographic scanning. Deaths and strokes were analyzed casewise and in aggregate.\nIn 10 patients (19.2%), the endoaortic balloon had to be replaced by the ordinary cross-clamp because of incomplete occlusion (n = 5), hindered exposure (n = 2), or balloon rupture (n = 3). In-hospital death occurred in 13 patients (25%), and stroke on awakening from anesthesia in 2 (3.8%). The death rate of patients treated by endoaortic occlusion was significantly higher compared with all other patients (4.2%, p<0.0001) and with the expected estimate by European System for Cardiac Operative Risk Evaluation (10.5%, p = 0.05). By multivariable analysis, use of endoaortic occlusion was independently associated with in-hospital death (odds ratio = 5.609, 95% confidence interval: 2.684 to 11.719). Although the stroke rate was higher in the endoaortic occlusion group compared with all other patients, the difference was only possibly significant (3.8% versus 0.8%, p = 0.067).\"\nQuestion:\n\"Surgical management of the atherosclerotic ascending aorta: is endoaortic balloon occlusion safe?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21074975": {
                "source": [
                    "\"The ultra high risk (UHR) for psychosis criteria have been validated in a number of studies. However, it is not known whether particular UHR criteria (Attenuated Psychotic Symptoms (APS), Brief Limited Intermittent Psychotic Symptoms (BLIPS) or Trait vulnerability criteria), or combination of criteria, is associated with a higher risk of transition to psychosis. The current study investigated this issue over a 6-month follow-up period. We hypothesised that the risk of transition would increase in the following order: Trait alone<APS alone<APS+Trait<BLIPS.\nData on UHR intake criteria and transition to psychosis status at 6 months were analysed for UHR patients seen at the PACE clinic, Orygen Youth Health between January 2000 and November 2008.\nA total of 928 new referrals were accepted into the PACE clinic over this period of whom 817 (88%) had baseline information available for analysis. The percentage of subjects who presented with APS, Trait and BLIPS were 83%, 27% and 4%, respectively. When the two intermediate groups (APS alone and APS+Trait) were combined, there was evidence that the risk of transition increased in the order of Trait alone<APS<BLIPS (p=0.024, adjusted analysis).\"\nQuestion:\n\"Ultra high risk (UHR) for psychosis criteria: are there different levels of risk for transition to psychosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25487603": {
                "source": [
                    "\"The aim of this study was to describe the evolution and epidemiologic characteristics of shigellosis patients over a 25 year period in a large city.\nShigellosis is a notifiable disease in Spain since 1988. Cases are analyzed in Barcelona residents included in the registry between 1988-2012. A descriptive analysis by sex, age, mode of transmission and Shigella species is presented. Trend analysis and time series were performed.\nOf the 559 cases analyzed, 60.15% were males. A sustained increase was observed in the trend since 2008 in males (p<0,05), especially at the expense of males who had no history of food poisoning or travel to endemic areas. The increasing tendency was greater in males from 21 to 60 years, both for S. flexneri (since 2009), and for S. sonnei (since 2004). In 2012 it was noted that in the men with S. flexneri, the 63% were men who have sex with men.\"\nQuestion:\n\"Analysis of the epidemiological pattern of Shigellosis in Barcelona between 1988 and 2012: Is it an emerging sexually transmitted infection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21402341": {
                "source": [
                    "\"Extracranial internal carotid artery stenosis is a risk factor for perioperative stroke in patients undergoing coronary artery bypass surgery (CAB). Although selective and non-selective methods of preoperative carotid screening have been advocated, it remains unclear if this screening is clinically relevant.AIM: To test whether selective carotid screening is as effective as non-selective screening in detecting significant carotid disease.\nThe case records of patients consecutively undergoing CAB were reviewed. Patients were stratified retrospectively into high- or low-risk groups according to risk factors for significant carotid stenosis and perioperative stroke: peripheral vascular disease (PVD), carotid bruit, diabetes mellitus, age>70 years and/or history of cerebrovascular disease. Prevalence of carotid stenosis detected by ultrasonography, surgical management and perioperative stroke rates were determined in each group.\nOverall, 205 consecutive patients underwent preoperative carotid screening. The prevalence of significant carotid stenosis was 5.8%. Univariate analysis confirmed that PVD (P=0.005), carotid bruit (P=0.003) and diabetes mellitus (P=0.05) were significant risk factors for stenosis. Carotid stenosis was a risk factor for stroke (P=0.03). Prevalence of carotid stenosis was higher in the high-risk group (9.1%) than the low-risk group (1.2%) (P<0.05). All concomitant or staged carotid endarterectomies/CAB (5/205) and all patients who had perioperative strokes (5/205) were in the high-risk group (P=0.01).\"\nQuestion:\n\"Assessment of carotid artery stenosis before coronary artery bypass surgery. Is it always necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20338971": {
                "source": [
                    "\"There are various causes of halitosis, one of which is chronic renal failure. The objective of this study was to investigate halitosis levels in end-stage renal disease (ESRD) patients before and after peritoneal dialysis (PD) therapy.\n42 subjects with ESRD were included in this study. The presence of halitosis was assessed using an organoleptic measurement and compared with blood urea nitrogen (BUN) levels and salivary flow rates. Decayed, missing, and filled teeth (DMFT) index and Community Periodontal Index (CPI) were calculated. All measurements were done before and after patients had received 3 months of PD therapy.\nMean serum BUN level was found to be lower (46.05 \u00b1 13.30 vs 91.24 \u00b1 31.28 mg/dL), salivary flow rate higher (0.34 \u00b1 0.07 vs 0.26 \u00b1 0.04 mL/minute), and halitosis level lower (2.39 \u00b1 0.60 vs 3.90 \u00b1 0.37) at the end of 3 months of PD therapy than at the beginning of PD therapy. There was no significant difference in CPI or DMFT index before and after PD therapy (p>0.05). There was statistically significant positive correlation between the presence of halitosis and BUN levels (r = 0.702, p = 0.001 before PD; r = 0.45, p = 0.002 after PD) and a negative correlation between the presence of halitosis and salivary flow rates (r = -0.69, p = 0.000 before PD; r = -0.37, p = 0.01 after PD).\"\nQuestion:\n\"Does peritoneal dialysis affect halitosis in patients with end-stage renal disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9363529": {
                "source": [
                    "\"To evaluate psychological distress as a predictor of disability due to common chronic disorders.\nA 10-year follow-up study was carried out among a representative cohort (N = 8655) of 18-64 year old Finnish farmers, who had participated in a health survey in 1979 and were able to work at baseline. A record linkage with the nationwide register of the Social Insurance Institution was made to identify disability pensions granted between 1980 and 1990 in the cohort. The medical certificates of 1004 (11.6%) prematurely retired farmers were reviewed to confirm and classify disabling conditions. A sum score based on self-reports of 11 symptoms at the baseline was used as a measure of psychological distress.\nAfter adjustment for age, sex, smoking and body mass index, the cause-specific relative risks (RR) (95% confidence intervals [CI]) of disability in the highest quartile of the psychological distress score as compared with the lowest quartile were for myocardial infarction 2.34 (95% CI: 1.17-4.69), for depression 2.50 (95% CI: 1.09-5.72), for neck-shoulder disorders 1.98 (95% CI: 1.26-3.11), for unspecified low-back disorders 1.76 (95% CI: 1.24-2.49), for knee osteoarthritis 1.55 (95% CI: 0.91-2.63) and for trip osteoarthritis 0.89 (95% CI: 0.42-1.85). The corresponding RR for overall disability was 1.76 (95% CI: 1.44-2.14) in the highest quartile of psychological distress score as compared with the lowest quartile.\"\nQuestion:\n\"Does psychological distress predict disability?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9446993": {
                "source": [
                    "\"To determine the ability of dentists to recognize digitally manipulated radiographs.\nA poster was presented at the Annual Meeting of the German Society for Periodontology displaying the intra-oral radiographs of 12 different patients. Half of the radiographs were subjected to digital manipulation to add or remove specific features. Dentists were asked to identify these radiographs by means of a questionnaire.\nThirty-nine dentists submitted usable questionnaires. Statistical evaluation revealed a distribution of hits similar to the random distribution. None of the dentists detected all the six manipulated radiographs; three dentists had five correct, but there were five with only one. An authentic radiograph scored highest as a manipulation.\"\nQuestion:\n\"Can dentists recognize manipulated digital radiographs?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25218577": {
                "source": [
                    "\"Reconstruction of the joint line is crucial in total knee arthroplasty (TKA). A routine height of tibial cut to maintain the natural joint line may compromise the preservation of the PCL. Since the PCL footprint is not accessible prior to tibial osteotomy, it seems beneficial to identify a reliable extraarticular anatomic landmark for predicting the PCL footprint and being visible within standard TKA approach. The fibula head predicts reliably the location of PCL footprint; however, it is not accessible during TKA. The aim of this study now was to analyze whether the tibial tuberosity can serve as a reliable referencing landmark to estimate the PCL footprint height prior to tibial cut.\nThe first consecutive case series included 216 CR TKA. Standing postoperative lateral view radiographs were utilized to measure the vertical distance between tibial tuberosity and tibial osteotomy plane. In the second case series, 223 knee MRIs were consecutively analyzed to measure the vertical distance between tibial tuberosity and PCL footprint. The probability of partial or total PCL removal was calculated for different vertical distances between tibial tuberosity and tibial cutting surface.\nThe vertical distance between the tibial tuberosity and tibial cut averaged 24.7 \u00b1 4 mm. The average vertical distance from tibial tuberosity to proximal and to distal PCL footprint was found to be 22 \u00b1 4.4 and 16 \u00b1 4.4 mm, respectively. Five knees were considered at 50% risk of an entire PCL removal after CR TKA.\"\nQuestion:\n\"Preservation of the PCL when performing cruciate-retaining TKA: Is the tibial tuberosity a reliable predictor of the PCL footprint location?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8017535": {
                "source": [
                    "\"This study was undertaken to examine whether use of alcohol, cigarettes, marijuana, cocaine, and other illicit drugs is related to the likelihood of sexual behaviors that increase risk for human immunodeficiency virus (HIV) infection among youth.\nThe 1990 national Youth Risk Behavior Survey was used to collect self-reported information about a broad range of health risk behaviors from a representative sample of 11,631 high school students in the United States.\nStudents who reported no substance use were least likely to report having had sexual intercourse, having had four or more sex partners, and not having used a condom at last sexual intercourse. Adjusted for age, sex, and race/ethnicity, odds ratios for each of these sexual risk behaviors were greatest among students who had used marijuana, cocaine, or other illicit drugs. Students who had used only alcohol or cigarettes had smaller but still significant increases in the likelihood of having had sexual intercourse and of having had four or more sex partners.\"\nQuestion:\n\"Substance use and HIV-related sexual behaviors among US high school students: are they related?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14612308": {
                "source": [
                    "\"PRISM is claimed to score disease severity which has attributed an impact on length of PICU stay (LOS).\nTo determine the impact of PRISM on LOS, and evaluate an Artificial Neural Network's (ANN) performance to estimate LOS from PRISM item patterns.\nRetrospectively we performed correlation and regression analyses on routinely scored PRISM data of all consecutive admissions to our level-III PICU from 1994 to 1999 (n>2000) with individual LOS. In addition, an ANN was trained on the chronologically first 75% of those data (inputs, PRISM items + age + sex; output, LOS). The ANN's performance was tested on the remaining most recent 25% of the data sets.\nThe Spearman and Pearson coefficients of correlation between PRISM and LOS were 0.2 (p<0.001) and 0.08 (p = 0.0003), the latter being slightly higher when LOS was logarithmically transformed. Pearson's coefficient of correlation between ANN derived LOS estimate and actual LOS was 0.21 (p<0.001) (LOS logarithmically transformed: 0.34; p<0.001) in the independent validation sample.\"\nQuestion:\n\"Can PRISM predict length of PICU stay?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26536001": {
                "source": [
                    "\"It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan\u00ae in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors.\nAll 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC.\nIn total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).\"\nQuestion:\n\"Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18537964": {
                "source": [
                    "\"To determine the impact of a physician's specialty on the frequency and depth of medication history documented in patient medical records.\nA cross-sectional assessment of the frequency and depth of medication history information documented by 123 physicians for 900 randomly selected patients stratified across Cardiology, Chest, Dermatology, Endocrine, Gastroenterology, Haematology, Neurology, Psychiatry and Renal specialties was carried out at a 900-bed teaching hospital located in Ibadan, Nigeria.\nFour hundred and forty-three (49.2%) of the cohort were males and 457 (50.8%) were females; with mean ages 43.2 +/- 18.6 and 43.1 +/- 17.9 years respectively. Physicians' specialties significantly influenced the depth of documentation of the medication history information across the nine specialties (P<0.0001). Post hoc pair-wise comparisons with Tukey's HSD test showed that the mean scores for adverse drug reactions and adherence to medicines was highest in the Cardiology specialty; while the Chest specialty had the highest mean scores for allergy to drugs, food, chemicals and cigarette smoking. Mean scores for the use of alcohol; illicit drugs; dietary restrictions was highest for Gastroenterology, Psychiatry and Endocrine specialties respectively. Physicians' specialties also significantly influenced the frequency of documentation of the medication history across the nine specialties (P<0.0001).\"\nQuestion:\n\"Does a physician's specialty influence the recording of medication history in patients' case notes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25150098": {
                "source": [
                    "\"It is not known whether common carotid intima media thickness (CIMT) can serve as a surrogate marker of cardiovascular risk among black Africans. Therefore, we examined whether CIMT differed significantly among individuals with distinct cardiovascular phenotype and correlated significantly with traditional cardiovascular risk factors in a black African population.\nCIMT was measured in 456 subjects with three distinct cardiovascular phenotypes - 175 consecutive Nigerian African stroke patients, 161 hypertensive patients without stroke and 120 normotensive non-smoking adults. For each pair of cardiovascular phenotypes, c-statistics were obtained for CIMT and traditional vascular risk factors (including age, gender, weight, waist circumference, smoking, alcohol, systolic and diastolic blood pressures, fasting plasma glucose, fasting total cholesterol). Pearson's correlation coefficients were calculated to quantify bivariate relationships.\nBilaterally, CIMT was significantly different among the three cardiovascular phenotypes (right: p\u2009<\u20090.001, F\u2009=\u200933.8; left: p\u2009<\u20090.001, F\u2009=\u200948.6). CIMT had a higher c-statistic for differentiating stroke versus normotension (c\u2009=\u20090.78 right; 0.82 left, p\u2009<\u20090.001) and hypertension versus normotension (c\u2009=\u20090.65 right; 0.71 left, p\u2009<\u20090.001) than several traditional vascular risk factors. Bilaterally, combining all subjects, CIMT was the only factor that correlated significantly (right: 0.12\u2009\u2264\u2009r\u2009\u2264\u20090.41, 0.018\u2009\u2264\u2009p\u2009<\u20090.0001; left: 0.18\u2009\u2264\u2009r\u2009\u2264\u20090.41, 0.005\u2009\u2264\u2009p\u2009<\u20090.0001) to all the traditional cardiovascular risk factors assessed.\"\nQuestion:\n\"Can common carotid intima media thickness serve as an indicator of both cardiovascular phenotype and risk among black Africans?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12090319": {
                "source": [
                    "\"To determine the necessity of pelvic computed tomography (CT) in patients of renal cell carcinoma (RCC).\nWe reviewed the records of 400 patients of RCC, who underwent treatment at our institution between January 1988 and February 2001. These patients were evaluated pre-operatively with ultrasonograms (USG) and contrast enhanced CT scan of the abdomen and pelvis. USG or CT scans of these cases were reviewed for presence of pathology in the pelvis, which were classified into 3 categories viz; benign and likely to be insignificant, benign and likely to be significant; and malignant.\nOf the 400 cases, 114 were stage I, 68 were stage II, 99 were stage III and 119 were stage IV. In all patients, tumour was identified in the kidney on preoperative CT scan. Fourteen patients (3.5%) had an abnormality on pelvic CT. Five (1.25%) had category 1, three (0.75%) had category 2 and six (1.5%) had category 3 abnormality on pelvic CT. However, all these abnormalities in pelvis were detected prior to CT by other investigations (USG or plain x-ray). Of the six cases with malignant findings, two had superficial bladder cancer, one had RCC in a pelvic kidney and three had bone metastases in the pelvis.\"\nQuestion:\n\"Is there a need for pelvic CT scan in cases of renal cell carcinoma?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21214884": {
                "source": [
                    "\"Using polymerase chain reaction techniques, we evaluated the presence of HPV infection in human breast milk collected from 21 HPV-positive and 11 HPV-negative mothers.\nOf the 32 studied human milk specimens, no 'high-risk' HPV 16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58 or 58 DNA was detected.\"\nQuestion:\n\"Can 'high-risk' human papillomaviruses (HPVs) be detected in human breast milk?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22902073": {
                "source": [
                    "\"The purpose of this study was to investigate whether knowledge of ultrasound-obtained estimated fetal weight (US-EFW) is a risk factor for cesarean delivery (CD).\nRetrospective cohort from a single center in 2009-2010 of singleton, term live births. CD rates were compared for women with and without US-EFW within 1 month of delivery and adjusted for potential confounders.\nOf the 2329 women in our cohort, 50.2% had US-EFW within 1 month of delivery. CD was significantly more common for women with US-EFW (15.7% vs 10.2%; P<.001); after we controlled for confounders, US-EFW remained an independent risk factor for CD (odds ratio, 1.44; 95% confidence interval, 1.1-1.9). The risk increased when US-EFW was>3500 g (odds ratio, 1.8; 95% confidence interval, 1.3-2.7).\"\nQuestion:\n\"Estimated fetal weight by ultrasound: a modifiable risk factor for cesarean delivery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11838307": {
                "source": [
                    "\"To prospectively evaluate the amount of tissue removed at loop electrosurgical excision procedure (LEEP) vs. cold knife conization.\nForty consecutive LEEP or cold knife conization specimens were prospectively measured and weighed by a single pathology technician. Diameter, length and weight of the specimens were compared using Student's t test.\nMean diameter of cold knife cone specimens was 2.6 vs. 2.2 cm for LEEP (P = .07). Mean length of cold knife cone specimens was 1.5 vs. 1.0 cm for LEEP (P = .001). Mean weight for cold knife cone specimens was 4.4 vs. 2.0 g for LEEP (P = .001).\"\nQuestion:\n\"Cold knife conization vs. LEEP. Are they the same procedure?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17220021": {
                "source": [
                    "\"Group B Streptococci (GBS) asymptomatically colonize the vaginal or rectal areas of about 20% of pregnant women (4-40%). About 50% of infants to mothers with GBS colonization also become colonized at rectal, umbilical or oral sites. GBS is a leading bacterial cause of neonatal illness and death. The present prevalence rate of GBS carriers among parturients in the western Galilee in Israel is unknown.AIM: A prospective study of the GBS carrier rate according to origin and gestational age in the western Galilee in Israel.\nA prospective study including 700 pregnant women. All women were screened for carriage of GBS by vaginal and rectal cultures.\nSixteen percent of the parturients were found to be GBS colonized. The prevalence of GBS was 13.7% in Jewish women and 19% in Arab women, P=0.038. The women were also divided into two groups according to the gestational age one group included 414 women in 24-37 weeks gestation, and the other group included 286 women in term pregnancy. No difference was found in the rate of GBS carriers between the two gestational age groups.\"\nQuestion:\n\"Is there an increase in the incidence of gbs carrier rates among pregnant women in northern Israel?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12006913": {
                "source": [
                    "\"Studies on coronary risk factors in men and women are mainly based on mortality data and few compare results of both sexes with consistent study design and diagnostic criteria. This study assesses the major risk factors for coronary events in men and women from the Reykjavik Study.\nWithin a prospective, population-based cohort study individuals without history of myocardial infarction were identified and the relative risk of baseline variables was assessed in relation to verified myocardial infarction or coronary death during follow-up.\nOf the 9681 women and 8888 men who attended risk assessment from 1967-1991, with follow-up period of up to 28 years, 706 women and 1700 men suffered a non-fatal myocardial infarction or coronary death.\nSerum cholesterol was a significant risk factor for both sexes, with hazard ratios (HR) decreasing with age. Systolic blood pressure was a stronger risk factor for women as was ECG-confirmed left ventricular hypertrophy (women HR 2.89, 95% confidence interval [CI] 1.67-5.01; men HR 1.11 [CI 0.86-1.43]). Fasting blood glucose>or =6.7 mmol/L identified significantly higher risk for women (HR 2.65) than men (HR 2.08) as did self-reported diabetes. Triglyceride risk was significantly higher for women and decreased significantly with age. Smoking increased risk two- to five-fold, increasing with dose, for women, which was significantly higher than the doubling in risk for men.\"\nQuestion:\n\"Do lipids, blood pressure, diabetes, and smoking confer equal risk of myocardial infarction in women as in men?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24947183": {
                "source": [
                    "\"To analyze the changes in the curve extent, pattern and the fusion level in adolescent idiopathic scoliosis (AIS) patients who undergo delayed surgery instead of early surgery.\nThirty-five immature AIS patients whose radiographs demonstrated an initial primary curve of more than 40\u00b0 with a subsequent increase of 10\u00b0 before attaining skeletal maturity with brace were enrolled. The initial and the final radiographs taken before surgery were compared to assess the changes in curve extent, pattern and the fusion levels as recommended by King's, Lenke's and Suk's guidelines.\nThe average age of 35 AIS patients was 12.7 \u00b1 1.6 years. The time interval between initial and final radiography was 39.3 \u00b1 20.2 months and the degree of progress of the primary curve was 13 \u00b1 9.7\u00b0. Fusion levels changed in 33 (94.2%), 33 (94.2%) and 32 (91.4%) patients according to King's, Lenke's and Suk's guidelines, respectively. Curve pattern was changed in 2 (5.7%), 12 (34.3%) and 10 (28.6) patients by King's, Lenke's and Suk's guidelines. The mean number of levels requiring fusion increased from 9.4 \u00b1 2.1 at initial visit to 11.1 \u00b1 1.8 at the final follow-up using King's guidelines, 9.7 \u00b1 2.2-11.6 \u00b1 2.0 as per Lenke's guidelines and 9.1 \u00b1 2.0-11.5 \u00b1 2.3 when fusion was planned using Suk's guidelines (p<0.001 in all guidelines).\"\nQuestion:\n\"Does delaying surgery in immature adolescent idiopathic scoliosis patients with progressive curve, lead to addition of fusion levels?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15918864": {
                "source": [
                    "\"Little is known about how information needs change over time in the early postpartum period or about how these needs might differ given socioeconomic circumstances. This study's aim was to examine women's concerns at the time of hospital discharge and unmet learning needs as self-identified at 4 weeks after discharge.\nData were collected as part of a cross-sectional survey of postpartum health outcomes, service use, and costs of care in the first 4 weeks after postpartum hospital discharge. Recruitment of 250 women was conducted from each of 5 hospitals in Ontario, Canada (n = 1,250). Women who had given vaginal birth to a single live infant, and who were being discharged at the same time as their infant, assuming care of their infant, competent to give consent, and able to communicate in one of the study languages were eligible. Participants completed a self-report questionnaire in hospital; 890 (71.2%) took part in a structured telephone interview 4 weeks after hospital discharge.\nApproximately 17 percent of participants were of low socioeconomic status. Breastfeeding and signs of infant illness were the most frequently identified concerns by women, regardless of their socioeconomic status. Signs of infant illness and infant care/behavior were the main unmet learning needs. Although few differences in identified concerns were evident, women of low socioeconomic status were significantly more likely to report unmet learning needs related to 9 of 10 topics compared with women of higher socioeconomic status. For most topics, significantly more women of both groups identified learning needs 4 weeks after discharge compared with the number who identified corresponding concerns while in hospital.\"\nQuestion:\n\"Learning needs of postpartum women: does socioeconomic status matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17489316": {
                "source": [
                    "\"To determine whether there is a relationship between VEGF expression and renal vein and vena cava invasion in stage pT3 renal cell carcinoma and to evaluate the impact of VEGF expression on survival in pT3 renal cell carcinoma.\n78 patients with a pT3a or pT3b tumour without vena cava invasion or pT3b tumour with vena cava invasion were compared for age, gender, Fuhrman grade and immunohistochemical expression of VEGF. All these variables were submitted to univariate and multivariate analysis to establish their impact on survival.\nOnly tumour size appeared to be significantly different between the 3 groups. On univariate analysis, invasion of the perirenal fat, lymph node involvement, distant metastases and VEGF expression were significantly associated with survival (p<0.01). On multivariate analysis, lymph node involvement, distant metastases and VEGF expression (OR 6.07) were identified as independent predictive factors of survival.\"\nQuestion:\n\"Is tumour expression of VEGF associated with venous invasion and survival in pT3 renal cell carcinoma?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19430778": {
                "source": [
                    "\"To correlate magnetic resonance (MR) image findings with pain response by provocation discography in patients with discogenic low back pain, with an emphasis on the combination analysis of a high intensity zone (HIZ) and disc contour abnormalities.\nSixty-two patients (aged 17-68 years) with axial low back pain that was likely to be disc related underwent lumbar discography (178 discs tested). The MR images were evaluated for disc degeneration, disc contour abnormalities, HIZ, and endplate abnormalities. Based on the combination of an HIZ and disc contour abnormalities, four classes were determined: (1) normal or bulging disc without HIZ; (2) normal or bulging disc with HIZ; (3) disc protrusion without HIZ; (4) disc protrusion with HIZ. These MR image findings and a new combined MR classification were analyzed in the base of concordant pain determined by discography.\nDisc protrusion with HIZ [sensitivity 45.5%; specificity 97.8%; positive predictive value (PPV), 87.0%] correlated significantly with concordant pain provocation (P<0.01). A normal or bulging disc with HIZ was not associated with reproduction of pain. Disc degeneration (sensitivity 95.4%; specificity 38.8%; PPV 33.9%), disc protrusion (sensitivity 68.2%; specificity 80.6%; PPV 53.6%), and HIZ (sensitivity 56.8%; specificity 83.6%; PPV 53.2%) were not helpful in the identification of a disc with concordant pain.\"\nQuestion:\n\"Can magnetic resonance imaging accurately predict concordant pain provocation during provocative disc injection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18222909": {
                "source": [
                    "\"The hypothesis was tested that pectin content and methylation degree participate in regulation of cell wall mechanical properties and in this way may affect tissue growth and freezing resistance over the course of plant cold acclimation and de-acclimation.\nExperiments were carried on the leaves of two double-haploid lines of winter oil-seed rape (Brassica napus subsp. oleifera), differing in winter survival and resistance to blackleg fungus (Leptosphaeria maculans).\nPlant acclimation in the cold (2 degrees C) brought about retardation of leaf expansion, concomitant with development of freezing resistance. These effects were associated with the increases in leaf tensile stiffness, cell wall and pectin contents, pectin methylesterase (EC 3.1.1.11) activity and the low-methylated pectin content, independently of the genotype studied. However, the cold-induced modifications in the cell wall properties were more pronounced in the leaves of the more pathogen-resistant genotype. De-acclimation promoted leaf expansion and reversed most of the cold-induced effects, with the exception of pectin methylesterase activity.\"\nQuestion:\n\"Are pectins involved in cold acclimation and de-acclimation of winter oil-seed rape plants?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25488308": {
                "source": [
                    "\"The proper angle of miniscrew insertion is important for cortical anchorage, patient safety, and biomechanical control. The purposes of this study are to report the alveolar process thickness and inter-radicular space in the posterior region of the mandible, to assess the impact of different miniscrew insertion angle protocols, and to identify differences between the genders or types of malocclusion.\nIn this retrospective study, 100 individuals were selected for orthodontic treatment at a radiology clinic. Cone-beam computed tomography data were imported into 3-dimensional software. The predictor variable was the location in the mandible and insertion angle. The demographic variables collected included age, gender, and malocclusion (Angle Classes I and II). The primary outcome variables were bone thickness and inter-radicular space. The inter-radicular spaces were evaluated 5 mm from the cement-enamel junction. The bone thicknesses were taken at 45\u00b0, 60\u00b0, and 90\u00b0 in relation to the alveolar ridge, simulating a miniscrew insertion. These factors were evaluated for sexual dimorphism and malocclusion (Angle Classes I and II). Sexual dimorphism and malocclusion were evaluated with t tests. To compare the inter-radicular space and the thickness of bone between areas, an analysis of variance for repeated measures was used.\nThe sample was composed of 100 patients with a mean age of 17.4 \u00b1 6.74 years. There were 61 female and 39 male patients and 60 Class I and 40 Class II molar relationships. The inter-radicular space ranged from 2.46 to 3.31 mm, and alveolar bone thickness ranged from 8.01 to 13.77 mm. The thickness tended to decrease with the increase in insertion angle from 45\u00b0 to 90\u00b0. No significant differences between the genders or types of malocclusion were found.\"\nQuestion:\n\"Can bone thickness and inter-radicular space affect miniscrew placement in posterior mandibular sites?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16735905": {
                "source": [
                    "\"To investigate polysomnographic and anthropomorphic factors predicting need of high optimal continuous positive airway pressure (CPAP).\nRetrospective data analysis.\nThree hundred fifty-three consecutive obstructive sleep apnea (OSA) patients who had a successful manual CPAP titration in our sleep disorders unit.\nThe mean optimal CPAP was 9.5 +/- 2.4 cm H2O. The optimal CPAP pressure increases with an increase in OSA severity from 7.79 +/- 2.2 in the mild, to 8.7 +/- 1.8 in the moderate, and to 10.1 +/- 2.3 cm H2O in the severe OSA group. A high CPAP was defined as the mean + 1 standard deviation (SD;>or =12 cm H2O). The predictor variables included apnea-hypopnea index (AHI), age, sex, body mass index (BMI), Epworth Sleepiness Scale (ESS), and the Multiple Sleep Latency Test (MSLT). High CPAP was required in 2 (6.9%), 6 (5.8%), and 63 (28.6%) patients with mild, moderate, and severe OSA, respectively. On univariate analysis, AHI, BMI, ESS score, and the proportion of males were significantly higher in those needing high CPAP. They also have a lower MSLT mean. On logistic regression, the use of high CPAP was 5.90 times more frequent (95% confidence interval 2.67-13.1) in severe OSA patients after adjustment for the other variables. The area under the receiver operator curve was 72.4%, showing that the model was adequate.\"\nQuestion:\n\"Does the severity of obstructive sleep apnea predict patients requiring high continuous positive airway pressure?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18801797": {
                "source": [
                    "\"Fruit and vegetables are protective of a number of chronic diseases; however, their intakes have been shown to vary by socioeconomic position (SEP). Household and food shopping environmental factors are thought to contribute to these differences. To determine whether household and food shopping environmental factors are associated with fruit and vegetable (FV) intakes, and contribute to socioeconomic inequalities in FV consumption.\nCross-sectional data were obtained by a postal questionnaire among 4333 adults (23-85 years) living in 168 neighbourhoods in the south-eastern Netherlands. Participants agreed/disagreed with a number of statements about the characteristics of their household and food shopping environments, including access, prices and quality. Education was used to characterise socioeconomic position (SEP). Main outcome measures were whether or not participants consumed fruit or vegetables on a daily basis. Multilevel logistic regression models examined between-area variance in FV consumption and associations between characteristics of the household and food shopping environments and FV consumption.\nOnly a few household and food shopping environmental factors were significantly associated with fruit and vegetable consumption, and their prevalence was low. Participants who perceived FV to be expensive were more likely to consume them. There were significant socioeconomic inequalities in fruit and vegetable consumption (ORs of not consuming fruit and vegetables were 4.26 and 5.47 among the lowest-educated groups for fruit and vegetables, respectively); however, these were not explained by any household or food shopping environmental factors.\"\nQuestion:\n\"Household and food shopping environments: do they play a role in socioeconomic inequalities in fruit and vegetable consumption?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28143468": {
                "source": [
                    "\"Prior literature identified the use of Performance Measurement Systems (PMS) as crucial in addressing improved processes of care. Moreover, a strategic use of PMS has been found to enhance quality, compared to non-strategic use, although a clear understanding of this linkage is still to be achieved. This paper deals with the test of direct and indirect models related to the link between the strategic use of PMS and the level of improved processes in health care organizations. Indirect models were mediated by the degree of perceived managerial discretion.\nA PLS analysis on a survey of 97 Italian managers working for health care organizations in the Lombardy region was conducted. The response rate was 77.6%.\nThe strategic use of PMS in health care organizations directly and significantly (p\u2009<\u20090.001) enhances performance in terms of improved processes. Perceived managerial discretion is positively and significantly (p\u2009<\u20090.001) affected by the strategic use of PMS, whereas the mediation effect is non-significant.\"\nQuestion:\n\"Are performance measurement systems useful?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16772913": {
                "source": [
                    "\"This study provides the first large-volume (1000 implant) comparison of the deflation rates of Poly Implant Prosthesis prefilled textured saline breast implants versus a control group of Mentor Siltex textured saline implants.\nA consecutive series of 500 Poly Implant Prosthesis prefilled textured saline breast implants was compared with a consecutive series of 500 Mentor Siltex breast implants. Each breast implant was evaluated for a 4-year period, and the annual deflation rate (number of deflations during a given year divided by the total number of implants) and cumulative deflation rate (cumulative total of deflations through a given year divided by the total number of implants) were recorded. Statistical significance was calculated using the Fisher's exact test at year 1 and the chi-square analysis at years 2 through 4.\nThe cumulative deflation rates of the Poly Implant Prosthesis implants was as follows: year 1, 1.2 percent; year 2, 5.6 percent; year 3, 11.4 percent; and year 4, 15.4 percent. The cumulative deflation rates of the Mentor implants was: year 1, 0.2 percent; year 2, 0.6 percent; year 3, 1.6 percent; and year 4, 4.4 percent. At year 1, the difference between deflation rates was not statistically significant (Fisher's exact test, p>0.05). However, at year 2 (chi-square, 13.29; p<0.001), year 3 (chi-square, 37.91; p<0.001), and year 4 (chi-square, 32.69; p<0.001), the difference was statistically significant.\"\nQuestion:\n\"A comparison of 500 prefilled textured saline breast implants versus 500 standard textured saline breast implants: is there a difference in deflation rates?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19653482": {
                "source": [
                    "\"The present study investigated factors that explain when and why different groups of teammates are more likely to request and accept backup from one another when needed in an environment characterized by extreme time pressure and severe consequences of error: commercial air traffic control (ATC).\nTransactive memory theory states that teammates develop consensus regarding the distribution of their relative expertise as well as confidence in that expertise over time and that this facilitates coordination processes. The present study investigated whether this theory could help to explain between-team differences in requesting and accepting backup when needed.\nThe present study used cross-sectional data collected from 51 commercial ATC teams. Hypotheses were tested using multiple regression analysis.\nTeammates with greater experience working together requested and accepted backup from one another more than those with lesser experience working together. Teammate knowledge consensus and perceived team efficacy appear to have mediated this relationship.\"\nQuestion:\n\"Do familiar teammates request and accept more backup?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12963175": {
                "source": [
                    "\"It was the aim of the present study to elaborate criteria for the assessment of rapid hemodynamic progression of valvar aortic stenosis. These criteria are of special importance when cardiac surgery is indicated for other reasons but the established criteria for aortic valve replacement are not yet fulfilled. Such aspects of therapeutic planing were mostly disregarded in the past so that patients had to undergo cardiac reoperation within a few years.\nHemodynamic, echocardiographic, and clinical data of 169 men and 88 women with aortic stenosis, aged 55.2 +/- 15.7 years at their first and 63.4 +/- 15.6 years at their second cardiac catheterization, were analyzed.\nThe progression rate of aortic valve obstruction was found to be dependent on the degree of valvar calcification ([VC] scoring 0 to III) and to be exponentially correlated with the aortic valve opening area (AVA) at initial catheterization. Neither age nor sex of the patient nor etiology of the valvar obstruction significantly influence the progression of aortic stenosis. If AVA decreases below 0.75 cm(2) with a present degree of VC = 0, or AVA of 0.8 with VC of I, AVA of 0.9 with VC of II, or AVA of 1.0 with VC of III, it is probable that aortic stenosis will have to be operated upon in the following years.\"\nQuestion:\n\"Can progression of valvar aortic stenosis be predicted accurately?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23453079": {
                "source": [
                    "\"To determine whether prostate morphology or technique used has any effect on postoperative outcomes after holmium laser enucleation of the prostate.\nA retrospective review of prospectively collected data was completed for all patients undergoing a holmium laser enucleation of the prostate at our institution. Prostate morphology was classified as either \"bilobar\" or \"trilobar\" according to the cystoscopic appearance. The baseline characteristics, complications, and postoperative outcomes were collected.\nA total of 304 patients with either \"bilobar\" (n\u00a0= 142) or \"trilobar\" (n\u00a0= 162) prostate morphology were included. The trilobar group was more likely to have longer operative times (112 vs 100 minutes, P\u00a0= .04), although this difference was not significant on multivariate analysis. The postoperative outcomes were similar between the 2 groups for American Urological Association symptom score, change in American Urological Association symptom score, bother score, maximal flow rate, change in maximal flow rate, postvoid residual urine volume, and complication rate. However, the trilobar group had a significantly greater decrease in their PVR urine volume (296 vs 176 mL, P\u00a0= .01), a difference that persisted on multivariate analysis. A subset analysis of the trilobar prostates revealed that performing a 2-lobe technique achieved shorter operative and enucleation times, although the difference was not significant.\"\nQuestion:\n\"Does prostate morphology affect outcomes after holmium laser enucleation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9767546": {
                "source": [
                    "\"Advanced glycation end products (AGEs), formed by non-enzymatic glycation and oxidation (glycoxidation) reactions, have been implicated in the pathogenesis of several diseases, including normoglycemic uremia. AGE research in uremia has focused on the accumulation of carbohydrate-derived adducts generated by the Maillard reaction. Recent studies, however, have demonstrated that one AGE, the glycoxidation product carboxymethyllysine (CML), could be derived not only from carbohydrates but also from oxidation of polyunsaturated fatty acids in vitro, raising the possibility that both carbohydrate and lipid autoxidation might be increased in uremia.\nTo address this hypothesis, we applied gas chromatography-mass spectrometry and high performance liquid chromatography to measure protein adducts formed in uremic plasma by reactions between carbonyl compounds and protein amino groups: pentosidine derived from carbohydrate-derived carbonyls, malondialdehyde (MDA)-lysine derived from lipid-derived carbonyls, and CML originating possibly from both sources.\nAll three adducts were elevated in uremic plasma. Plasma CML levels were mainly (>95%) albumin bound. Their levels were not correlated with fructoselysine levels and were similar in diabetic and non-diabetic patients on hemodialysis, indicating that their increase was not driven by glucose. Pentosidine and MDA-lysine were also increased in plasma to the same extent in diabetic and non-diabetic hemodialysis patients. Statistical analysis indicated that plasma levels of CML correlated weakly (P<0.05) with those of pentosidine and MDA-lysine, but that pentosidine and MDA-lysine varied independently (P>0.5).\"\nQuestion:\n\"Autoxidation products of both carbohydrates and lipids are increased in uremic plasma: is there oxidative stress in uremia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10354335": {
                "source": [
                    "\"Health care delivery has undertaken a major shift from inpatient management to ambulatory surgical care with increasing emphasis on quality assurance (QA) processes. Educational opportunities for medical undergraduate programmes are being sought in the day surgery environment. Our study was undertaken to explore ways in which senior medical students can actively contribute to QA processes as part of an undergraduate day surgery educational programme.\nHealth care delivery has undertaken a major shift from inpatient management to ambulatory surgical care with increasing emphasis on quality assurance (QA) processes. Educational opportunities for medical undergraduate programmes are being sought in the day surgery environment. Our study was undertaken to explore ways in which senior medical students can actively contribute to the QA processes as part of an undergraduate day surgery educational programme.\nFifty-nine final year medical students followed allocated patients with common surgical conditions through all phases of the day surgery process. Students kept records about each case in a log book and also presented their cases at weekly Problem Based Learning tutorials. An audit of student log books and review of tutorial records was conducted for the 1996 and 1997 academic years, in order to evaluate student contribution to QA.\nStudents followed 621 cases, representing a sampling of 14. 1% day surgery cases. Categories of problems highlighted by students included inappropriate patient and procedure selection, inadequate pain management, discharge, communication and resource issues. Students made a number of recommendations including the development of multilingual videotapes and patient information sheets for non-English speaking patients, avoidance of bilateral surgical procedures and improved links with local medical officers. They also developed new guidelines and protocols.\"\nQuestion:\n\"Can medical students contribute to quality assurance programmes in day surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23025584": {
                "source": [
                    "\"That alcohol consumption is strongly influenced by the drinking behavior of social company has been demonstrated in observational research. However, not everyone is equally vulnerable to other people's drinking, and it is important to unravel which factors underlie these individual differences. This study focuses on the role of psychosocial stress in attempting to explain individual differences in the propensity to imitate alcohol consumption.\nWith a 2 (confederate's drinking condition: alcohol vs. soda) \u00d7 2 (participant's stress condition: stress vs. no stress) experimental design, we tested whether the tendency to imitate other people's drinking was related to participants' induced stress levels. The young male adults (N = 106) were randomly assigned to each of the conditions. In each session, directly after the stress or no-stress period, confederates and participants entered a bar laboratory where we observed their drinking behavior. Prior to entering the session, confederates were instructed to drink alcohol or soda.\nParticipants in both stress and no-stress conditions consumed substantially more alcohol when confederates drank alcohol than when they drank soda. There was no difference in alcohol consumed between stress and no-stress conditions. No moderating effect of stress on the tendency to drink along with peers was found.\"\nQuestion:\n\"Does stress increase imitation of drinking behavior?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16968183": {
                "source": [
                    "\"This study reviewed the results of performing day case laparoscopic cholecystectomy to assess the feasibility and safety of the procedure as a day case.\nThis is a prospective study of 150 day case laparoscopic cholecystectomies performed between September 1999 and December 2004 under the care of the senior author. The results of a follow-up questionnaire to assess post-discharge clinical course and patient satisfaction were analyzed. All patients had commenced eating and drinking and were fully mobile before discharge home. The length of hospital stay was 4-8 hours.\nThe mean age of the patients was 43 years; 134 patients had an American Society of Anesthesiologists grade I, the remaining 16 patients were grade II. The mean operative time was 41 minutes. There were no conversions to open procedures. There was no bleeding, no visceral injury, and no mortality. There was one admission directly from the day surgical unit (admission rate of 0.6%), but no readmission following discharge. No patients were admitted due to postoperative nausea or pain. Ninety-nine (66%) of 150 patients responded to our questionnaire: 97% were satisfied about the information they had received. Patients rated their satisfaction with the procedure as follows: 75% excellent, 21% good, 3% satisfied, and 1 patient un-satisfied. Ninety-four percent of the patients would recommend the procedure as a day case.\"\nQuestion:\n\"Is laparoscopic cholecystectomy safe and acceptable as a day case procedure?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27908583": {
                "source": [
                    "\"To report the outcomes of surgical treatment of lower limb fractures in patients with chronic spinal cord injuries.\nA total of 37 lower limb fractures were treated from 2003 to 2010, of which 25 fractures were treated surgically and 12 orthopaedically.\nPatients of the surgical group had better clinical results, range of motion, bone consolidation, and less pressure ulcers and radiological misalignment. No differences were detected between groups in terms of pain, hospital stay, and medical complications.\nThere is no currently consensus regarding the management of lower limb fractures in patients with chronic spinal cord injuries, but the trend has been conservative treatment due to the high rate of complications in surgical treatment.\"\nQuestion:\n\"Should lower limb fractures be treated surgically in patients with chronic spinal injuries?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27818079": {
                "source": [
                    "\"This study evaluated the outcomes and complications of the surgical treatment of condylar fractures by the retromandibular transparotid approach. The authors hypothesized that such an approach would be safe and reliable for the treatment of most condylar fractures.\nA retrospective evaluation of patients who underwent surgical reduction of a condylar fracture from January 2012 to December 2014 at the Clinic of Dentistry and Maxillofacial Surgery of the University Hospital of Verona (Verona, Italy) was performed. Inclusion criteria were having undergone surgical treatment of condylar fractures with a retromandibular transparotid approach and the availability of computed tomograms of the preoperative and postoperative facial skeleton with a minimum follow-up of 1\u00a0year. Static and dynamic occlusal function, temporomandibular joint health status, presence of neurologic impairments, and esthetic outcomes were evaluated in all patients.\nThe sample was composed of 25 patients. Preinjury occlusion and temporomandibular joint health were restored in most patients. Esthetic outcomes were deemed satisfactory by clinicians and patients. Neither permanent neurologic impairments nor major postoperative complications were observed.\"\nQuestion:\n\"Is the Retromandibular Transparotid Approach a Reliable Option for the Surgical Treatment of Condylar Fractures?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22540518": {
                "source": [
                    "\"To analyze the reliability of micro-computed tomography (micro-CT) to assess bone density and the microstructure of the maxillary bones at the alveolar process in human clinics by direct comparison with conventional stereologic-based histomorphometry.\nAnalysis of osseous microstructural variables including bone volumetric density (BV/TV) of 39 biopsies from the maxillary alveolar bone was performed by micro-CT. Conventional stereologic-based histomorphometry of 10 bone biopsies was performed by optic microscopy (OM) and low-vacuum surface electronic microscopy (SEM). Percentages of bone between micro-CT and conventional stereologic-based histomorphometry were compared.\nSignificant positive correlations were observed between BV/TV and the percentage of bone (%Bone) analyzed by SEM (r\u00a0=\u00a00.933, P\u00a0<\u00a00.001), by toluidine blue staining OM (r\u00a0=\u00a00.950, P\u00a0<\u00a00.001) and by dark field OM (r\u00a0=\u00a00.667, P\u00a0=\u00a00.05). The high positive correlation coefficient between BV/TV and trabecular thickness illustrates that a value of BV/TV upper than 50% squares with a bone presenting most of their trabecules thicker than 0.2\u00a0mm. The high negative correlation between BV/TV and trabecular separation shows that values of BV/TV upper than 50% squares with a bone presenting most of their trabecules separated less than 0.3\u00a0mm each other.\"\nQuestion:\n\"Is micro-computed tomography reliable to determine the microstructure of the maxillary alveolar bone?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12407608": {
                "source": [
                    "\"To investigate whether prepuncture ultrasound evaluation of vascular anatomy facilitates internal jugular vein cannulation compared with landmark-guided puncture.\nProspective randomized study.\nSingle community hospital.\nAdult patients undergoing general anesthesia (n = 240).\nThe right internal jugular vein was cannulated using either anatomic landmarks or prepuncture ultrasound (3.75/7.5 MHz) guidance. In the landmark group, respiratory jugular venodilation was used as the primary landmark for locating the vein. Results of cannulation and the incidence of complications were compared.\nPatients were randomly assigned to the ultrasound or landmark group. Respiratory jugular venodilation was identified in 188 patients (78.3%), in whom results of cannulation did not differ between the 2 techniques with respect to the venous access rate (cannulated at the first attempt: 83.5% in the landmark v 85.7% in the ultrasound group), the success rate (cannulated within 3 attempts: 96.9% v 95.6%), and the incidence of arterial puncture (1.0% v 3.3%). In the remaining 52 respiratory jugular venodilation-unidentified patients, the access rate (30.4% v 86.2%, p<0.001) and the success rate (78.3 v 100%, p<0.05) were significantly better in the ultrasound group, and no arterial puncture was recorded in the ultrasound group, whereas the incidence was 13.0% in the landmark group. The results were similar regardless of the ultrasound frequency used.\"\nQuestion:\n\"Does ultrasound imaging before puncture facilitate internal jugular vein cannulation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24747511": {
                "source": [
                    "\"Interference from irrelevant negative material might be a key mechanism underlying intrusive ruminative thoughts in depression. Considering commonalities between depression and social anxiety and the presence of similar intrusive thoughts in social anxiety, the current study was designed to assess whether interference from irrelevant material in working memory is specific to depression or is also present in social anxiety disorder.\nTo examine the effects of irrelevant emotional material on working memory performance, participants memorized two lists of words on each trial and were subsequently instructed to ignore one of the lists. Participants were then asked to indicate whether a probe word belonged to the relevant list or not.\nCompared to control and social anxiety groups, the depression groups (both pure and comorbid with social anxiety disorder) exhibited greater difficulties removing irrelevant emotional material from working memory (i.e., greater intrusion effects). Greater intrusion effects were also associated with increased rumination.\nAlthough we included three clinical groups (depression, social anxiety, and the comorbid groups), the results are based on a relatively small number of participants.\"\nQuestion:\n\"Updating emotional content in working memory: a depression-specific deficit?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17008699": {
                "source": [
                    "\"Examine whether patients with prostate cancer choose the more aggressive of two radiotherapeutic options, whether this choice is reasoned, and what the determinants of the choice are.\nOne hundred fifty patients with primary prostate cancer (T(1-3)N(0)M(0)) were informed by means of a decision aid of two treatment options: radiotherapy with 70 Gy versus 74 Gy. The latter treatment is associated with more cure and more toxicity. The patients were asked whether they wanted to choose, and if so which treatment they preferred. They also assigned importance weights to the probability of various outcomes, such as survival, cure and adverse effects. Patients who wanted to choose their own treatment (n = 119) are described here.\nThe majority of these patients (75%) chose the lower radiation dose. Their choice was highly consistent (P<or = .001), with the importance weights assigned to the probability of survival, cure (odds ratio [OR] = 6.7 and 6.9) and late GI and genitourinary adverse effects (OR = 0.1 and 0.2). The lower dose was chosen more often by the older patients, low-risk patients, patients without hormone treatment, and patients with a low anxiety or depression score.\"\nQuestion:\n\"Do patients with localized prostate cancer treatment really want more aggressive treatment?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24516646": {
                "source": [
                    "\"The ImmunoCAP ISAC 112 is a fluoro-immunoassay that allows detection of specific IgE to 112 molecular components from 51 allergenic sources. We studied the reliability of this technique intra- and inter- assay, as well as inter-batch- and inter-laboratory-assay.\nTwenty samples were studied, nineteen sera from polysensitized allergic patients, and the technique calibrator provided by the manufacturer (CTR02). We measured the sIgE from CTR02 and three patients' sera ten times in the same and in different assays. Furthermore, all samples were tested in two laboratories and with two batches of ISAC kit. To evaluate the accuracy of ISAC 112, we contrasted the determinations of CTR02 calibrator with their expected values by T Student test. To analyse the precision, we calculated the coefficient of variation (CV) of the 15 allergens that generate the calibration curve, and to analyse the repeatability and the reproducibility, we calculated the intraclass coefficient correlation (ICC) to each allergen.\nThe results obtained for CTR02 were similar to those expected in 7 of 15 allergens that generate the calibration curve, whereas in 8 allergens the results showed significant differences. The mean CV obtained in the CTR02 determinations was of 9.4%, and the variability of sera from patients was of 22.9%. The agreement in the intra- and inter-assay analysis was very good to 94 allergens and good to one. In the inter-batch analyse, we obtained a very good agreement to 82 allergens, good to 14, moderate to 5 allergens, poor to one, and bad to 1 allergen. In the inter-laboratory analyse, we obtained a very good agreement to 73 allergens, good to 22, moderate to 6 and poor to two allergens.\"\nQuestion:\n\"Is the determination of specific IgE against components using ISAC 112 a reproducible technique?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16100194": {
                "source": [
                    "\"Angiotensin-converting enzyme inhibitors (ACE-I) are considered safe, but they are associated with characteristic side effects, namely cough and angioedema, usually requiring discontinuation. We perceived that referrals for these side effects have become more and more frequent; therefore, we evaluated the degree of knowledge on the safety of ACE-I in different medical categories.\nA questionnaire (13 questions) on side effects of ACE-I was posted to physicians.\nEveryday clinical practice.\nCardiologists, allergists, and general practitioners (GPs) from the National Healthcare System.\nThree hundred twelve physicians were contacted, and 154 returned questionnaires that could be analyzed. Of the 154 physicians (mean age, 45 years) 48 were cardiologists, 52 were GPs, and 54 were allergists. The percentage of correct answers was low: 31.9% for cardiologists, 40% for GPs, and 33% for allergists. Thus, GPs provided a significantly higher percentage of correct answers with respect to the remaining categories (p = 0.05). The lower rate of correct answers (0 to 15.9%) concerned the time of onset of cough and the action to take. Cardiologists seemed to be less aware of the fact that angiotensin receptor blockers (sartans) can cross-react with ACE-I.\"\nQuestion:\n\"Are physicians aware of the side effects of angiotensin-converting enzyme inhibitors?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10877371": {
                "source": [
                    "\"Phacodonesis can occur in pseudoexfoliation syndrome because of impaired zonular support. This study investigates whether the increased mobility of the lens influences anterior chamber depth in patients with pseudoexfoliation while assuming a prone position.\nCentral anterior chamber depth was measured in 39 patients with clinically apparent unilateral pseudoexfoliation and elevated intraocular pressure. Patients were placed in a face-up position for 5 minutes, at which time anterior chamber depth and axial length were measured by A scan, and intraocular pressure was measured by Tonopen (Oculab, La Jolla, CA) in both eyes. The measurements were repeated on both eyes after 5 minutes in a face-down position.\nNo significant differences in intraocular pressure or axial length between the prone and supine positions were found in either eye. Anterior chamber depth in eyes with pseudoexfoliation decreased from a mean of 3.08 mm in the supine position to a mean of 2.95 mm in the prone position, whereas mean anterior chamber depth in the fellow eyes decreased from 3.01 mm to 2.97 mm. The decrease in anterior chamber depth when facing down in the eyes with pseudoexfoliation was significantly greater than in the fellow eyes.\"\nQuestion:\n\"Does head positioning influence anterior chamber depth in pseudoexfoliation syndrome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22683044": {
                "source": [
                    "\"Some studies suggest that open access articles are more often cited than non-open access articles. However, the relationship between open access and citations count in a discipline such as intensive care medicine has not been studied to date. The present article analyzes the effect of open access publishing of scientific articles in intensive care medicine journals in terms of citations count.\nWe evaluated a total of 161 articles (76% being non-open access articles) published in Intensive Care Medicine in the year 2008. Citation data were compared between the two groups up until April 30, 2011. Potentially confounding variables for citation counts were adjusted for in a linear multiple regression model.\nThe median number (interquartile range) of citations of non-open access articles was 8 (4-12) versus 9 (6-18) in the case of open access articles (p=0.084). In the highest citation range (>8), the citation count was 13 (10-16) and 18 (13-21) (p=0.008), respectively. The mean follow-up was 37.5 \u00b1 3 months in both groups. In the 30-35 months after publication, the average number (mean \u00b1 standard deviation) of citations per article per month of non-open access articles was 0.28 \u00b1 0.6 versus 0.38 \u00b1 0.7 in the case of open access articles (p=0.043). Independent factors for citation advantage were the Hirsch index of the first signing author (\u03b2=0.207; p=0.015) and open access status (\u03b2=3.618; p=0.006).\"\nQuestion:\n\"Does open access publishing increase the impact of scientific articles?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22955530": {
                "source": [
                    "\"The range of injury severity that can be seen within the category of type II supracondylar humerus fractures (SCHFs) raises the question whether some could be treated nonoperatively. However, the clinical difficulty in using this approach lies in determining which type II SCHFs can be managed successfully without a surgical intervention.\nWe reviewed clinical and radiographic information on 259 pediatric type II SCHFs that were enrolled in a prospective registry of elbow fractures. The characteristics of the patients who were treated without surgery were compared with those of patients who were treated surgically. Treatment outcomes, as assessed by the final clinical and radiographic alignment, range of motion of the elbow, and complications, were compared between the groups to define clinical and radiographic features that related to success or failure of nonoperative management.\nDuring the course of treatment, 39 fractures were found to have unsatisfactory alignment with nonoperative management and were taken for surgery. Ultimately, 150 fractures (57.9%) were treated nonoperatively, and 109 fractures (42.1%) were treated surgically. At final follow-up, outcome measures of change in carrying angle, range of motion, and complications did not show clinically significant differences between treatment groups. Fractures without rotational deformity or coronal angulation and with a shaft-condylar angle of>15 degrees were more likely to be associated with successful nonsurgical treatment. A scoring system was developed using these features to stratify the severity of the injury. Patients with isolated extension deformity, but none of the other features, were more likely to complete successful nonoperative management.\"\nQuestion:\n\"Type II supracondylar humerus fractures: can some be treated nonoperatively?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12145243": {
                "source": [
                    "\"Type 2 diabetes may be present for several years before diagnosis, by which time many patients have already developed diabetic complications. Earlier detection and treatment may reduce this burden, but evidence to support this approach is lacking.\nGlycemic control and clinical and surrogate outcomes were compared for 5,088 of 5,102 U.K. Diabetes Prospective Study participants according to whether they had low (<140 mg/dl [<7.8 mmol/l]), intermediate (140 to<180 mg/dl [7.8 to<10.0 mmol/l]), or high (>or =180 mg/dl [>or =10 mmol/l]) fasting plasma glucose (FPG) levels at diagnosis. Individuals who presented with and without diabetic symptoms were also compared.\nFewer people with FPG in the lowest category had retinopathy, abnormal biothesiometer measurements, or reported erectile dysfunction. The rate of increase in FPG and HbA(1c) during the study was identical in all three groups, although absolute differences persisted. Individuals in the low FPG group had a significantly reduced risk for each predefined clinical outcome except stroke, whereas those in the intermediate group had significantly reduced risk for each outcome except stroke and myocardial infarction. The low and intermediate FPG groups had a significantly reduced risk for progression of retinopathy, reduction in vibration sensory threshold, or development of microalbuminuria.\"\nQuestion:\n\"Are lower fasting plasma glucose levels at diagnosis of type 2 diabetes associated with improved outcomes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24666444": {
                "source": [
                    "\"The \"July effect\" refers to the phenomenon of adverse impacts on patient care arising from the changeover in medical staff that takes place during this month at academic medical centres in North America. There has been some evidence supporting the presence of the July effect, including data from surgical specialties. Uniformity of care, regardless of time of year, is required for patients undergoing major cancer surgery. We therefore sought to perform a population-level assessment for the presence of a July effect in this field.\nWe used the Nationwide Inpatient Sample to abstract data on patients undergoing 1 of 8 major cancer surgeries at academic medical centres between Jan. 1, 1999, and Dec. 30, 2009. The primary outcomes examined were postoperative complications and in-hospital mortality. Univariate analyses and subsequently multivariate analyses, controlling for patient and hospital characteristics, were performed to identify whether the time of surgery was an independent predictor of outcome after major cancer surgery.\nOn univariate analysis, the overall postoperative complication rate, as well as genitourinary and hematologic complications specifically, was higher in July than the rest of the year. However, on multivariate analysis, only hematologic complications were significantly higher in July, with no difference in overall postoperative complication rate or in-hospital mortality for all 8 surgeries considered separately or together.\"\nQuestion:\n\"Is there any evidence of a \"July effect\" in patients undergoing major cancer surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18005142": {
                "source": [
                    "\"To explore whether early treatment would shorten the duration of headache from headache onset to its peak and reduce headache severity at peak.\nPrior clinical studies almost exclusively focused on headache relief after dosing. No data are available on whether early intervention affects the duration from headache onset to peak and headache severity at peak.\nAdult migraineurs were enrolled in this observational study from multi-site headache clinics. Patients recorded their migraine experiences via an electronic diary over 1 month. Patients reported the time and pain severity at onset, dosing, and peak. We used a linear mixed model to evaluate the impact of the timing of treatment and to adjust for covariates and correlation of observations within subjects.\nA total of 182 patients reported 970 migraine episodes, 620 of which were treated before headaches progressed to peak. Mean time from headache onset to peak varied from 1.9 hours to 8.9 hours for patients treated within 15 minutes of onset and those who waited for 4 or more hours, respectively. However, early intervention was not associated with reduced headache severity at peak. In multivariate analysis, early treatment, use of triptans, and mild migraine headache in the past 3 months were significantly associated with shorter time from onset to headache peak. A separate model indicated that the timing of medication was not associated with the duration between dosing and headache peak, but use of triptans shortened the time from dosing to headache peak.\"\nQuestion:\n\"Does early migraine treatment shorten time to headache peak and reduce its severity?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15919266": {
                "source": [
                    "\"The criteria for administration of adjuvant radiation therapy after thymoma resection remains controversial, and it is unclear whether patients with Masaoka stage III thymoma benefit from adjuvant radiation. The goal of this report was to determine whether or not this group benefits from radiation therapy in disease-specific survival and disease-free survival.\nCase records of the Massachusetts General Hospital were retrospectively reviewed from 1972 to 2004. One hundred and seventy-nine patients underwent resection for thymoma, of which 45 had stage III disease.\nForty-five stage III patients underwent resection and in 36 it was complete. Thirty-eight stage III patients received radiation therapy. Baseline prognostic factors between radiated and nonradiated groups were similar. The addition of adjuvant radiotherapy did not alter local or distant recurrence rates in patients with stage III thymoma. Disease-specific survival at 10 years in stage III patients who did not receive radiation was 75% (95% confidence interval, 32% to 100%) and in patients who did receive radiation therapy it was 79% (95% confidence interval, 64% to 94%) (p = 0.21). The most common site of relapse was the pleura.\"\nQuestion:\n\"Adjuvant radiation of stage III thymoma: is it necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16538201": {
                "source": [
                    "\"To determine whether the use of hydrophilic guidewires has increased the technical success rate of peripheral percutaneous transluminal angioplasty (PTA).MATERIAL/\nWe performed 125 procedures and analyzed the technical success rates of PTA using the conventional guidewire first and then if needed, the hydrophilic guidewire for iliac and SFA stenoses or occlusions. Angioplasty was performed on 25 stenosed, 25 occluded iliac arteries and 25 stenosed, 50 occluded femoral arteries. The result was defined as technical success when the lesion was crossed by a guidewire and balloon, then it was dilated with restoration of vessel lumen and less than 30% residual stenosis and the rise in ABI values was at least 0.15 after 24 hours.\nThe technical success rate after PTA of stenosed iliac arteries was achieved in 96% (24/25) using conventional wires and 100% using hydrophilic guidewire; in iliac occlusions, the rates were 60% (15/25) and 96%, respectively; in femoral stenosis - 84% (21/25) and 100%; in occlusions in the first group: lesions<10 cm -64% (16/25) and 96%, in the second group: lesions>10 cm -48% (12/25) and 88%. In the iliac group, there was no significant difference in the success of iliac stenosis PTA. However, there were significant changes in the success rates of PTA performed for SFA stenosis and iliac and SFA occlusions.\"\nQuestion:\n\"Does use of hydrophilic guidewires significantly improve technical success rates of peripheral PTA?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15475728": {
                "source": [
                    "\"The apparent favorable effect of alcohol on the risk of acute myocardial infarction (MI) may be related to its hypoinsulinemic effect when consumed with meals. We studied how the timing of alcohol consumption in relation to meals might affect the risk of MI in a population with relatively high regular alcohol consumption.\nWe conducted a case-control study between 1995 and 1999 in Milan, Italy. Cases were 507 subjects with a first episode of nonfatal acute MI, and controls were 478 patients admitted to hospitals for other acute diseases. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by multiple logistic regression models.\nCompared with nondrinkers, an inverse trend in risk was observed when alcohol was consumed during meals only (for>or =3 drinks per day: OR = 0.50; 95% CI = 0.30-0.82). In contrast, no consistent trend in risk was found for subjects drinking outside of meals (for>or =3 drinks per day: 0.98; 0.49-1.96). The pattern of risk was similar when we considered people who drank only wine.\"\nQuestion:\n\"Alcohol consumption and acute myocardial infarction: a benefit of alcohol consumed with meals?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17445978": {
                "source": [
                    "\"To evaluate renal damage and impairment of renal function 1 yr after laparoscopic partial nephrectomy (LPN) with warm ischemia>30 min.\nFrom July 2004 to June 2005, 18 patients underwent LPN with warm ischemia time>30 min. Kidney damage markers (daily proteinuria and tubular enzymes) and renal function (serum creatinine, cystatin C, and creatinine clearances) were assessed on postoperative days 1 and 5 and at 12 mo. Glomerular filtration rate (GFR) was evaluated before surgery and at 3 mo. Renal scintigraphy was performed before the procedure, at 5 d and at 3 and 12 mo postoperatively. Statistical analysis was performed using the Student t test and logistic regression analysis.\nIn terms of kidney damage and renal function markers, the statistical analysis demonstrated that at 1 yr there was complete return to the normal range and no statistical difference between the values at the various time points. The GFR was not significantly different before and 3 mo after surgery. In terms of scintigraphy of the operated kidney, the values were 48.35+/-3.82% (40-50%) before the procedure, 36.88+/-8.42 (16-50%) on postoperative day 5 (p=0.0001), 40.56+/-8.96 (20-50%) at 3 mo (p=0.003), and 42.8+/-7.2% (20-50%) 1 yr after surgery (p=0.001).\"\nQuestion:\n\"Is renal warm ischemia over 30 minutes during laparoscopic partial nephrectomy possible?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17890090": {
                "source": [
                    "\"The aim of this study was to determine whether bone scans (BS) can be avoided if pelvis was included in CT thorax and abdomen to detect bony metastases from breast cancer.\nResults of 77 pairs of CT (thorax, abdomen, and pelvis) and BS in newly diagnosed patients with metastatic breast cancer (MBC) were compared prospectively for 12 months. Both scans were blindly assessed by experienced radiologists and discussed at multidisciplinary team meetings regarding the diagnosis of bone metastases.\nCT detected metastatic bone lesions in 43 (98%) of 44 patients with bone metastases. The remaining patient had a solitary, asymptomatic bony metastasis in shaft of femur. BS was positive in all patients with bone metastases. There were 11 cases of false positive findings on BS.\"\nQuestion:\n\"Can computerised tomography replace bone scintigraphy in detecting bone metastases from breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18041059": {
                "source": [
                    "\"Despite the advantages from using aromatase inhibitors (AIs) compared with tamoxifen for early breast cancer, an unexpectedly greater number of grade 3 and 4 cardiovascular events (CVAE) (as defined by National Cancer Institute of Canada-Common Toxicity Criteria [version 2.0] was demonstrated.\nPhase 3 randomized clinical trials (RCTs) comparing AI with tamoxifen in early breast cancer were considered eligible for this review. The event-based risk ratios (RRs) with 95% confidence intervals (95% CIs) were derived, and a test of heterogeneity was applied. Finally, absolute differences (ADs) in event rates and the number of patients needed to harm 1 patient (NNH) were determined.\nSeven eligible RCTs (19,818 patients) reported CVAE results. When considering all RCTs, the AD of the primary endpoint (CVAE) between the 2 arms (0.52%), tamoxifen versus AI, was statistically significant (RR, 1.31; 95% CI, 1.07-1.60; P= .007). This translated into an NNH value of 189 patients; when only third-generation AIs were considered, the difference (0.57%) remained significant (RR, 1.34; 95% CI, 1.09-1.63; P= .0038). Thromboembolic events were significantly more frequent in the tamoxifen arm, regardless of the strategy adopted (RR, 0.53; 95% CI, 0.42-0.65; P<.0001), without significant heterogeneity (P= .21). An AD of 1.17% and an NNH value of 85 patients were observed.\"\nQuestion:\n\"Do adjuvant aromatase inhibitors increase the cardiovascular risk in postmenopausal women with early breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24977765": {
                "source": [
                    "\"Concussions are commonly diagnosed in pediatric patients presenting to the emergency department (ED). The primary objective of this study was to evaluate compliance with ED discharge instructions for concussion management.\nA prospective cohort study was conducted from November 2011 to November 2012 in a pediatric ED at a regional Level 1 trauma center, serving 35,000 pediatric patients per year. Subjects were aged 8 years to 17 years and were discharged from the ED with a diagnosis of concussion. Exclusion criteria included recent (past 3 months) diagnosis of head injury, hospital admission, intracranial injury, skull fracture, suspected nonaccidental trauma, or preexisting neurologic condition. Subjects were administered a baseline survey in the ED and were given standardized discharge instructions for concussion by the treating physician. Telephone follow-up surveys were conducted at 2 weeks and 4 weeks after ED visit.\nA total of 150 patients were enrolled. The majority (67%) of concussions were sports related. Among sports-related concussions, soccer (30%), football (11%), lacrosse (8%), and basketball (8%) injuries were most common. More than one third (39%) reported return to play (RTP) on the day of the injury. Physician follow-up was equivalent for sport and nonsport concussions (2 weeks, 58%; 4 weeks, 64%). Sports-related concussion patients were more likely to follow up with a trainer (2 weeks, 25% vs. 10%, p = 0.06; 4 weeks, 29% vs. 8%, p<0.01). Of the patients who did RTP or normal activities at 2 weeks (44%), more than one third (35%) were symptomatic, and most (58%) did not receive medical clearance. Of the patients who had returned to activities at 4 weeks (64%), less than one quarter (23%) were symptomatic, and most (54%) received medical clearance.\"\nQuestion:\n\"Are pediatric concussion patients compliant with discharge instructions?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24298614": {
                "source": [
                    "\"The clinical and prognostic value of the previous node classification of TNM staging in early gastric cancer (EGC) has been less definitive. The aim was to assess the suitability of the revised node staging for prediction of clinical behavior of EGC.\nBetween 2005 and 2008, 1,845 patients were diagnosed with EGC and underwent surgery at Severance Hospitals. Clinicopathological characteristics were analyzed with comparisons between sixth and seventh TNM staging.\nWhen comparing IB with IIA upstaged based on seventh staging, poor differentiation, signet ring cell, diffuse, undifferentiated types, perineural invasion (PNI), larger size and younger age, were more significantly associated with IIA. Clinicopathological factors were compared between N0/N1 and N2/N3 based on both staging. In mucosal cancer, younger age, diffuse and undifferentiated types were more significantly associated with N2/N3 based on seventh staging. In submucosal cancer, larger size, poor differentiation, signet ring cell, diffuse, undifferentiated types, PNI and deeper submucosal invasion, were more significantly associated with N2/N3 based on seventh staging.\"\nQuestion:\n\"Is the 7th TNM edition suitable for biological predictor in early gastric cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14968373": {
                "source": [
                    "\"Treatment of obstructive hydrocephalus in children with tuberculous meningitis (TBM) depends on the level of the cerebrospinal fluid (CSF) block. Air-encephalography is regarded as the gold standard for differentiating communicating and non-communicating hydrocephalus. Since air-encephalography involves a lumbar puncture, it carries the risk of cerebral herniation. AIM. The aim of this study was to determine whether communicating and non-communicating hydrocephalus in TBM can be differentiated by means of cranial computerised tomography (CT).\nA number of CT indices were measured in 50 children with communicating and 34 children with non-communicating hydrocephalus according to air-encephalographic findings.\nThe only CT finding that correlated with the type of hydrocephalus was the shape of the third ventricle. Significantly more children with non-communicating hydrocephalus had a rounded third ventricle than those with communicating hydrocephalus.\"\nQuestion:\n\"Can CT predict the level of CSF block in tuberculous hydrocephalus?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21361755": {
                "source": [
                    "\"Two common causes of cervical myelopathy include degenerative stenosis and ossification of the posterior longitudinal ligament (OPLL). It has been postulated that patients with OPLL have more complications and worse outcomes than those with degenerative stenosis. The authors sought to compare the surgical results of laminoplasty in the treatment of cervical stenosis with myelopathy due to either degenerative changes or segmental OPLL.\nThe authors conducted a retrospective review of 40 instrumented laminoplasty cases performed at a single institution over a 4-year period to treat cervical myelopathy without kyphosis. Twelve of these patients had degenerative cervical stenotic myelopathy ([CSM]; degenerative group), and the remaining 28 had segmental OPLL (OPLL group). The 2 groups had statistically similar demographic characteristics and number of treated levels (mean 3.9 surgically treated levels; p>0.05). The authors collected perioperative and follow-up data, including radiographic results.\nThe overall clinical follow-up rate was 88%, and the mean clinical follow-up duration was 16.4 months. The mean radiographic follow-up rate was 83%, and the mean length of radiographic follow-up was 9.3 months. There were no significant differences in the estimated blood loss (EBL) or length of hospital stay (LOS) between the groups (p>0.05). The mean EBL and LOS for the degenerative group were 206 ml and 3.7 days, respectively. The mean EBL and LOS for the OPLL group were 155 ml and 4 days, respectively. There was a statistically significant improvement of more than one grade in the Nurick score for both groups following surgery (p<0.05). The Nurick score improvement was not statistically different between the groups (p>0.05). The visual analog scale (VAS) neck pain scores were similar between groups pre- and postoperatively (p>0.05). The complication rates were not statistically different between groups either (p>0.05). Radiographically, both groups lost extension range of motion (ROM) following laminoplasty, but this change was not statistically significant (p>0.05).\"\nQuestion:\n\"Laminoplasty outcomes: is there a difference between patients with degenerative stenosis and those with ossification of the posterior longitudinal ligament?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9140335": {
                "source": [
                    "\"The authors determine whether prevention influences the use of health services. Fluoridation's effect on restorative dental demand among 972 Washington state employees and spouses, aged 20 to 34 years, in two fluoridated communities and a nonfluoridated community was examined.\nAt baseline, adults were interviewed by telephone, and oral assessments were conducted to measure personal characteristics, lifetime exposure to fluoridated water, oral disease, and the quality of restorations. Adults were followed for 2 years to measure dental demand from dental claims. Each adult's baseline and claims data were linked with provider and practice variables collected from the dentist who provided treatment.\nRelative to adults with no lifetime exposure to fluoridated water, adults drinking fluoridated water for half or more of their lives had less disease at baseline and a lower but nonsignificant probability of receiving a restoration in the follow-up period. In the 2-year follow-up period, however, more than half of the restorations were performed to replace fillings of satisfactory or ideal quality at baseline. When only teeth with decay and unsatisfactory fillings at baseline were considered, adults with high fluoridation exposure had a lower probability of receiving a restoration than adults with no exposure. Market effects also were detected in demand equations; relative to adults in the nonfluoridated community, adults residing in the fluoridated community with a large dentist supply received a greater number of restorations, suggesting potential supplier-induced demand from less disease and fewer patients.\"\nQuestion:\n\"Does fluoridation reduce the use of dental services among adults?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24695920": {
                "source": [
                    "\"The purpose of this study was to evaluate the association between the postoperative outcomes of anterior cruciate ligament (ACL) reconstruction and the anterior laxity of the uninjured knee.\nWe retrospectively reviewed 163 patients who had undergone unilateral ACL reconstruction from January 2002 to August 2009. Patients were divided into three groups according to the anterior laxity of the contralateral, normal knee in 30\u00b0 of knee flexion as measured with a KT2000 arthrometer exerting a force of 134 N:<5 mm for Group 1, 5 to 7.5 mm for Group 2, and>7.5 mm for Group 3. Anterior laxity of the uninjured knee was assessed preoperatively, and anterior laxity of the reconstructed knee was assessed at twenty-four months postoperatively. Anterior stability of the knee was also assessed with use of the Lachman and pivot-shift tests. Functional outcomes were assessed with the Lysholm score and the International Knee Documentation Committee (IKDC) score.\nThe three groups differed significantly with respect to the postoperative side-to-side difference in anterior laxity (p = 0.015), Lysholm score (p<0.001), and IKDC subjective score (p<0.001). The mean side-to-side difference in anterior laxity of the reconstructed knee was 2.1 \u00b1 1.3 mm in Group 1, 2.2 \u00b1 1.3 mm in Group 2, and 2.9 \u00b1 1.4 mm in Group 3. The postoperative Lysholm score was 91.8 \u00b1 4.5 in Group 1, 90.3 \u00b1 5.5 in Group 2, and 85.4 \u00b1 6.6 in Group 3. The postoperative IKDC subjective score was 89.3 \u00b1 6.4 in Group 1, 87.9 \u00b1 6.0 in Group 2, and 82.6 \u00b1 8.2 in Group 3. Post hoc testing showed that Group 3 had significantly greater anterior laxity (p \u2264 0.039) and lower functional scores (p \u2264 0.001) compared with Groups 1 and 2.\"\nQuestion:\n\"Does anterior laxity of the uninjured knee influence clinical outcomes of ACL reconstruction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23348330": {
                "source": [
                    "\"This study aims to evaluate the efficacy of closed reduction and the effects of timing and fracture types on patient satisfaction.\nOnly patients with isolated nasal fractures were included in the study. Patients with additional maxillofacial fractures and patients whose application time to our clinic was more than 10 days after the trauma were excluded. Patients were classified into 5 types according to their fracture. All patients underwent closed reduction and external fixation under local anesthesia. Patients were asked about their satisfaction in a survey at 28th day and sixth month after the surgery. Patients were divided into groups according to fracture type and intervention time, and the results of the survey were evaluated.\nOf the 43 patients included in the study, 38 were male, 5 were female, and the average age was 24.9. The average intervention time of the patients was 5.44 days. Twenty-eight (65%) of 43 patients were satisfied with the result, whereas 15 (35%) patients were not happy with their operation. In a comparison of patient satisfaction rates according to fracture type, the mild fracture group had a higher satisfaction rate compared to the severe fracture group.\"\nQuestion:\n\"Nasal fractures:  is closed reduction satisfying?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25747932": {
                "source": [
                    "\"This paper uses a life-course approach to explore whether the timing and/or duration of urban (vs rural) exposure was associated with risk factors for NCDs.\nA cross-sectional survey was conducted among health care workers in two hospitals in Thailand. Two measures of urbanicity were considered: early-life urban exposure and the proportion of urban life years. We explored four behavioral NCD risk factors, two physiological risk factors and four biological risk factors.\nBoth measures of urbanicity were each independently associated with increases in all behavioral and physiological risk factors. For some biological risk factors, people spending their early life in an urban area may be more susceptible to the effect of increasing proportion of urban life years than those growing up in rural areas.\"\nQuestion:\n\"Living in an urban environment and non-communicable disease risk in Thailand: Does timing matter?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23870157": {
                "source": [
                    "\"Precursor events are undesirable events that can lead to a subsequent adverse event and have been associated with postoperative mortality. The purpose of the present study was to determine whether precursor events are associated with a composite endpoint of major adverse cardiac events (MACE) (death, acute renal failure, stroke, infection) in a low- to medium-risk coronary artery bypass grafting, valve, and valve plus coronary artery bypass grafting population. These events might be targets for strategies aimed at quality improvement.\nThe present study was a retrospective cohort design performed at the Queen Elizabeth Health Science Centre. Low- to medium-risk patients who had experienced postoperative MACE were matched 1:1 with patients who had not experienced postoperative MACE. The operative notes, for both groups, were scored by 5 surgeons to determine the frequency of 4 precursor events: bleeding, difficulty weaning from cardiopulmonary bypass, repair or regrafting, and incomplete revascularization or repair. A univariate comparison of \u22651 precursor events in the matched groups was performed.\nA total of 311 MACE patients (98.4%) were matched. The primary outcome occurred more frequently in the MACE group than in the non-MACE group (33% vs 24%; P\u00a0=\u00a0.015). The incidence of the individual events of bleeding and difficulty weaning from cardiopulmonary bypass was significantly higher in the MACE group. Those patients with a precursor event in the absence of MACE also appeared to have a greater prevalence of other important postoperative outcomes.\"\nQuestion:\n\"Are intraoperative precursor events associated with postoperative major adverse events?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25503376": {
                "source": [
                    "\"Obstructive sleep apnea (OSA) is tightly linked to increased cardiovascular disease. Surgery is an important method to treat OSA, but its effect on serum lipid levels in OSA patients is unknown. We aimed to evaluate the effect of upper airway surgery on lipid profiles.\nWe performed a retrospective review of 113 adult patients with OSA who underwent surgery (nasal or uvulopalatopharyngoplasty [UPPP]) at a major, urban, academic hospital in Beijing from 2012 to 2013 who had preoperative and postoperative serum lipid profiles.\nSerum TC (4.86\u00b10.74 to 4.69\u00b10.71) and LP(a) (median 18.50 to 10.90) all decreased significantly post-operatively (P<0.01, 0.01, respectively), with no changes in serum HDL, LDL, or TG (P>0.05, all). For UPPP patients (n=51), serum TC, HDL and LP(a) improved (P=0.01, 0.01,<0.01, respectively). For nasal patients (n=62), only the serum LP(a) decreased (P<0.01). In patients with normal serum lipids at baseline, only serum LP(a) decreased (P<0.01). In contrast, in patients with isolated hypertriglyceridemia, the serum HDL, TG and LP(a) showed significant improvements (P=0.02, 0.03,<0.01, respectively). In patients with isolated hypercholesterolemia, the serum LP(a) decreased significantly (P=0.01), with a similar trend for serum TC (P=0.06). In patients with mixed hyperlipidemia, the serum TC and LDL also decreased (P=0.02, 0.03, respectively).\"\nQuestion:\n\"Does airway surgery lower serum lipid levels in obstructive sleep apnea patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9278754": {
                "source": [
                    "\"The purpose of this study was to determine whether head and neck-specific health status domains are distinct from those assessed by general measures of quality-of-life (QOL).\nCross-sectional study of 55 head and neck cancer patients in tertiary academic center was made. Three head and neck-specific measures,-including the Head&Neck Survey (H&NS); a brief, multi-item test which generates domain scores; and a general health measure,-were administered.\nThe H&NS was highly reliable and more strongly correlated to the specific measures than to the general measure. Eating/swallowing (ES) and speech/communication (SC) were not well correlated with general health domains. Head and neck pain was highly correlated to general bodily pain (0.88, p<.0001). Despite correlations to some general health domains, appearance (AP) was not fully reflected by any other domain.\"\nQuestion:\n\"Are head and neck specific quality of life measures necessary?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18359123": {
                "source": [
                    "\"Swedish hospital mergers seem to stem from a conviction among policy makers that bigger hospitals lead to lower average costs and improved clinical outcomes. The effects of mergers in the form of multisited hospitals have not been systematically evaluated. The purpose of this article is to contribute to this area of knowledge by exploring responses to the merger of Blekinge Hospital.\nThe evaluation was guided by the philosophy of triangulation. A questionnaire was sent to 597 randomly selected employees, that is 24% of the health care staff. Four hundred ninety-eight employees answered the questionnaire, giving a response rate of 83%. Furthermore, interviews of different groups of stakeholders were conducted.\nA moderate increase of quality was assessed, which, a low proportion of the employees perceived had decisively or largely to do with the merger. The majority perceives economical incentives as the drivers of change, but, at the same time, only 10% of this group believes this target was reached completely or to a large extent.\"\nQuestion:\n\"Is it better to be big?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24336869": {
                "source": [
                    "\"The 'law of spatiotemporal concentrations of events' introduced major preventative shifts in policing communities. 'Hotspots' are at the forefront of these developments yet somewhat understudied in emergency medicine. Furthermore, little is known about interagency 'data-crossover', despite some developments through the Cardiff Model. Can police-ED interagency data-sharing be used to reduce community-violence using a hotspots methodology?\n12-month (2012) descriptive study and analysis of spatiotemporal clusters of police and emergency calls for service using hotspots methodology and assessing the degree of incident overlap. 3775 violent crime incidents and 775 assault incidents analysed using spatiotemporal clustering with k-means++ algorithm and Spearman's rho.\nSpatiotemporal location of calls for services to the police and the ambulance service are equally highly concentrated in a small number of geographical areas, primarily within intra-agency hotspots (33% and 53%, respectively) but across agencies' hotspots as well (25% and 15%, respectively). Datasets are statistically correlated with one another at the 0.57 and 0.34 levels, with 50% overlap when adjusted for the number of hotspots. At least one in every two police hotspots does not have an ambulance hotspot overlapping with it, suggesting half of assault spatiotemporal concentrations are unknown to the police. Data further suggest that more severely injured patients, as estimated by transfer to hospital, tend to be injured in the places with the highest number of police-recorded crimes.\"\nQuestion:\n\"Can routinely collected ambulance data about assaults contribute to reduction in community violence?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9602458": {
                "source": [
                    "\"This paper assesses the usefulness of the Child Health Computing System as a source of information about children with cerebral palsy.\nA comparative survey of information held on the Child Health Computing System (CHCS) and the Northern Ireland Cerebral Palsy Register (NICPR) in one Health and Social Services Board in Northern Ireland was carried out. The sample comprised children with cerebral palsy aged 5-9 years.\nOf the 135 cases recorded on the NICPR, 47 per cent were not found on the CHCS; the majority of these children had no computer record of any medical diagnosis. Of the 82 cases recorded on the CHCS, 10 (12 per cent) were not found on the NICPR; five of these cases (6 per cent) were found on follow-up not to have CP.\"\nQuestion:\n\"Does the Child Health Computing System adequately identify children with cerebral palsy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22990761": {
                "source": [
                    "\"Elevated resting heart rate (RHR) is a neglected marker in cardiovascular risk factor studies of sub-Saharan African populations. This study aimed to determine the prevalence of elevated RHR and other risk factors for cardiovascular disease (CVD) and to investigate any associations between RHR and these risk factors in a rural population in Ghana.\nCross-sectional analysis.\nA total of 574 adults aged between 18-65 years were randomly sampled from a population register. Data collected included those on sociodemographic variables and anthropometric, blood pressure (BP), and RHR measurements. Within-person variability in RHR was calculated using data from repeat measurements taken 2 weeks apart.\nOf study participants, 36% were male. Prevalence of casual high BP was 19%. In the population, 10% were current cigarette smokers and habitual alcohol use was high at 56%. As measured by body mass index, 2% were obese and 14% had abdominal obesity. RHR was elevated (>90 bpm) in 19%. Overall, 79% of study participants were found to have at least one CVD risk factor. RHR was significantly associated with age, waist circumference, and BP. Individuals with an elevated RHR had a higher risk (OR 1.94, 95% CI 1.15-3.26%, p = 0.013) of casual high BP compared with participants with normal RHR independently of several established CVD risk factors. The regression dilution ratio of RHR was 0.75 (95% CI 0.62-0.89).\"\nQuestion:\n\"Cardiovascular risk in a rural adult West African population: is resting heart rate also relevant?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19615731": {
                "source": [
                    "\"Epidemiologic findings support a positive association between asthma and obesity.\nDetermine whether obesity or increasing level of body mass index (BMI) are associated with worse asthma control in an ethnically diverse urban population.\nCross-sectional assessment of asthma control was performed in patients with asthma recruited from primary care offices by using 4 different validated asthma control questionnaires: the Asthma Control and Communication Instrument (ACCI), the Asthma Control Test (ACT), the Asthma Control Questionnaire (ACQ), and the Asthma Therapy Assessment Questionnaire (ATAQ). Multiple linear regression analysis was performed to evaluate the association between obesity and increasing BMI level and asthma control.\nOf 292 subjects with a mean age of 47 years, the majority were women (82%) and African American (67%). There was a high prevalence of obesity with 63%, with only 15% normal weight. The mean score from all 4 questionnaires showed an average suboptimal asthma control (mean score/maximum possible score): ACCI (8.3/19), ACT (15.4/25), ACQ (2.1/6), and ATAQ (1.3/4). Regression analysis showed no association between obesity or increasing BMI level and asthma control using all 4 questionnaires. This finding persisted even after adjusting for FEV(1), smoking status, race, sex, selected comorbid illnesses, and long-term asthma controller use.\"\nQuestion:\n\"Does higher body mass index contribute to worse asthma control in an urban population?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18616781": {
                "source": [
                    "\"Pigmentary dilution is observed in patients with homocystinuria. Therefore, it is possible that an increase of local homocysteine (Hcy) interferes with normal melanogenesis and plays a role in the pathogenesis of vitiligo. Vitamin B12 and folic acid, levels of which are decreased in vitiligo, are important cofactors in the metabolism of Hcy. Consequently, a nutritional deficiency in either of these two vitamins will result in an increase in homocysteine in the circulation, a finding that we expect to find in vitiligo.\nTo determine the level of Hcy in the blood of patients with vitiligo as a first step in revealing if it has any relationship with the pathogenesis of vitiligo and consequently if this will have an impact on the treatment of vitiligo.\nTwenty-six patients of both sexes with vitiligo (age range 20-50 years, mean 31.4 +/- 8.09) and 26 age-matched healthy controls were included in the study. After excluding factors that may affect serum Hcy levels, blood samples from patients and controls were obtained for homocysteine determination by enzyme immunoassay.\nThe mean serum level of Hcy was significantly higher in patients with vitiligo than in controls (21.61 +/- 13.28 vs. 13.1 +/- 4.88 micromol L(-1); P<0.001). The Hcy level was significantly higher in male patients than in female patients (28.67 +/- 15.95 vs. 15.56 +/- 6.2 micromol L(-1); P<0.001) and in male controls compared with female controls (15.07 +/- 4.61 vs. 12.05 +/- 4.82 micromol L(-1); P<0.001). The homocysteine level was related to the activity of vitiligo and was significantly higher in patients with progressive disease than in controls (25.4 +/- 14.99 vs. 13.1 +/- 4.88 micromol L(-1); P<0.001). No significant difference in Hcy levels was found between either untreated vitiligo patients (22.77 +/- 13.36 micromol L(-1)) or patients receiving ultraviolet therapy (20.45 +/- 13.73 micromol L(-1)) and the total patient group (21.62 +/- 13.28 micromol L(-1)).\"\nQuestion:\n\"Is there a relationship between homocysteine and vitiligo?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27832831": {
                "source": [
                    "\"Poor bone quality and unstable fractures increase the cut-out rate in implants with gliding lag screws. The U-Blade (RC) lag screw for the Gamma3\u00aenail was introduced to provide monoaxial rotational stability of the femoral head and neck fragment. The purpose of this study was to evaluate whether the use of the U-Blade (RC) lag screw is associated with reduced cut-out in patients with OTA/AO 31A1-3 fractures.MATERIAL &\nBetween 2009 and 2014, 751 patients with OTA/AO 31A1-3 fractures were treated with a Gamma3\u00aenail at our institution. Out of this sample 199 patients were treated with U-blade (RC) lag screws. A total of 135 patients (117 female, 18 male) with standard lag screw (treatment group A) were matched equally regarding age (\u00b14 years) sex, fracture type and location to 135 patients with U-blade (RC) lag screw (treatment group B). Within a mean follow up of 9.2 months (range 6-18 months) we assessed the cut-out rate, the calTAD, lag screw migration, the Parker's mobility score and the Parker's ratio at postoperatively, six and 12 months following surgery. Furthermore we recorded all complications, ASA-Score, hospital stay and duration of surgery retrospectively.\nThe most common fracture among group B with a cut-out of the lag screw were AO/OTA 2.3 and 3.2 fractures whereas in group A cut-out was most commonly seen in AO/OTA 2.1, 2.2 and 2.3 fractures, there was no significant reduction of the cut-out rate in group B 2.2% (n=3) compared to group A 3.7% (n=5). The duration of surgery was significantly shorter in group A (p<0.05). There was no significant difference in lag screw placement, the Parker's ratio and mobilization.\"\nQuestion:\n\"Does an additional antirotation U-Blade (RC) lag screw improve treatment of AO/OTA 31 A1-3 fractures with gamma 3 nail?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27044366": {
                "source": [
                    "\"The technique of induced sputum has allowed to subdivide asthma patients into inflammatory phenotypes according to their level of granulocyte airway infiltration. There are very few studies which looked at detailed sputum and blood cell counts in a large cohort of asthmatics divided into inflammatory phenotypes. The purpose of this study was to analyze sputum cell counts, blood leukocytes and systemic inflammatory markers in these phenotypes, and investigate how those groups compared with healthy subjects.\nWe conducted a retrospective cross-sectional study on 833 asthmatics recruited from the University Asthma Clinic of Liege and compared them with 194 healthy subjects. Asthmatics were classified into inflammatory phenotypes.\nThe total non-squamous cell count per gram of sputum was greater in mixed granulocytic and neutrophilic phenotypes as compared to eosinophilic, paucigranulocytic asthma and healthy subjects (p\u2009<\u20090.005). Sputum eosinophils (in absolute values and percentages) were increased in all asthma phenotypes including paucigranulocytic asthma, compared to healthy subjects (p\u2009<\u20090.005). Eosinophilic asthma showed higher absolute sputum neutrophil and lymphocyte counts than healthy subjects (p\u2009<\u20090.005), while neutrophilic asthmatics had a particularly low number of sputum macrophages and epithelial cells. All asthma phenotypes showed an increased blood leukocyte count compared to healthy subjects (p\u2009<\u20090.005), with paucigranulocytic asthmatics having also increased absolute blood eosinophils compared to healthy subjects (p\u2009<\u20090.005). Neutrophilic asthma had raised CRP and fibrinogen while eosinophilic asthma only showed raised fibrinogen compared to healthy subjects (p\u2009<\u20090.005).\"\nQuestion:\n\"Detailed analysis of sputum and systemic inflammation in asthma phenotypes: are paucigranulocytic asthmatics really non-inflammatory?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22188074": {
                "source": [
                    "\"To investigate whether problems in instrumental activities of daily living (IADL) can add to conventionally used clinical measurements in helping to predict a diagnosis of dementia at 1- and 2-year follow-up.\nMulticenter prospective cohort study.\nMemory clinics in Europe.\nIndividuals aged 55 and older without dementia.\nIADLs were measured using pooled activities from five informant-based questionnaires. Structural equation modeling (SEM) was used to investigate the relation between IADLs and dementia. Age, sex, education, depression, and cognitive measures (Mini-Mental State Examination and verbal memory) were included in the model.\nFive hundred thirty-one participants had baseline and 1-year follow-up assessments; 69 (13.0%) of these had developed dementia at 1-year follow-up. At 2-year follow-up, 481 participants were seen, of whom 100 (20.8%) had developed dementia. Participants with IADL disabilities at baseline had a higher conversion rate (24.4%) than participants without IADL disabilities (16.7%) (chi-square\u00a0=\u00a04.28, degrees of freedom\u00a0=\u00a01, P\u00a0=\u00a0.04). SEM showed that IADL disability could help predict dementia in addition to the measured variables at 1-year follow-up (odds ratio (OR)\u00a0=\u00a02.20, 95% confidence interval (CI)\u00a0= 1.51-3.13) and 2-year follow-up (OR\u00a0=\u00a02.11, 95% CI\u00a0=\u00a01.33-3.33).\"\nQuestion:\n\"Do instrumental activities of daily living predict dementia at 1- and 2-year follow-up?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24352924": {
                "source": [
                    "\"The purpose of this study was to investigate the efficacy of ultrasonography to confirm Schanz pin placement in a cadaveric model, and the interobserver repeatability of the ultrasound methodology.\nThis investigation is a repeated measures cadaveric study with multiple examiners.\nCadaveric preparation and observations were done by an orthopaedic traumatologist and resident, and two general surgery traumatologists.\nA total of 16 Schanz pins were equally placed in bilateral femora and tibiae. Four examiners took measurements of pin protrusion beyond the distal cortices using first ultrasonography and then by direct measurement after gross dissection.MAIN OUTCOME MEASURE(S): Distal Schanz pin protrusion length measurements from both ultrasonography and direct measurement post dissection.\nSchanz pin protrusion measurements are underestimated by ultrasonography (p<0.01) by an average of 10 percent over the range of 5 to 18 mm, and they display a proportional bias that increases the under reporting as the magnitude of pin protrusion increases. Ultrasound data demonstrate good linear correlation and closely represent actual protrusion values in the 5 to 12 mm range. Interobserver repeatability analysis demonstrated that all examiners were not statistically different in their measurements despite minimal familiarity with the ultrasound methodology (p>0.8).\"\nQuestion:\n\"Is portable ultrasonography accurate in the evaluation of Schanz pin placement during extremity fracture fixation in austere environments?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17894828": {
                "source": [
                    "\"An association has been described between elevated serum angiotensin-converting enzyme (ACE) and an increased risk of severe hypoglycaemia (SH). To ascertain whether this reported association could be replicated in a different country, it was re-examined in 300 individuals with Type 1 diabetes.\nPeople with Type 1 diabetes, none of whom was taking renin-angiotensin system blocking drugs, were recruited. Participants recorded the frequency with which they had experienced SH. Glycated haemoglobin (HbA(1c)) and serum ACE were measured. The difference in the incidence of SH between different quartiles of ACE activity and the relationship between serum ACE and SH were examined using non-parametric statistical tests and a negative binomial model.\nData were obtained from 300 patients [158 male; HbA(1c) median (range) 8.2% (5.2-12.8%), median age 36 years (16-88); duration of diabetes 14.5 years (2-49)]. The incidence of SH was 0.93 episodes per patient year. The mean incidence of SH in the top and bottom quartiles of ACE activity was 0.5 and 1.7 episodes per patient year, respectively, but this difference was not statistically significant (P = 0.075). Spearman's test showed a very weak, although statistically significant, association between serum ACE level and SH incidence (r = 0.115, P = 0.047). The binomial model also showed a statistically significant (P = 0.002), but clinically weak, relationship between serum ACE and SH.\"\nQuestion:\n\"Serum angiotensin-converting enzyme and frequency of severe hypoglycaemia in Type 1 diabetes: does a relationship exist?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26200172": {
                "source": [
                    "\"In recent years, biofeedback has become increasingly popular for its proven success in peak performance training - the psychophysiological preparation of athletes for high-stakes sport competitions, such as the Olympic games. The aim of this research was to test whether an 8-week period of exposure to biofeedback training could improve the psychophysiological control over competitive anxiety and enhance athletic performance in participating subjects.\nParticipants of this study were highly competent athletes, each training in different sport disciplines. The experimental group consisted of 18 athletes (4 women, 14 men), whereas the Control group had 21 athletes (4 women, 17 men). All athletes were between 16 and 34 years old. The biofeedback device, Nexus 10, was used to detect and measure the psychophysiological responses of athletes. Athletes from both groups (control and experimental) were subjected to stress tests at the beginning of the study and once again at its conclusion. In between, the experimental group received training in biofeedback techniques. We then calculated the overall percentage of athletes in the experimental group compared with those in the control group who were able to control respiration, skin conductance, heart rate, blood flow amplitude, heart rate variability, and heart respiration coherence. One year following completion of the initial study, we questioned athletes from the experimental group, to determine whether they continued to use these skills and if they could detect any subsequent enhancement in their athletic performance.\nWe demonstrated that a greater number of participants in the experimental group were able to successfully control their psychophysiological parameters, in comparison to their peers in the control group. Significant results (p<0.05) were noted in regulation of GSR following short stress test conditions (p = 0.037), in regulation of HR after exposure to STROOP stressor (p = 0.037), in regulation of GSR following the Math and GSR stressors (p = 0.033, p = 0.409) and in achieving HR - breathing coherence following the math stressor (p = 0.042).\"\nQuestion:\n\"Can biofeedback training of psychophysiological responses enhance athletes' sport performance?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15488260": {
                "source": [
                    "\"Rates of relapse and predictive relapse factors were studied over more than 4 years in a sample of Spanish outpatients with DSM-III-R criteria for unipolar major depressive episode.\nA final sample of 139 outpatient was followed monthly in a naturalistic study. The Structured Clinical Interview for DSM-III-R was used. Phases of evolution were recorded using the Hamilton Depression Rating Scale, applying the Frank criteria. Survival analysis, Kaplan-Meier product limit and proportional hazards models were used.\nA higher rate of relapses was observed in the partial remission group (91.4%) compared to the complete remission one (51.3%). The four factors with predictive relapse value were: \"partial remission versus complete remission\", \"the intensity of clinical symptoms\", \"the age\" and \"the number of previous depressive episodes\". The existence of partial remission was the most powerful predictive factor.\nThe decreasing sample size during the follow-up and the difficulty in warranting the treatment compliance.\"\nQuestion:\n\"Is the type of remission after a major depressive episode an important risk factor to relapses in a 4-year follow up?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24191126": {
                "source": [
                    "\"Surgical excision of ovarian endometriomas in patients desiring pregnancy has recently been criticized because of the risk of damage to healthy ovarian tissue and consequent reduction of ovarian reserve. A correct diagnosis in cases not scheduled for surgery is therefore mandatory in order to avoid unexpected ovarian cancer misdiagnosis. Endometriosis is often associated with high levels of CA125. This marker is therefore not useful for discriminating ovarian endometrioma from ovarian malignancy. The aim of this study was to establish if the serum marker CA72-4 could be helpful in the differential diagnosis between ovarian endometriosis and epithelial ovarian cancer.\nSerums CA125 and CA72-4 were measured in 72 patients with ovarian endometriomas and 55 patients with ovarian cancer.\nHigh CA125 concentrations were observed in patients with ovarian endometriosis and in those with ovarian cancer. A marked difference in CA72-4 values was observed between women with ovarian cancer (71.0%) and patients with endometriosis (13.8%) (P<0.0001).\"\nQuestion:\n\"Is CA72-4 a useful biomarker in differential diagnosis between ovarian endometrioma and epithelial ovarian cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18594195": {
                "source": [
                    "\"Refusal of patients to participate in intervention programs is an important problem in clinical trials but, in general, researchers devote relatively little attention to it. In this article, a comparison is made between patients who, after having been invited, agreed to participate in a self-management intervention (participants) and those who refused (refusers). Compared with other studies of refusers, relatively more information could be gathered with regard to both their characteristics and reasons for refusing, because all potential participants were invited personally.\nOlder patients from a Dutch outpatient clinic were invited to participate in a self-management intervention, and their characteristics were assessed. Demographic data were collected, as well as data on physical functioning and lack of emotional support. People who refused to participate were asked to give their reasons for refusing.\nOf the 361 patients invited, 267 (74%) refused participation. These refusers were more restricted in their mobility, lived further away from the location of the intervention, and had a partner more often than did the participants. No differences were found in level of education, age or gender. The main reasons given by respondents for refusing to participate were lack of time, travel distance, and transport problems.\"\nQuestion:\n\"Do older patients who refuse to participate in a self-management intervention in the Netherlands differ from older patients who agree to participate?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17274051": {
                "source": [
                    "\"The aim of the study was to evaluate the outcomes and patterns of failure in patients with metastatic carcinoma to cervical lymph nodes from an unknown head and neck primary origin, who were treated curatively with radiotherapy, with or without neck dissection.\nThe study included 61 patients referred to the McGill University Hospital Centers from 1987 to 2002. The median age was 57 years, with male to female ratio of 4:1. Distribution of patients by N status was as follows: N1, 16 patients (26%); N2a, 18 (30%); N2b, 13 (22%); N2c, 7 (11%); and N3, 7 (11%). Twenty patients underwent neck dissection (11 radical, 9 functional) and 41 patients had biopsy (9 fine-needle aspiration and 32 excisional biopsy). All patients received radiotherapy. The median dose to the involved node(s) was 64 Gy, and 60 Gy to the rest of the neck. Treatment of the neck was bilateral in 50 patients (82%) and ipsilateral in 11 (18%). The minimum duration of the follow-up was 12 months, with the median of 32 months.\nThe 5- and 8-year overall survival for the whole population was 79% and 67%, respectively. There was no statistically significant difference in the 8-year actuarial overall survival (64.8% and 67.6%, respectively, p = .64) and local relapse-free survival (75% vs 74.5%, respectively, p = .57), among patients who had biopsy versus those who had neck dissection.\"\nQuestion:\n\"Metastatic carcinoma to the cervical nodes from an unknown head and neck primary site: Is there a need for neck dissection?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20497146": {
                "source": [
                    "\"In patients with Los Angeles (LA) grade C or D oesophagitis, a positive relationship has been established between the duration of intragastric acid suppression and healing.AIM: To determine whether there is an apparent optimal time of intragastric acid suppression for maximal healing of reflux oesophagitis.\nPost hoc analysis of data from a proof-of-concept, double-blind, randomized study of 134 adult patients treated with esomeprazole (10 or 40 mg od for 4 weeks) for LA grade C or D oesophagitis. A curve was fitted to pooled 24-h intragastric pH (day 5) and endoscopically assessed healing (4 weeks) data using piecewise quadratic logistic regression.\nMaximal reflux oesophagitis healing rates were achieved when intragastric pH>4 was achieved for approximately 50-70% (12-17 h) of the 24-h period. Acid suppression above this threshold did not yield further increases in healing rates.\"\nQuestion:\n\"A model of healing of Los Angeles grades C and D reflux oesophagitis: is there an optimal time of acid suppression for maximal healing?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22519710": {
                "source": [
                    "\"The temporal pattern of the biologic mechanism linking red blood cell (RBC) storage duration with clinical outcomes is yet unknown. This study investigates how such a temporal pattern can affect the power of randomized controlled trials (RCT) to detect a relevant clinical outcome mediated by the transfusion of stored RBCs.\nThis study was a computer simulation of four RCTs, each using a specific categorization of the RBC storage time. The trial's endpoint was evaluated assuming five hypothetical temporal patterns for the biologic mechanism linking RBC storage duration with clinical outcomes.\nPower of RCTs to unveil a significant association between RBC storage duration and clinical outcomes was critically dependent on a complex interaction among three factors: 1) the way the RBC storage time is categorized in the trial design, 2) the temporal pattern assumed for the RBC storage lesion, and 3) the age distribution of RBCs in the inventory from which they are picked up for transfusion. For most combinations of these factors, the power of RCTs to detect a significant treatment effect was below 80%. All the four simulated RCTs had a very low power to disclose a harmful clinical effect confined to last week of the maximum 42-day shelf life of stored RBCs.\"\nQuestion:\n\"Will clinical studies elucidate the connection between the length of storage of transfused red blood cells and clinical outcomes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17076590": {
                "source": [
                    "\"To observe if medical providers alter their prescribing patterns of three relatively expensive categories of medications provided as samples by manufacturers (focus medications) when they receive additional education from pharmacists concerning the appropriate use of lower cost alternatives (counter samples) that are made available to dispense.\nPretest, post-test with a control group.\nTwo rural, private care clinics in southeastern Idaho providing immediate care services.\nEight medical providers at a clinic where interventions were employed (active intervention group) and seven medical providers in a clinic where no interventions occurred (control group).\nMedical providers in the active intervention group had: 1) education from pharmacists concerning the appropriate use of lower-cost alternatives compared with expensive focus medications 2) counter samples and patient sample handouts available to dispense to patients at their own discretion.\nThe percentage of the total yearly prescriptions for nonsteroidal anti-inflammatory drugs (NSAIDs), antihistamines, and acid-relief medications that consisted of focus-COX-2 NSAIDs, nonsedating antihistamines, and proton pump inhibitors (PPIs), respectively.\nThe prescribing behavior of medical providers in the active intervention and control groups were significantly different at baseline in all three categories of focus medications. This suggested that the results should focus on changes across the two years of the study within the intervention and control groups rather than across the two groups. Medical providers in the intervention group significantly decreased the use of COX-2 NSAID prescriptions relative to total NSAID prescriptions following active intervention (38.9% in year 1 versus 23.7% in year 2, P<0.05). Over the same two time periods, a nonstatistically significant decrease in COX-2 NSAID prescribing was seen at the control site (67.5% versus 62%, P>0.05). Education and counter sampling did not stop medical providers from significantly increasing the total yearly prescriptions for antihistamines and acid-relief medications that consisted of focus-nonsedating antihistamines (86.7% versus 93.1%, P<0.05) and PPIs (68.9% versus 86.2%, P<0.05). Statistically significant increases in the prescribing of focus-nonsedating antihistamines (77.9% versus 98.3%, P<0.05) and PPIs (77.5% versus 91.4%, P<0.05) were also observed in the control group.\"\nQuestion:\n\"Counter sampling combined with medical provider education: do they alter prescribing behavior?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17551944": {
                "source": [
                    "\"To determine whether spectral Doppler measurements obtained from bilateral uterine, arcuate, radial, and spiral arteries in early gestation correlate with adverse pregnancy outcome.\nOne hundred five pregnant women underwent transvaginal Doppler sonographic examination of uteroplacental circulation at 6-12 weeks' gestation. Resistance index (RI) and pulsatility index (PI) of bilateral uterine, arcuate, radial, and spiral arteries were measured. Diameters of gestational sac (GS) and yolk sac, crown-rump length (CRL), GS-CRL difference, and GS/CRL ratio were also recorded. Correlation was made with pregnancy outcome.\nSixteen women developed adverse pregnancy outcome. In these women, right uterine artery PI and RI were significantly higher than in women with normal obstetrical outcome. Spiral artery PI and RI values were also higher, but the difference was not statistically significant. GS-CRL difference, GS/CRL ratio, and yolk sac diameters were significantly lower in this group.\"\nQuestion:\n\"Doppler examination of uteroplacental circulation in early pregnancy: can it predict adverse outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25787073": {
                "source": [
                    "\"The serum C-reactive protein (CRP) level correlates with the clinical prognosis in patients with kidney, penile and metastatic castration-resistant prostate cancer (PC). We prospectively evaluated the preoperative CRP level as a predictive marker for an advanced tumor stage or high-grade cancer in patients with clinically localized PC.\nThe study evaluated 629 patients with clinically localized PC who underwent radical prostatectomy between 2010 and 2013. Exclusion criteria were signs of systemic infection, symptoms of an autoimmune disease or neoadjuvant androgen deprivation.\nPoorly differentiated PC tends to be more common in patients with elevated CRP levels (15.5 vs. 9.5%, p = 0.08). Analogously, patients with a Gleason score \u22658 PC had significantly higher median CRP levels than those with a Gleason score \u22647 PC (1.9 vs. 1.2 mg/l, p = 0.03). However, neither uni- nor multivariate analysis showed an association between the preoperative CRP level and the presence of a locally advanced tumor stage, lymph node metastases or a positive surgical margin. CRP also failed to correlate with the initial PSA level and the clinical tumor-associated findings. Moreover, multivariate analysis relativized the association between an elevated CRP level and poor tumor differentiation.\"\nQuestion:\n\"Do preoperative serum C-reactive protein levels predict the definitive pathological stage in patients with clinically localized prostate cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15208005": {
                "source": [
                    "\"Low intakes or blood levels of eicosapentaenoic and docosahexaenoic acids (EPA + DHA) are independently associated with increased risk of death from coronary heart disease (CHD). In randomized secondary prevention trials, fish or fish oil have been demonstrated to reduce total and CHD mortality at intakes of about 1 g/day. Red blood cell (RBC) fatty acid (FA) composition reflects long-term intake of EPA + DHA. We propose that the RBC EPA + DHA (hereafter called the Omega-3 Index) be considered a new risk factor for death from CHD.\nWe conducted clinical and laboratory experiments to generate data necessary for the validation of the Omega-3 Index as a CHD risk predictor. The relationship between this putative marker and risk for CHD death, especially sudden cardiac death (SCD), was then evaluated in several published primary and secondary prevention studies.\nThe Omega-3 Index was inversely associated with risk for CHD mortality. An Omega-3 Index of>or = 8% was associated with the greatest cardioprotection, whereas an index of<or = 4% was associated with the least.\"\nQuestion:\n\"The Omega-3 Index: a new risk factor for death from coronary heart disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10173769": {
                "source": [
                    "\"To consider whether the Barthel Index alone provides sufficient information about the long term outcome of stroke.\nCross sectional follow up study with a structured interview questionnaire and measures of impairment, disability, handicap, and general health. The scales used were the hospital anxiety and depression scale, mini mental state examination, Barthel index, modified Rankin scale, London handicap scale, Frenchay activities index, SF36, Nottingham health profile, life satisfaction index, and the caregiver strain index.\nSouth east London.\nPeople, and their identified carers, resident in south east London in 1989-90 when they had their first in a life-time stroke aged under 75 years.\nObservational study.\nComparison and correlation of the individual Barthel index scores with the scores on other outcome measures.\nOne hundred and twenty three (42%) people were known to be alive, of whom 106 (86%) were interviewed. The median age was 71 years (range 34-79). The mean interval between the stroke and follow up was 4.9 years. The rank correlation coefficients between the Barthel and the different dimensions of the SF36 ranged from r = 0.217 (with the role emotional dimension) to r = 0.810 (with the physical functioning dimension); with the Nottingham health profile the range was r = -0.189 (with the sleep dimension, NS) to r = -0.840 (with the physical mobility dimension); with the hospital and anxiety scale depression component the coefficient was r = -0.563, with the life satisfaction index r = 0.361, with the London handicap scale r = 0.726 and with the Frenchay activities index r = 0.826.\"\nQuestion:\n\"Longer term quality of life and outcome in stroke patients: is the Barthel index alone an adequate measure of outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12098035": {
                "source": [
                    "\"We tested the hypothesis that the treatment of patients with acute cholecystitis (AC) would be improved under the care of laparoscopic specialists.\nThe records of patients undergoing cholecystectomy for AC from 1 January 1996 to 31 December 1998 were reviewed retrospectively. Of 170 patients, 48 were cared for by three laparoscopic specialists (LS group), whereas 122 were treated by nine general surgeons who perform only laparoscopic cholecystectomy (LC) (GS group). The rates of successful LC, complications, and length of hospital stay were compared. Multivariate analysis was used to control for baseline differences.\nThe patients in the GS group were older (median age, 63 vs 53 years; p = 0.01). In all, 31 LS patients (65%), as compared with 44 GS patients (36%), had successful laparoscopic treatment (p = 0.001). The operating time was the same (median, 70 min). The proportion of patients with postoperative complications was similar in the two groups (37% in the GS vs 31% in the LS group; p = 0.6). The median postoperative hospital stay (3 vs 5 days; p<0.01) was shorter in the LS group. On logistic regression analysis, significant predictors of a successful laparoscopic operation included LS group (p<0.01) and age (p = 0). Predictors of prolonged length of hospital stay were age (p<0.01) and comorbidity score (p<0.01), with LS group status not a significant factor (p = 0.21).\"\nQuestion:\n\"Does a special interest in laparoscopy affect the treatment of acute cholecystitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15053041": {
                "source": [
                    "\"Increased aortic stiffness is a independent risk factor of cardiovascular disease in patients with hypertension. Acute changes of the heart rate (HR) have been reported not to affect the aortic stiffness in pacing. However, it is unknown whether acute changes in HR caused by sympathomimetics can affect the aortic stiffness in patients with hypertension. We investigated the effect of acute changes in HR produced by isoproterenol on the aortic stiffness in 17 hypertensive patientss (mean age: 59 +/- 9 years).\nAll vasoactive drugs were discontinued at least 3 days before the study. The carotid-to-femoral pulse wave velocity (PWV) was measured by the foot-to-foot method. The pulse waves were recorded at the baseline and at every increase of HR by 5 to 10 bpm with a gradual increase of the dose of isoproterenol. The blood pressures and HR were measured simultaneously. For the analysis, HR, PWV, compliance (C), and compliance index (Ci) were converted as percent changes (delta) from the baseline values. Percent changes of the parameters of the aortic stiffness, i.e., deltaPWV, deltaC, and deltaCi, were grouped by every 10% increase in deltaHR.\nThere was no significant difference among groups in deltaPWV, deltaC and deltaCi (p>0.05 for each of the group). The regression analysis showed no significant correlation of deltaHR with deltaPWV and deltaC (r = 0.18, 0.13 respectively, p>0.05 for each). deltaCi had a poor correlation with deltaHR (r = 0.22, p<0.05). However, only 4.6% of deltaCi could be referred to deltaHR (r2 = 0.046).\"\nQuestion:\n\"Do acute changes in heart rate by isoproterenol affect aortic stiffness in patients with hypertension?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12094116": {
                "source": [
                    "\"The purpose of this study was to identify the relationships between leg muscle power and sprinting speed with changes of direction.\nthe study was designed to describe relationships between physical qualities and a component of sports performance.\ntesting was conducted in an indoor sports hall and a biomechanics laboratory.\n15 male participants were required to be free of injury and have recent experience competing in sports involving sprints with changes of direction.\nsubjects were timed in 8 m sprints in a straight line and with various changes of direction. They were also tested for bilateral and unilateral leg extensor muscle concentric power output by an isokinetic squat and reactive strength by a drop jump.\nThe correlations between concentric power and straight sprinting speed were non-significant whereas the relationships between reactive strength and straight speed were statistically significant. Correlations between muscle power and speed while changing direction were generally low and non-significant for concentric leg power with some moderate and significant (p<0.05) coefficients found for reactive strength. The participants who turned faster to one side tended to have a reactive strength dominance in the leg responsible for the push-off action.\"\nQuestion:\n\"Is muscle power related to running speed with changes of direction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15631914": {
                "source": [
                    "\"Clinically positive axillary nodes are widely considered a contraindication to sentinel lymph node (SLN) biopsy in breast cancer, yet no data support this mandate. In fact, data from the era of axillary lymph node dissection (ALND) suggest that clinical examination of the axilla is falsely positive in as many as 30% of cases. Here we report the results of SLN biopsy in a selected group of breast cancer patients with palpable axillary nodes classified as either moderately or highly suspicious for metastasis.\nAmong 2,027 consecutive SLN biopsy procedures performed by two experienced surgeons, clinically suspicious axillary nodes were identified in 106, and categorized as group 1 (asymmetric enlargement of the ipsilateral axillary nodes moderately suspicious for metastasis, n = 62) and group 2 (clinically positive axillary nodes highly suspicious for metastasis, n = 44).\nClinical examination of the axilla was inaccurate in 41% of patients (43 of 106) overall, and was falsely positive in 53% of patients (33 of 62) with moderately suspicious nodes and 23% of patients (10 of 44) with highly suspicious nodes. False-positive results were less frequent with larger tumor size (p = 0.002) and higher histologic grade (p = 0.002), but were not associated with age, body mass index, or a previous surgical biopsy.\"\nQuestion:\n\"Is the clinically positive axilla in breast cancer really a contraindication to sentinel lymph node biopsy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15095519": {
                "source": [
                    "\"The purpose of this study was to determine if registered dietitian (RD) and registered nurse (RN) certified diabetes educators (CDEs) provide similar recommendations regarding carbohydrates and dietary supplements to individuals with diabetes.\nA survey was mailed to CDEs in the southern United States. Participants were asked to indicate their recommendations for use of carbohydrates, fiber, artificial sweeteners, and 12 selected dietary and herbal supplements when counseling individuals with diabetes.\nThe survey sample consisted of 366 CDEs: 207 were RNs and 159 were RDs. No statistically significant differences were found between RNs and RDs in typical carbohydrate recommendations for treatment of diabetes. However, RDs were more likely than RNs to make recommendations for fiber intake or use of the glycemic index. A significant difference also was found in the treatment of hypoglycemia: RNs were more likely than RDs to recommend consuming a carbohydrate source with protein to treat hypoglycemia.\"\nQuestion:\n\"Are patients with diabetes receiving the same message from dietitians and nurses?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21865668": {
                "source": [
                    "\"Most older drivers continue to drive as they age. To maintain safe and independent transport, mobility is important for all individuals, but especially for older drivers.\nThe objective of this study was to investigate whether automatic transmission, compared with manual transmission, may improve the driving behavior of older drivers.\nIn total, 31 older drivers (mean age 75.2 years) and 32 younger drivers - used as a control group (mean age 39.2 years) - were assessed twice on the same fixed route; once in a car with manual transmission and once in a car with automatic transmission. The cars were otherwise identical. The driving behavior was assessed with the Ryd On-Road Assessment driving protocol. Time to completion of left turns (right-hand side driving) and the impact of a distraction task were measured.\nThe older group had more driving errors than the younger group, in both the manual and the automatic transmission car. However, and contrary to the younger drivers, automatic transmission improved the older participants' driving behavior as demonstrated by safer speed adjustment in urban areas, greater maneuvering skills, safer lane position and driving in accordance with the speed regulations.\"\nQuestion:\n\"Does automatic transmission improve driving behavior in older drivers?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22411435": {
                "source": [
                    "\"Infliximab, a chimeric monoclonal anti-TNF\u03b1 antibody, has been found to increase the risk of serious infections compared with the TNF receptor fusion protein etanercept in some studies. It is unclear whether the risk varies by patient characteristics. We conducted a study to address this question.\nWe identified members of Kaiser Permanente Northern California who initiated infliximab (n = 793) or etanercept (n = 2692) in 1997-2007. Using a Cox model, we estimated the propensity-score-adjusted hazard ratio (HR) and 95% confidence interval (CI) of serious infections requiring hospitalization or opportunistic infections comparing infliximab initiators to etanercept initiators. We tested whether the adjusted HR differed by age, sex, race/ethnicity, body mass index, and smoking status.\nThe crude incidence rate of serious infections per 100 person-years was 5.4 (95%CI: 3.8, 7.5) in patients<65 years and 16.0 (95%CI: 10.4, 23.4) in patients \u2265 65 years during the first 3 months following treatment initiation. Compared with etanercept, the adjusted HR during this period was elevated for infliximab in patients<65 years (HR: 3.01; 95%CI: 1.49, 6.07), but not in those \u2265 65 years (HR 0.94; 95%CI: 0.41, 2.13). Findings did not suggest that the HR varied by the other patient characteristics examined.\"\nQuestion:\n\"Comparative safety of infliximab and etanercept on the risk of serious infections: does the association vary by patient characteristics?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24270957": {
                "source": [
                    "\"Our aim was to investigate the effects of growth hormone (GH), hyperbaric oxygen and combined therapy on normal and ischemic colonic anastomoses in rats.\nEighty male Wistar rats were divided into eight groups (n\u200a=\u200a10). In the first four groups, non-ischemic colonic anastomosis was performed, whereas in the remaining four groups, ischemic colonic anastomosis was performed. In groups 5, 6, 7, and 8, colonic ischemia was established by ligating 2 cm of the mesocolon on either side of the anastomosis. The control groups (1 and 5) received no treatment. Hyperbaric oxygen therapy was initiated immediately after surgery and continued for 4 days in groups 3 and 4. Groups 2 and 6 received recombinant human growth hormone, whereas groups 4 and 8 received GH and hyperbaric oxygen treatment. Relaparotomy was performed on postoperative day 4, and a perianastomotic colon segment 2 cm in length was excised for the detection of biochemical and mechanical parameters of anastomotic healing and histopathological evaluation.\nCombined treatment with hyperbaric oxygen and GH increased the mean bursting pressure values in all of the groups, and a statistically significant increase was noted in the ischemic groups compared to the controls (p<0.05). This improvement was more evident in the ischemic and normal groups treated with combined therapy. In addition, a histopathological evaluation of anastomotic neovascularization and collagen deposition showed significant differences among the groups.\"\nQuestion:\n\"Is combined therapy more effective than growth hormone or hyperbaric oxygen alone in the healing of left ischemic and non-ischemic colonic anastomoses?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17312514": {
                "source": [
                    "\"Seroma is the most frequent complication in abdominoplasty. Some patients are more prone to develop this complication. Ultrasound is a well-known method with which to diagnose seroma in the abdominal wall. The purpose of this study was to verify the efficacy of the use of quilting suture to prevent seroma.\nTwenty-one female patients who presented with abdominal deformity type III/A according to the authors' classification of abdominal skin and myoaponeurotic deformity had undergone abdominoplasty. The selected patients should have had at least one of the following characteristics: body mass index greater than 25 kg/m; weight loss greater than 10 kg; previous incision in the supraumbilical region; or present thinning of the subcutaneous in the area above the umbilicus. Ultrasound was performed for every patient from 15 to 18 days after the operation to search for fluid collection in the abdominal wall.\nThe average fluid collection found was 8.2 cc per patient. Only two patients underwent aspiration because ultrasound showed greater than 20 cc collected above the fascial layer. These patients did not present with recurrence of seroma after aspiration.\"\nQuestion:\n\"Does quilting suture prevent seroma in abdominoplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14655021": {
                "source": [
                    "\"Juvenile and adult forms of ankylosing spondylitis (AS) have been shown to have different clinical presentation and outcome in Caucasians. We did this retrospective analysis to see if similar differences exist in the Indian population.\nCase records of 210 Indian patients diagnosed with AS according to modified New York criteria were reviewed. Data were collected regarding age of onset, clinical features, drug treatment, and outcome at last follow-up. Patients with onset before 17 years of age were classified as having juvenile AS (JAS) and the rest with adult AS (AAS).\nThere were 150 patients with AAS and 60 with JAS. The latter had higher male preponderance, more frequent onset with peripheral arthritis, and greater involvement of hip and knee joints. Valvular dysfunction was seen only in patients with JAS.\"\nQuestion:\n\"Juvenile ankylosing spondylitis--is it the same disease as adult ankylosing spondylitis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25614468": {
                "source": [
                    "\"The aim of this study was to prospectively compare the diagnostic performance of magnetic resonance imaging (MRI), multidetector computed tomography (MDCT) and endoscopic ultrasonography (EUS) in the preoperative locoregional staging of gastric cancer.\nThis study had Institutional Review Board approval, and informed consent was obtained from all patients. Fifty-two patients with biopsy-proven gastric cancer underwent preoperative 1.5-T MRI, 64-channel MDCT and EUS. All images were analysed blind, and the results were compared with histopathological findings according to the seventh edition of the TNM classification. After the population had been divided on the basis of the local invasion (T1-3 vs T4a-b) and nodal involvement (N0 vs N+), sensitivity, specificity, positive and negative predictive value, and accuracy were calculated and diagnostic performance measures were assessed using the McNemar test.\nFor T staging, EUS showed higher sensitivity (94%) than MDCT and MRI (65 and 76%; p = 0.02 and p = 0.08). MDCT and MRI had significantly higher specificity (91 and 89%) than EUS (60%) (p = 0.0009 and p = 0.003). Adding MRI to MDCT or EUS did not result in significant differences for sensitivity. For N staging, EUS showed higher sensitivity (92%) than MRI and MDCT (69 and 73%; p = 0.01 and p = 0.02). MDCT showed better specificity (81%) than EUS and MRI (58 and 73%; p = 0.03 and p = 0.15).\"\nQuestion:\n\"Preoperative locoregional staging of gastric cancer: is there a place for magnetic resonance imaging?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16097998": {
                "source": [
                    "\"The benefits of serologic screening for coeliac disease in asymptomatic individuals are debatable.AIM: To investigate dietary compliance, quality of life and bone mineral density after long-term treatment in coeliac disease patients found by screening in risk groups.\nThe study comprised 53 consecutive screen-detected coeliac patients diagnosed 14 years (median) ago. Dietary compliance was assessed by interview, 4-day food record and serology. Quality of life was evaluated by the Psychological General Well-Being and SF-36 questionnaires, gastrointestinal symptoms by the Gastrointestinal Symptom Rating Scale and bone mineral density by dual-energy x-ray absorptiometry. Comparisons were made to 44 symptom-detected-treated coeliac patients, 110 non-coeliac subjects and the general population.\nA total of 96% of screen-detected and 93% of symptom-detected coeliac patients adhered to a strict or fairly strict gluten-free diet. In screen-detected patients, quality of life and gastrointestinal symptoms were similar to those in symptom-detected patients or non-coeliac controls and bone mineral density was similar to that in the general population.\"\nQuestion:\n\"Is coeliac disease screening in risk groups justified?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20549895": {
                "source": [
                    "\"There has never been a nationally representative survey of medical students' personal health-related practices, although they are inherently of interest and may affect patient-counseling practices. This study evaluated the health practices and the vaccination status of first year residents working at the academic hospital H\u00f4tel-Dieu de France.\nThe medical files of all medicine and surgery residents in their first year of specialization between the years 2005 and 2008 were reviewed. These residents were required to go through a preventive medical visit at the University Center of Family and Community Health.\nOne hundred and nine residents (109) were included in the study; 68 (6239%) were male and 41 (37.61%) were female with a mean age of 26 years. Only 6 residents (5.50%) practiced physical activity according to international guidelines (more than three times a week for more than 30 minutes each time). Most residents (n = 76 ; 69.73%) used to skip one or two meals especially breakfast and as a consequence 30 male (44.11%) and 4 female (9.75%) students were overweight, with a statistical difference between the two sexes (Fisher test, p-value = 0.001). Twenty-eight residents (25.69%) were smokers with a male predominance. Fourteen residents of both genders (12.84%) drank alcohol regularly (>3 times a week) and 71 (65.14%) had a drink occasionally (once a month or less). Only 25 residents (23%) of the cohort had a complete and up-to-date immunization status. The immunization gap was basically against measles, mumps, rubella (MMR) and diphtheria, tetanus, poliomyelitis (dT Polio). Ninety-nine residents (90.83%) had full immunization against hepatitis B with an adequate response in 78 residents (71.56%).\"\nQuestion:\n\"Health habits and vaccination status of Lebanese residents: are future doctors applying the rules of prevention?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27287237": {
                "source": [
                    "\"Little is known about the validity and reliability of expert assessments of the quality of antimicrobial prescribing, despite their importance in antimicrobial stewardship. We investigated how infectious disease doctors' assessments compared with a reference standard (modal expert opinion) and with the assessments of their colleagues.\nTwenty-four doctors specialized in infectious diseases or clinical microbiology (16 specialists and 8 residents) from five hospitals were asked to assess the appropriateness of antimicrobial agents prescribed for a broad spectrum of indications in 56 paper cases. They were instructed how to handle guideline applicability and deviations. We created a reference standard of antimicrobial appropriateness using the modal assessment of 16 specialists. We calculated criterion validity and interrater and intrarater overall and specific agreement with an index expert (senior infectious disease physician) and analysed the influence of doctor characteristics on validity.\nSpecialists agreed with the reference standard in 80% of cases (range 75%-86%), with a sensitivity and specificity of 75% and 84%, respectively. This did not differ by clinical specialty, hospital or years of experience, and residents had similar results. Specialists agreed with the index expert in 76% of cases and the index expert agreed with his previous assessments in 71% of cases.\"\nQuestion:\n\"Assessment of appropriate antimicrobial prescribing: do experts agree?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26418796": {
                "source": [
                    "\"Blood stream infection (BSI) and the subsequent development of sepsis are among the most common infection complications occurring in severe burn patients. This study was designed to evaluate the relationship between the burn wound flora and BSI pathogens.\nDocumentation of all bacterial and fungal wound and blood isolates from severe burn patients hospitalized in the burn unit and intensive care unit was obtained from medical records retrieved retrospectively from a computerized, hospital-wide database over a 13-year period. All data were recorded in relation to the Ryan score.\nOf 195 severe burn patients, 88 had at least 1 BSI episode. Transmission of the same pathogen from wound to blood was documented in 30% of the patients, with a rising BSI frequency as the Ryan score increased. There were a total of 263 bacteremic episodes in 88 study patients, 44% of blood isolates were documented previously in wound cultures, and transmission of the same pathogen from wound to blood was noted in 65% of bacteremic patients.\"\nQuestion:\n\"Do Wound Cultures Give Information About the Microbiology of Blood Cultures in Severe Burn Patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25228241": {
                "source": [
                    "\"The elephant trunk technique for aortic dissection is useful for reducing false lumen pressure; however, a folded vascular prosthesis inside the aorta can cause haemolysis. The purpose of this study was to investigate whether an elephant trunk in a small-calibre lumen can cause haemolysis.\nInpatient and outpatient records were retrospectively reviewed.\nTwo cases of haemolytic anaemia after aortic surgery using the elephant trunk technique were identified from 2011 to 2013. A 64-year-old man, who underwent graft replacement of the ascending aorta for acute Stanford type A aortic dissection, presented with enlargement of the chronic dissection of the descending aorta and moderate aortic regurgitation. A two-stage surgery was scheduled. Total arch replacement with an elephant trunk in the true lumen and concomitant aortic valve replacement were performed. Postoperatively, he developed severe haemolytic anaemia because of the folded elephant trunk. The anaemia improved after the second surgery, including graft replacement of the descending aorta. Similarly, a 61-year-old man, who underwent total arch replacement for acute Stanford type A aortic dissection, presented with enlargement of the chronic dissection of the descending aorta. Graft replacement of the descending aorta with an elephant trunk inserted into the true lumen was performed. The patient postoperatively developed haemolytic anaemia because of the folded elephant trunk, which improved after additional stent grafting into the elephant trunk.\"\nQuestion:\n\"Elephant trunk in a small-calibre true lumen for chronic aortic dissection: cause of haemolytic anaemia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9722752": {
                "source": [
                    "\"To evaluate the outcome of a new modification of percutaneous needle suspension, using a bone anchor system for fixing the suture at the public bone, and to compare the results with those published previously.\nFrom March 1996, 37 patients with stress urinary incontinence (>2 years) were treated using a bone anchor system. On each side the suture was attached to the pubocervical fascia and the vaginal wall via a broad 'Z'-stitch. A urodynamic investigation performed preoperatively in all patients confirmed stress incontinence and excluded detrusor instability. The outcome was assessed by either by a clinical follow-up investigation or using a standardized questionnaire, over a mean follow-up of 11 months (range 6-18).\nIn the 37 patients, the procedure was successful in 25 (68%), with 16 (43%) of the patients completely dry and nine (24%) significantly improved. Removal of the bone anchor and suture was necessary in two patients, because of unilateral bacterial infection in one and a bilateral soft tissue granuloma in the other. One bone anchor became dislocated in a third patient. In two cases where the treatment failed, new detrusor instability was documented urodynamically. Minor complications were prolonged wound pain in 10 (26%) and transient urinary retention or residual urine in 12 patients (32%).\"\nQuestion:\n\"Does bone anchor fixation improve the outcome of percutaneous bladder neck suspension in female stress urinary incontinence?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9792366": {
                "source": [
                    "\"To assess the impact of the comprehensive HIV/STD Control Program established in Jamaica since the late 1980s on the HIV/AIDS epidemic.\nAIDS case reports, HIV testing of blood donors, antenatal clinic attenders (ANC), food service workers, sexually transmitted disease (STD) clinic attenders, female prostitutes, homosexuals and other groups were used to monitor the HIV/AIDS epidemic. Primary and secondary syphilis and cases of congenital syphilis were also monitored. National knowledge, attitude and practice (KAP) surveys were conducted in 1988, 1989, 1992, 1994 and 1996.\nThe annual AIDS incidence rate in Jamaica increased only marginally in the past three years from 18.5 per 100000 population to 21.4 in 1997. HIV prevalence in the general population groups tested has been about 1% or less. Among those at high risk, HIV prevalence rates have risen to 6.3% (95% confidence interval 5.0-8.0) in STD clinic attenders, around 10% and 21% in female prostitutes in Kingston and Montego Bay respectively and approximately 30% among homosexuals. Syphilis rates and congenital syphilis cases have declined. The proportion of men aged 15-49 years reporting sex with a non-regular partner declined from 35% in 1994 to 26% in 1996 (P<0.001). Women ever having used condoms increased from 51% in 1988 to 62.5% in 1992 and 73% in 1994 and 1996 (P<0.001), while condom use with a non-regular partner increased from 37% in 1992 to 73% in 1996 (P= 0.006). Condom use among men was high over the period: around 81% had ever used condoms and 77% used them with non-regular partners. Gay men, inner-city adults and adolescents aged 12-14 years all reported increases in condom use while condom sales and distribution increased from around 2 million in 1985 to 10 million in 1995.\"\nQuestion:\n\"Is HIV/STD control in Jamaica making a difference?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19648304": {
                "source": [
                    "\"This randomized controlled study addressed whether sonographic needle guidance affected clinical outcomes of intraarticular (IA) joint injections.\nIn total, 148 painful joints were randomized to IA triamcinolone acetonide injection by conventional palpation-guided anatomic injection or sonographic image-guided injection enhanced with a one-handed control syringe (the reciprocating device). A one-needle, 2-syringe technique was used, where the first syringe was used to introduce the needle, aspirate any effusion, and anesthetize and dilate the IA space with lidocaine. After IA placement and synovial space dilation were confirmed, a syringe exchange was performed, and corticosteroid was injected with the second syringe through the indwelling IA needle. Baseline pain, procedural pain, pain at outcome (2 weeks), and changes in pain scores were measured with a 0-10 cm visual analog pain scale (VAS).\nRelative to conventional palpation-guided methods, sonographic guidance resulted in 43.0% reduction in procedural pain (p<0.001), 58.5% reduction in absolute pain scores at the 2 week outcome (p<0.001), 75% reduction in significant pain (VAS pain score>or = 5 cm; p<0.001), 25.6% increase in the responder rate (reduction in VAS score>or = 50% from baseline; p<0.01), and 62.0% reduction in the nonresponder rate (reduction in VAS score<50% from baseline; p<0.01). Sonography also increased detection of effusion by 200% and volume of aspirated fluid by 337%.\"\nQuestion:\n\"Does sonographic needle guidance affect the clinical outcome of intraarticular injections?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18439500": {
                "source": [
                    "\"To assess whether it is possible for an experienced laparoscopic surgeon to perform efficient laparoscopic myomectomy regardless of the size, number, and location of the myomas.\nProspective observational study (Canadian Task Force classification II-1).\nTertiary endoscopy center.\nA total of 505 healthy nonpregnant women with symptomatic myomas underwent laparoscopic myomectomy at our center. No exclusion criteria were based on the size, number, or location of myomas.\nLaparoscopic myomectomy and modifications of the technique: enucleation of the myoma by morcellation while it is still attached to the uterus with and without earlier devascularization.\nIn all, 912 myomas were removed in these 505 patients laparoscopically. The mean number of myomas removed was 1.85 +/- 5.706 (95% CI 1.72-1.98). In all, 184 (36.4%) patients had multiple myomectomy. The mean size of the myomas removed was 5.86 +/- 3.300 cm in largest diameter (95% CI 5.56-6.16 cm). The mean weight of the myomas removed was 227.74 +/- 325.801 g (95% CI 198.03-257.45 g) and median was 100 g. The median operating time was 60 minutes (range 30-270 minutes). The median blood loss was 90 mL (range 40-2000 mL). Three comparisons were performed on the basis of size of the myomas (<10 cm and>or=10 cm in largest diameter), number of myomas removed (<or=4 and>or=5 myomas), and the technique (enucleation of the myomas by morcellation while the myoma is still attached to the uterus and the conventional technique). In all these comparisons, although the mean blood loss, duration of surgery, and hospital stay were greater in the groups in which larger myomas or more myomas were removed or the modified technique was performed as compared with their corresponding study group, the weight and size of removed myomas were also proportionately larger in these groups. Two patients were given the diagnosis of leiomyosarcoma in their histopathology and 1 patient developed a diaphragmatic parasitic myoma followed by a leiomyoma of the sigmoid colon. Six patients underwent laparoscopic hysterectomy 4 to 6 years after the surgery for recurrent myomas. One conversion to laparotomy occurred and 1 patient underwent open subtotal hysterectomy for dilutional coagulopathy.\"\nQuestion:\n\"Laparoscopic myomectomy: do size, number, and location of the myomas form limiting factors for laparoscopic myomectomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24946973": {
                "source": [
                    "\"To evaluate accelerated partial breast irradiation (APBI) in patients after oncoplastic surgery for early breast cancer.\nA retrospective analysis of 136 breasts of 134 patients, who received breast-conserving oncoplastic surgery for low-risk breast cancer between 2002 and 2010 in the Universities of Vienna and Luebeck followed by adjuvant APBI applying total doses of pulse dose rate of 50.4\u00a0Gy or high-dose rate (HDR) of 32\u00a0Gy over 4\u00a0days. Target volume definition was performed by the use of surgical-free margin data, related to intraoperatively fixed clip positions, pre- and postoperative imaging, and palpation.\nAt the time of data acquisition, 131 of 134 patients were alive. The median follow-up time was 39\u00a0months (range, 4-106 months). After high-dose rate treatment, 3 of 89 patients showed systemic progress after a mean follow-up of 47\u00a0months (range, 19-75 months) and 2 patients had a different quadrant in-breast tumor after 27 and 35\u00a0months. One patient died 7\u00a0months after treatment of unknown causes. After pulse dose rate treatment, 1 of 45 patients had a local recurrence after 42\u00a0months and 1 patient died because of another cause after 13\u00a0months. We observed mild fibrosis in 27 breasts, telangiectasia in 6, hyperpigmentation in 14 cases, and keloid formation in\u00a01.\"\nQuestion:\n\"Is oncoplastic surgery a contraindication for accelerated partial breast radiation using the interstitial multicatheter brachytherapy method?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24783217": {
                "source": [
                    "\"Currently the choice of breast cancer therapy is based on prognostic factors. The proliferation marker Ki-67 is used increasingly to determine the method of therapy. The current study analyses the predictive value of Ki-67 in foreseeing breast cancer patients' responses to neoadjuvant chemotherapy.\nThis study includes patients with invasive breast cancer treated between 2008 and 2013. The clinical response was assessed by correlating Ki-67 to histological examination, mammography, and ultrasonography findings.\nThe average Ki-67 value in our patients collectively (n = 77) is 34.9 \u00b1 24.6%. The average Ki-67 value is the highest with 37.4 \u00b1 24.0% in patients with a pCR. The Ki-67 values do not differ significantly among the 3 groups: pCR versus partial pathological response versus stable disease/progress (P = 0.896). However, Ki-67 values of patients with luminal, Her2 enriched, and basal-like cancers differed significantly from each other. Furthermore, within the group of luminal tumors Ki-67 values of patients with versus without pCR also differed significantly.\"\nQuestion:\n\"Can ki-67 play a role in prediction of breast cancer patients' response to neoadjuvant chemotherapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12221908": {
                "source": [
                    "\"The principal causes of morbidity and mortality during pregnancy in Mexico, are preeclampsia/eclampsia, obstetric hemorrhage and puerperium complications; this is, 62% of maternal deaths in last years. HELLP syndrome was observed between 5 to 25% of the mortality in pregnancies of 36 weeks or less.\nTo analyze patients with HELLP syndrome in ICU's (Intensive Care Unit) of a Gynecology and Obstetric Hospital, related to the abnormal hematological, hepatic and renal results with the obstetric case history and the clinical complications.\nA transversal study in patients with HELLP syndrome during 1998 and 1999 were carry out.\nPeripheral blood with Microangiopathic hemolysis, elevated liver enzymes: AST, ALT over 40 UI/L, even when were LDH lower than 600 UI/L. It was evaluated the hepatic and renal function, platelets count, microangiopathic hemolysis, arterial pressure, seizures, icteric skin color, blindness, visual disturbances, nausea, vomiting and upper quadrant right abdominal pain. In newborn we analyzed gestational age, sex, weight and APGAR. We studied for an association between maternal and biochemical variables with Correlation Pearson Test, and dependence between variables with lineal regression model.\n2878 patients with hypertensives disorders in pregnancy (11.64%). The 1.15% (n = 33) had HELLP syndrome with specific maternal mortality of 0.4 per 10,000 live birth, perinatal mortality of 1.62 per 10,000 live birth; and renal damage in 84.5%. Coefficient beta was higher between number of pregnancies to platelets count (-0.33) and creatinine clearance (-0.401).\"\nQuestion:\n\"The HELPP syndrome--evidence of a possible systemic inflammatory response in pre-eclampsia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20605051": {
                "source": [
                    "\"Reimbursement based on the total care of a patient during an acute episode of illness is believed to stimulate management and clinicians to reduce quality problems like waiting times and poor coordination of care delivery. Although many studies already show that this kind of case-mix based reimbursement leads to more efficiency, it remains unclear whether care coordination improved as well. This study aims to explore whether case-mix based reimbursement stimulates development of care coordination by the use of care programmes, and a process-oriented way of working.\nData for this study were gathered during the winter of 2007/2008 in a survey involving all Dutch hospitals. Descriptive and structural equation modelling (SEM) analyses were conducted.\nSEM reveals that adoption of the case-mix reimbursement within hospitals' budgeting processes stimulates hospitals to establish care programmes by the use of process-oriented performance measures. However, the implementation of care programmes is not (yet) accompanied by a change in focus from function (the delivery of independent care activities) to process (the delivery of care activities as being connected to a chain of interdependent care activities).\"\nQuestion:\n\"Does case-mix based reimbursement stimulate the development of process-oriented care delivery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25277731": {
                "source": [
                    "\"Sternal fractures in childhood are rare. The aim of the study was to investigate the accident mechanism, the detection of radiological and sonographical criteria and consideration of associated injuries.\nIn the period from January 2010 to December 2012 all inpatients and outpatients with sternal fractures were recorded according to the documentation.\nA total of 4 children aged 5-14\u00a0years with a sternal fracture were treated in 2\u00a0years, 2\u00a0children were hospitalized for pain management and 2 remained in outpatient care.\"\nQuestion:\n\"Sternal fracture in growing children : A rare and often overlooked fracture?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24785562": {
                "source": [
                    "\"A short course of systemic corticosteroids is an important therapy in the treatment of pediatric asthma exacerbations. Although a 5-day course of oral prednisone or prednisolone has become the most commonly used regimen, dexamethasone has also been used for a shorter duration (1-2 days) with potential for improvement in compliance and palatability. We reviewed the literature to determine if there is sufficient evidence that dexamethasone can be used as an effective alternative in the treatment of pediatric asthma exacerbations in the inpatient setting.\nA Medline search was conducted on the use of dexamethasone in the treatment of asthma exacerbations in children. The studies selected were clinical trials comparing the efficacy of dexamethasone with prednisone. Meta-analysis was performed examining physician revisitation rates and symptomatic return to baseline.\nSix completed pediatric clinical trials met the inclusion criteria. All of the pediatric trials found that prednisone is not superior to dexamethasone in treating mild to moderate asthma exacerbations. Meta-analysis demonstrated homogeneity between the dexamethasone and prednisone groups when examining symptomatic return to baseline and unplanned physician revisits after the initial emergency department encounter. Some studies found potential additional benefits of dexamethasone, including improved compliance and less vomiting.\"\nQuestion:\n\"Is dexamethasone an effective alternative to oral prednisone in the treatment of pediatric asthma exacerbations?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10490564": {
                "source": [
                    "\"To determine whether anginal episodes might be related to extremes of hypotension in patients with ischaemic heart disease taking drugs to treat angina and heart failure.\nObservational study of patients with ischaemic heart disease attending an urban tertiary referral cardiology centre.\nA selected patient population was enrolled, having: angina on one or more hypotensive cardiovascular medications; hypotension on clinic or ambulatory measurement; and a resting ECG suitable for ambulatory monitoring. Patients had echocardiography, ambulatory blood pressure monitoring, and Holter monitoring. Hypotension induced ischaemic (HII) events were defined as episodes of ST segment ischaemia occurring at least one minute after an ambulatory blood pressure measurement (systolic/diastolic) below 100/65 mm Hg during the day, or 90/50 mm Hg at night.\n25 suitable patients were enrolled, and 107 hypotensive events were documented. 40 ST events occurred in 14 patients, of which a quarter were symptomatic. Fourteen HII events occurred in eight patients, with 13 of the 14 preceded by a fall in diastolic pressure (median diastolic pressure 57.5 mm Hg, interquartile range 11, maximum 72 mm Hg, minimum 45 mm Hg), and six preceded by a fall in systolic pressure (chi(2) = 11.9, p<0.001). ST events were significantly associated with preceding hypotensive events (chi(2) = 40.2, p<0.0001). Patients with HII events were more frequently taking multiple hypotensive drug regimens (8/8 v 9/17, chi(2) = 5.54, p = 0.022).\"\nQuestion:\n\"Hypotension in patients with coronary disease: can profound hypotensive events cause myocardial ischaemic events?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21756515": {
                "source": [
                    "\"Medical units at an academic tertiary referral hospital in Southern India.\nTo investigate the impact of solid culture on L\u00f6wenstein-Jensen medium on clinical decision making.\nIn a retrospective review of 150 culture-positive and 150 culture-negative consecutively sampled tuberculosis (TB) suspects, treatment decisions were analysed at presentation, after the availability of culture detection results and after the availability of drug susceptibility testing (DST) culture results.\nA total of 124 (82.7%) culture-positive patients and 35 (23.3%) culture-negative patients started anti-tuberculosis treatment prior to receiving their culture results; 101 patients (33.7%) returned for their results; two (1.3%) initiated treatment based on positive culture and no culture-negative patients discontinued treatment. DST was performed on 119 (79.3%) positive cultures: 30 (25.2%) showed any resistance, eight (6.7%) showed multidrug resistance and one (0.84%) showed extensively drug-resistant TB. Twenty-eight patients (23.5%) returned for their DST results. Based on DST, treatment was modified in four patients (3.4%).\"\nQuestion:\n\"Does solid culture for tuberculosis influence clinical decision making in India?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26686513": {
                "source": [
                    "\"The precise correction of refractive error is especially important in young adults. It is unclear whether cycloplegic refraction is necessary in this age group. The purpose of this study was to compare the non-cycloplegic and cycloplegic spherical equivalent (SE) refractive error measured in young adults.\nThis was a prospective study of 1400 eyes (n\u2009=\u2009700) of enlisted soldiers aged 18 to 21\u00a0years who were consecutively evaluated in an outpatient army ophthalmology clinic. One drop of cyclopentolate 1\u00a0% was installed twice 10\u00a0min apart, and cycloplegic refraction was performed in both eyes 40\u00a0min later using an auto-refractor. The difference between non-cycloplegic and cycloplegic refractive measurements was analyzed.\nThe mean difference in SE between non-cycloplegic and cycloplegic measurements was 0.68\u2009\u00b1\u20090.83\u00a0D (95\u00a0% CI, 0.64-0.72). Significantly greater differences were observed in hypermetropes than myopes (1.30\u2009\u00b1\u20090.90\u00a0D versus 0.46\u2009\u00b1\u20090.68\u00a0D, p\u2009<\u20090.001). Moderate hypermetropes (2 to 5\u00a0D) demonstrated significantly greater refractive error than mild (0.5 to 2\u00a0D) or severe (>5\u00a0D) hypermetropes (1.71\u2009\u00b1\u20091.18\u00a0D versus 1.19\u2009\u00b1\u20090.74\u00a0D and 1.16\u2009\u00b1\u20091.08\u00a0D respectively, p\u2009<\u20090.001).\"\nQuestion:\n\"Cycloplegic autorefraction in young adults: is it mandatory?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17096624": {
                "source": [
                    "\"To examine patterns of knowledge and attitudes among adults aged>65 years unvaccinated for influenza.\nSurveyed Medicare beneficiaries in 5 areas; clustered unvaccinated seniors by their immunization related knowledge and attitudes.\nIdentified 4 clusters: Potentials (45%) would receive influenza vaccine to prevent disease; Fearful Uninformeds (9%) were unsure if influenza vaccine causes illness; Doubters (27%) were unsure if vaccine is efficacious; Misinformeds (19%) believed influenza vaccine causes illness. More Potentials (75%) and Misinformeds (70%) ever received influenza vaccine than did Fearful Uninformeds (18%) and Doubters (29%).\"\nQuestion:\n\"Do patterns of knowledge and attitudes exist among unvaccinated seniors?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25752912": {
                "source": [
                    "\"To compare the probability of prenatal diagnosis (PND) and termination of pregnancy for fetal anomaly (TOPFA) between fetuses conceived by assisted reproductive techniques (ART) and spontaneously-conceived fetuses with congenital heart defects (CHD).\nPopulation-based observational study.\nParis and surrounding suburbs.\nFetuses with CHD in the Paris registry of congenital malformations and cohort of children with CHD (Epicard).\nComparison of ART-conceived and spontaneously conceived fetuses taking into account potential confounders (maternal characteristics, multiplicity and year of birth or TOPFA).\nProbability and gestational age at PND and TOPFA for ART-conceived versus spontaneously conceived fetuses.\nThe probability of PND (28.1% versus 34.6%, P = 0.077) and TOPFA (36.2% versus 39.2%, P = 0.677) were not statistically different between ART-conceived (n = 171) and spontaneously conceived (n = 4620) fetuses. Estimates were similar after adjustment for potential confounders. Gestational age at PND tended to be earlier for ART fetuses (23.1 versus 24.8 weeks, P = 0.05) but no statistical difference was found after adjustment for confounders. Gestational age at TOPFA was comparable between ART-conceived and spontaneously conceived fetuses.\"\nQuestion:\n\"Is the probability of prenatal diagnosis or termination of pregnancy different for fetuses with congenital anomalies conceived following assisted reproductive techniques?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23598835": {
                "source": [
                    "\"Severe, immediate postprocedural pain and the need for analgesics after vertebroplasty can be a discouraging experience for patients and caregivers. The goal of this study was to investigate whether the presence of severe pain immediately after vertebroplasty predicts short- and long-term pain relief.\nA chart review was performed to categorize patients regarding pain severity and analgesic usage immediately after vertebroplasty (<4 h). \"Severe\" pain was defined as at least 8 of 10 with the 10-point VAS. Outcomes were pain severity and pain medication score and usage at 1 month and 1 year after vertebroplasty. Outcomes and clinical characteristics were compared between groups by using the Wilcoxon signed-rank test and the Fisher exact test.\nOf the 429 vertebroplasty procedures identified, 69 (16%) were associated with severe pain, and 133 (31%) were associated with analgesic administration immediately after the procedure. The group experiencing severe pain had higher preprocedure median VAS rest pain scores (5 [IQR, 2-7]) and activitypain scores (10 [IQR, 8-10]) compared with patients who did not experience severe pain (3 [IQR, 1-6]; P = .0208, and 8 [IQR, 7-10]; P = .0263, respectively). At 1 month postprocedure, VAS rest and activity pain scores were similar between the severe pain group and the nonsevere pain group (P = .16 and P = .25, respectively) and between the group receiving pain medication and the group not receiving pain medication (P = .25 and P = .67, respectively). This similarity continued for 1 year after the procedure. Analgesic usage was similar among all groups at 1 year postprocedure.\"\nQuestion:\n\"Is severe pain immediately after spinal augmentation a predictor of long-term outcomes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23375036": {
                "source": [
                    "\"To determine whether the OraQuick\u00ae HIV-1/2 Assay (OraSure Technologies, Inc., Bethlehem, PA, USA) in sputum is a valid tool for HIV surveillance among TB patients.\nA cross sectional study was carried out on sputa of patients diagnosed with tuberculosis. Sputa were tested for antibodies to HIV using OraQuick\u00ae HIV-1/2 Assay (OraSure Technologies, Inc., Bethlehem, PA, USA). The results were compared with results of serum ELISA.\nCompared to serum ELISA, the OraQuick\u00ae HIV-1/2 Assay in sputum specimens reported 90% sensitivity (9/10) and 100% specificity (307/307), with a positive predictive value of 100% (95%CI: 66.37%-100.00%) and a negative predictive value of 99.68% (95%CI: 98.20%-99.99%).\"\nQuestion:\n\"An HIV1/2 point of care test on sputum for screening TB/HIV co-infection in Central India - Will it work?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10605400": {
                "source": [
                    "\"As part of an MRC funded study into primary care oral anticoagulation management, INR measurements obtained in general practice were validated against values on the same samples obtained in hospital laboratories. A prospective comparative trial was undertaken between three hospital laboratories and nine general practices. All patients attending general practice based anticoagulant clinics had parallel INR estimations performed in general practice and in a hospital laboratory.\n405 tests were performed. Comparison between results obtained in the practices and those in the reference hospital laboratory (gold standard), which used the same method of testing for INR, showed a correlation coefficient of 0.96. Correlation coefficients comparing the results with the various standard laboratory techniques ranged from 0.86 to 0.92. It was estimated that up to 53% of tests would have resulted in clinically significant differences (change in warfarin dose) depending upon the site and method of testing. The practice derived results showed a positive bias ranging from 0.28 to 1.55, depending upon the site and method of testing.\"\nQuestion:\n\"Is the international normalised ratio (INR) reliable?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20736887": {
                "source": [
                    "\"A retrospective multicenter study of series of 12 patients with spinal cord sarcoidosis who underwent surgery.\nTo evaluate the postoperative outcomes of patients with cervical spinal cord sarcoidosis accompanied with compressive myelopathy and effect of decompressive surgery on the prognosis of sarcoidosis.\nSarcoidosis is a chronic, multisystem noncaseating granulomatous disease. It is difficult to differentiate spinal cord sarcoidosis from cervical compressive myelopathy. There are no studies regarding the coexistence of compressive cervical myelopathy with cervical spinal cord sarcoidosis and the effect of decompressive surgery.\nNagoya Spine Group database included 1560 cases with cervical myelopathy treated with cervical laminectomy or laminoplasty from 2001 to 2005. A total of 12 patients (0.08% of cervical myelopathy) were identified spinal cord sarcoidosis treated with decompressive surgery. As a control subject, 8 patients with spinal cord sarcoidosis without compressive lesion who underwent high-dose steroid therapy without surgery were recruited.\nIn the surgery group, enhancing lesions on magnetic resonance imaging (MRI) were mostly seen at C5-C6, coincident with the maximum compression level in all cases. Postoperative recovery rates in the surgery group at 1 week and 4 weeks were -7.4% and -1.1%, respectively. Only 5 cases had showed clinical improvement, and the condition of these 5 patients had worsened again at averaged 7.4 weeks after surgery. Postoperative oral steroid therapy was initiated at an average of 6.4 weeks and the average initial dose was 54.0 mg in the surgery group, while 51.3 mg in the nonsurgery group. The recovery rate of the Japanese Orthopedic Association score, which increased after steroid therapy, was better in the nonsurgery group (62.5%) than in the surgery group (18.6%) with significant difference (P<0.01).\"\nQuestion:\n\"Is decompressive surgery effective for spinal cord sarcoidosis accompanied with compressive cervical myelopathy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17483607": {
                "source": [
                    "\"The effect of topical N-acetylcysteine (NAC) application was investigated on the healing of acute experimental tympanic membrane perforations.\nTwenty guinea pigs were used in this study. Under intraperitoneal ketamine anesthesia, incisional myringotomies were performed in the posterosuperior quadrant of the tympanic membranes with a straight otologic hook. The diameter of the perforations was approximately 2 mm. Perforations in both ears were treated with freshly prepared sponges soaked in either 0.1 ml 0.9% NaCl solution (10 control animals) or 0.6 mg/0.1 ml NAC (10 animals) for three consecutive days. All the tympanic membranes were examined by otomicroscopy on the third, fifth, seventh, and ninth days.\nIn the control group, all the perforations were completely closed at the end of nine days. During the same period, only 40% of the perforations were completely closed in the NAC group. The remaining ears exhibited otorrhea by the third day.\"\nQuestion:\n\"Does topical N-acetylcysteine application after myringotomy cause severe otorrhea?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12068831": {
                "source": [
                    "\"To study whether nontriploid partial hydatidiform moles truly exist.\nWe conducted a reevaluation of pathology and ploidy in 19 putative nontriploid partial hydatidiform moles using standardized histologic diagnostic criteria and repeat flow cytometric testing by the Hedley technique.\nOn review of the 19 moles, 53% (10/19) were diploid nonpartial moles (initially pathologically misclassified), and 37% (7/19) were triploid partial moles (initial ploidy misclassifications). One additional case (5%) was a diploid early complete mole (initially pathologically misclassified).\"\nQuestion:\n\"Do nontriploid partial hydatidiform moles exist?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10834864": {
                "source": [
                    "\"Avascular necrosis of bone (AVN) is a well known complication in patients with systemic lupus erythematosus (SLE).\nTo investigate the role of antiphospholipid antibody status (IgM and IgG anticardiolipin antibodies and lupus anticoagulant) with adjustment for corticosteroid use as risk factors for the development of AVN.\nA cohort of 265 patients receiving long term follow up in our SLE clinic from 1978 to 1998 was analysed. Patients with AVN complications were detected and then matched for age, sex, ethnicity, duration of disease, and organ disease with two other patients with SLE. A further 31 patients were chosen at random for the analysis.\nEleven patients had AVN, giving a point prevalence of 4%. There were no significant differences demonstrable in the presence of individual antiphospholipid antibodies (aPL) or their combination between the group with AVN or the two control groups.\"\nQuestion:\n\"Risk factors for avascular necrosis of bone in patients with systemic lupus erythematosus: is there a role for antiphospholipid antibodies?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15687156": {
                "source": [
                    "\"Unicompartmental replacement can be an alternative to tibial osteotomy in younger, active patients with unicompartmental knee disease. In unicompartmental replacement, the other compartments and knee ligaments are largely untouched. Therefore, it was hypothesized that the knee kinematics after unicompartmental replacement may also be unchanged. To test this hypothesis, knee kinematics and quadriceps tension were recorded before and after replacement with a unicompartmental design and then with a tricompartmental design.\nSix human cadaver knees were tested before implantation, after implantation with a bicruciate-retaining unicompartmental knee prosthesis, and after implantation with a posterior cruciate-retaining tricompartmental knee prosthesis. The unicompartmental prosthesis was initially implanted, and it was then revised to a total condylar knee replacement. The knee kinematics were measured with use of an electromagnetic tracking device while the knee was put through dynamic simulated stair-climbing under peak flexion moments of approximately 40 N-m. Quadriceps tension was also measured for all three conditions.\nNo significant differences in tibial axial rotation were noted between the intact and unicompartmental conditions. However, tricompartmental replacement significantly affected tibial axial rotation (p = 0.001). Femoral rollback was not significantly affected by either unicompartmental or tricompartmental arthroplasty. Quadriceps tension was also similar among all three conditions.\"\nQuestion:\n\"Can normal knee kinematics be restored with unicompartmental knee replacement?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18928979": {
                "source": [
                    "\"The objective of the study was to determine whether myometrial electrical activity can differentiate false from true preterm labor.\nElectrical uterine myography (EUM) was measured prospectively on 87 women, gestational age less than 35 weeks. The period between contractions, power of contraction peaks and movement of center of electrical activity (RMS), was used to develop an index score (1-5) for prediction of preterm delivery (PTD) within 14 days of the test. The score was compared with fetal fibronectin (fFN) and cervical length (CL).\nPatients delivering within 14 days from testing showed a higher index and mean RMS (P = .000). No patients with EUM index scores of 1-2 delivered in this time frame. Combining EUM with CL or fFN increased predictability. Logistic regression revealed that history of PTD and EUM index had 4- to 5-fold increased risk for PTD. Gestational age at testing, body mass index, fFN, and CL were nonsignificant contributors to PTD risk.\"\nQuestion:\n\"Can myometrial electrical activity identify patients in preterm labor?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26505821": {
                "source": [
                    "\"The levels of bone formation and resorption can be assessed at the tissue level by bone histomorphometry on transiliac bone biopsies. Systemic biochemical markers of bone turnover reflect the overall bone formation and resorption at the level of the entire skeleton but cannot discriminate the different skeletal compartments.\nOur aim was to investigate the correlations between the serum biochemical markers of formation and resorption with histomorphometric parameters.\nWe performed post hoc analysis of a previous clinical study.\nPatients were selected from the general population.\nA total of 371 untreated postmenopausal osteoporotic women aged 50 to 84 years with a lumbar T-score \u2264 -2.5 SD or \u2264 -1 SD with at least one osteoporotic fracture.\nTransiliac bone biopsies were obtained after a double tetracycline labeling, and blood samples were collected.\nThe static and dynamic parameters of formation and bone resorption were measured by histomorphometry. Serum biochemical markers of formation (bone alkaline phosphatase [ALP]; procollagen type I N-terminal propeptide [PINP]) and resorption (C-terminal crosslinking telopeptide of collagen type 1 [sCTX]) were assessed.\nThe mean values of biochemical markers were: bone ALP, 15.0 \u00b1 5.2 ng/mL; PINP, 56.2 \u00b1 21.9 \u03bcg/mL; and sCTX, 0.58 \u00b1 0.26 ng/mL. Bone ALP and PINP were significantly correlated with both the static and dynamic parameters of formation (0.21 \u2264 r' \u2264 0.36; 0.01 \u2265 P \u2265 .0001). sCTX was significantly correlated with all resorption parameters (0.18 \u2264 r' \u2264 0.24; 0.02 \u2265 P \u2265 .0001).\"\nQuestion:\n\"Are Biochemical Markers of Bone Turnover Representative of Bone Histomorphometry in 370 Postmenopausal Women?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15670262": {
                "source": [
                    "\"Severe upper gastrointestinal (GI) motor disorders, including gastroparesis (GP), can consume significant health care resources. Many patients are refractory to traditional drug therapy.\nTo compare symptoms, healthcare resource utilization and costs in two groups of patients with the symptoms of GP: those treated via gastric electrical stimulation (GES) and those treated with traditional pharmacological agents in an intensive outpatient program (MED).\nA long-term comparison of patients with devices (n = 9) vs intensive medical therapy (n = 9).\nA total of 18 eligible patients with the symptoms of GP reported for 1-year baseline and long-term treatment for 3 years.\nPatients with the symptoms of GP were treated by a GES or intensive medical therapy (MED).\nGP Symptoms, healthcare resource utilization using investigator-derived independent outcome measure score (IDIOMS) and total hospital (inpatient and outpatient) billing costs.\nGastrointestinal symptoms were significantly different from baseline (F = 3.03, P<0.017) with GP patients treated via GES showing more sustained improvement over 36 months than those treated via MED. Healthcare resource usage, measured via the IDIOMS, significantly improved at 12, 24 and 36 month follow-up for GES patients (F = 10.49, P<0.001), compared with patients receiving medical therapy, who demonstrated further deterioration. GP patients treated via GES also proved superior to medical therapy at 24 and 36 months with regard to decreased costs (F = 4.85, P<0.001). Within group comparisons indicated significantly reduced hospital days for both patient groups; however, no statistical differences were noted between groups in terms of hospital days. Three of nine patients in the MED group died primarily from i.v. access related problems; none of the GES patients died.\"\nQuestion:\n\"Is gastric electrical stimulation superior to standard pharmacologic therapy in improving GI symptoms, healthcare resources, and long-term health care benefits?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25501465": {
                "source": [
                    "\"There is heterogeneity in how pediatric voiding cystourethrography (VCUG) is performed. Some institutions, including our own, obtain a radiographic scout image prior to contrast agent instillation.\nTo demonstrate that the radiographic scout image does not augment VCUG interpretation or contribute management-changing information but nonetheless carries a non-negligible effective dose.\nWe evaluated 181 children who underwent VCUG in 2012, with an age breakdown of less than 1\u00a0year (56 children), 1-5 years (66 children), 6-10 years (43 children) and 11-18 years (16 children), with a mean age of 4.0\u00a0years. We investigated patient demographics, clinical indication for the examination, scout image findings and estimated effective radiation dose, as well as overall exam findings and impression.\nNo clinically significant or management-changing findings were present on scout images, and no radiopaque urinary tract calculi or concerning incidental finding was identified. Scout image estimated effective radiation dose averaged 0.09\u00a0mSv in children younger than 1\u00a0y, 0.09\u00a0mSv in children age 1-5, 0.13\u00a0mSv in children age 6-10 and 0.18\u00a0mSv in children age 11-18. Total fluoroscopy time per examination averaged 36.7\u00a0s (range 34.8-39.6\u00a0s for all age group averages). Evaluation of known or suspected vesicoureteral reflux (VUR) and urinary tract infection (UTI) were the most common clinical indications, stated in 40.9% and 37.0% of exams, respectively.\"\nQuestion:\n\"Evaluation of pediatric VCUG at an academic children's hospital: is the radiographic scout image necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14692023": {
                "source": [
                    "\"Despite advances in therapies for breast cancer, improvement in survival for patients with recurrent or metastatic breast cancer has been difficult to establish. The objective of the current study was to determine whether the survival of women with recurrent breast cancer has improved from 1974 to 2000.\nThe authors analyzed the survival experience of 834 women who developed recurrent breast cancer between November 1974 and December 2000. All patients had been treated previously with adjuvant anthracycline-based protocols. Patients were divided into five consecutive groups based on year of breast cancer recurrence, and survival was compared across the five groups. Because some prognostic variables were divided unevenly divided among the cohorts, a multivariate model was created to determine the association of year of recurrence and survival after accounting for other prognostic factors.\nIn the unadjusted analysis, there was a statistically significant improvement in survival across the five groups, and the more recent cohorts had longer survival (P<0.001). Other variables that predicted longer survival after breast cancer recurrence included smaller initial tumor size, lower stage of disease, fewer lymph nodes involved, longer disease-free interval, estrogen receptor-positive tumors, and nonvisceral dominant site of disease recurrence. In the multivariate analysis, which adjusted for these prognostic factors, year of recurrence was associated with a trend toward improved survival, with a 1% reduction in risk for each increasing year.\"\nQuestion:\n\"Is breast cancer survival improving?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19925761": {
                "source": [
                    "\"To assess the feasibility and safety of diagnostic or therapeutic semirigid ureteroscopy without ureteral meatus dilatation.\nA comparative, retrospective study was conducted of patients undergoing ureteroscopy from January 2000 to May 2008. For data analysis purposes, the population was divided into two groups based on whether ureteroscopy had been performed with (Group 1) or without (Group 2) ureteral meatus dilatation. Variables recorded included age, sex, type of procedure, surgical diagnosis, passive or active dilatation, number of stones, stone location, stone diameter, peroperative and postoperative complications, internal urinary diversion after the procedure, therapeutic success rate, operating time, and hospital stay duration. A 8-9.8 Fr Wolf semirigid ureteroscope was used. Descriptive statistics of the population and cohorts were performed, providing medians, quartiles, and limit values for non-normally distributed interval variables, and absolute and relative frequencies for categorical variables. Shapiro-Wilk's, Mann-Whitney's U, Chi-square, and Fisher's exact tests were used for statistical analysis. A value of p 2 alpha<or = 0.005 was considered statistically significant. Arcus Quickstat Biomedical 1.0 software was used.\nAmong the 306 ureteroscopies studied, 286 performed in 256 patients were analyzed. Median age was 50 years (16-83), 59% of patients were male, and elective ureteroscopy was performed in 183 patients (64%). Group 1: 191 ureteroscopies, Group 2: 95 ureteroscopies. Stone location: 149 in distal ureter, 60 in middle ureter, and 35 in proximal ureter. Sixty-nine percent of stones had sizes ranging from 5 and 10 mm. The overall success rate was 86.5%. There were 5 peroperative and 22 postoperative complications, with no statistically significant differences between the groups.\"\nQuestion:\n\"Diagnostic and therapeutic ureteroscopy: is dilatation of ureteral meatus always necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24671913": {
                "source": [
                    "\"SYNTAX score (SxS) has been demonstrated to predict long-term outcomes in stable patients with coronary artery disease. But its prognostic value for patients with acute coronary syndrome remains unknown.AIM: To evaluate whether SxS could predict in-hospital outcomes for patients admitted with ST elevation myocardial infarction (STEMI) who undergo primary percutaneous coronary intervention (pPCI).\nThe study included 538 patients with STEMI who underwent pPCI between January 2010 and December 2012. The patients were divided into two groups: low SxS (<22) and high SxS (>22). The SxS of all patients was calculated from aninitial angiogram and TIMI flow grade of infarct related artery was calculated after pPCI. Left ventricular systolic functions of the patients were evaluated with an echocardiogram in the following week. The rates of reinfarction and mortality during hospitalisation were obtained from the medical records of our hospital.\nThe high SxS group had more no-reflow (41% and 25.1%, p<0.001, respectively), lower ejection fraction (38.2 \u00b1 7.5% and 44.6 \u00b1 8.8%, p<0.001, respectively), and greater rates of re-infarction (9.5% and 7.3%, p = 0.037, respectively) and mortality (0.9% and 0.2%, p = 0.021, respectively) during hospitalisation compared to the low SxS group. On multivariate logistic regression analysis including clinical variables, SxS was an independent predictor of no-reflow (OR 1.081, 95% CI 1.032-1.133, p = 0.001).\"\nQuestion:\n\"Does SYNTAX score predict in-hospital outcomes in patients with ST elevation myocardial infarction undergoing primary percutaneous coronary intervention?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15787677": {
                "source": [
                    "\"Twenty-seven healthy normal glucose-tolerant humans with either a previous diagnosis of gestational diabetes or having two parents with Type 2 diabetes and 27 healthy adults who had no history of diabetes were recruited. Maximal oxygen uptake was assessed using an incremental exercise test to exhaustion. Skin microvascular function was assessed using laser Doppler techniques as the maximum skin hyperaemic response to a thermal stimulus (maximum hyperaemia) and the forearm skin blood flow response to the iontophoretic application of acetylcholine (ACh) and sodium nitroprusside.\nMaximal oxygen uptake was not significantly different in the 'at-risk' group compared with healthy controls. Maximum hyperaemia was reduced in those 'at risk' (1.29 +/- 0.30 vs. 1.46 +/- 0.33 V, P = 0.047); however, the peak response to acetylcholine or sodium nitroprusside did not differ in the two groups. A significant positive correlation was demonstrated between maximal oxygen uptake and maximum hyperaemia (r = 0.52, P = 0.006 l/min and r = 0.60, P = 0.001 ml/kg/min) and peak ACh response (r = 0.40, P = 0.04 l/min and r = 0.47, P = 0.013 ml/kg/min) in the 'at-risk' group when expressed in absolute (l/min) or body mass-related (ml/kg/min) terms. No significant correlations were found in the control group.\"\nQuestion:\n\"Does aerobic fitness influence microvascular function in healthy adults at risk of developing Type 2 diabetes?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27991408": {
                "source": [
                    "\"Dermatomyositis (DM) and polymyositis (PM) commonly cause weakness of the thigh muscles. However, it is debated whether DM and PM affect similar thigh muscles. Muscle oedema on fat-suppressed MRI sequences is thought to represent active inflammation. In this study, we aimed to assess which thigh muscle groups are preferentially inflamed in DM and PM, respectively, using short-tau inversion-recovery MRI sequences.\nWe analysed 71 patients from 2 Rheumatology centres, 31 with DM and 40 with PM diagnosed according to the Bohan and Peter criteria. MRI oedema (1=present, 0=absent) was assessed bilaterally on fat-suppressed sequences in 17 pelvic floor and thigh muscles. An MRI oedema score (range 0-17) was calculated by adding the separate scores bilaterally and dividing them by two. Inter-rater variability was assessed by intraclass correlation coefficient. Fisher's exact test was used to compare binomial data.\nAge and gender ratio were similar in patients with DM and PM. Disease duration (months, mean\u00b1SD) was shorter (20\u00b131) in DM than in PM (53\u00b169) (p=0.02). The intraclass correlation coefficient between the radiologists involved was 0.78. Muscle oedema was more common in DM than in PM except in the posterior thigh muscles. In particular, 68% of patients with DM had involvement of at least one anterior thigh muscle versus 38% of patients with PM (p=0.02).\"\nQuestion:\n\"Do dermatomyositis and polymyositis affect similar thigh muscles?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12848629": {
                "source": [
                    "\"Tuberculosis has increased in parallel with the acquired immunodeficiency syndrome epidemic and the use of immunosuppressive therapy, and the growing incidence of extra-pulmonary tuberculosis, especially with intestinal involvement, reflects this trend. However, the duration of anti-tuberculous therapy has not been clarified in intestinal tuberculosis.AIM: To compare the efficacy of different treatment durations in tuberculous enterocolitis in terms of response and recurrence rates.\nForty patients with tuberculous enterocolitis were randomized prospectively: 22 patients into a 9-month and 18 into a 15-month group. Diagnosis was made either by colonoscopic findings of discrete ulcers and histopathological findings of caseating granuloma and/or acid-fast bacilli, or by clinical improvement after therapeutic trial. Patients were followed up with colonoscopy every other month until complete response or treatment completion, and then every 6 months for 1 year and annually. Complete response was defined as a resolution of symptoms and active tuberculosis by colonoscopy.\nComplete response was obtained in all patients in both groups. Two patients in the 9-month group and one in the 15-month group underwent operation due to intestinal obstruction and perianal fistula, respectively. No recurrence of active intestinal tuberculosis occurred during the follow-up period in either group.\"\nQuestion:\n\"Is a 9-month treatment sufficient in tuberculous enterocolitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24160268": {
                "source": [
                    "\"Breast cancer is the most common malignancy afflicting women, and the most common cancer overall in Jordan. A woman's decision to go for screening is influenced by her social support network. This study aims to explore Jordanian men's individual and contextual perspectives on women's breast cancer and their own role in the breast health of the females within their families.\nAn explorative qualitative design was used to purposively recruit 24 married men aged 27 to 65\u00a0years (median 43\u00a0years) from four governorates in Jordan. Data in the form of interviews transcriptions was subjected to qualitative content analysis.\nThree themes were identified: a) Supporting one's wife; b) Marital needs and obligations; c) Constrained by a culture of destiny and shame. The first theme was built on men's feelings of responsibility for the family's health and well-being, their experiences of encouraging their wives to seek health care and their providing counselling and instrumental support. The second theme emerged from men's views about other men's rejection of a wife inflicted by breast cancer, their own perceptions of diminished femininity due to mastectomy and their own concerns about protecting the family from the hereditary risk of breast cancer. The third theme was seen in men's perception of breast cancer as an inevitable act of God that is far away from one's own family, in associating breast cancer with improper behaviour and in their readiness to face the culture of Eib (shame).\"\nQuestion:\n\"\"Would a man smell a rose then throw it away?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23228527": {
                "source": [
                    "\"Mechanically ventilated patients experience profound stress. Interventions are needed to ameliorate stress that does not cause adverse effects. The purpose of this study was to explore the influence of music on stress in a sample of patients over the duration of ventilatory support.RESEARCH METHODOLOGY/\nRandomised controlled trial; randomised patients (56.8+16.9 years, 61% male, APACHE III 57.2+18.3) receiving ventilatory support to: (1) patient-directed music (PDM) where patients self-initiated music listening whenever desired from a preferred collection, (2) headphones only to block ICU noise, or (3) usual ICU care. Twenty-four hour urinary cortisol samples were collected from a sub-set of subjects with intact renal function and not receiving medications known to influence cortisol levels (n=65).\n12 ICUs in the Midwestern United States.\nUrinary free cortisol (UFC), an integrative biomarker of stress.\nControlling for illness severity, gender, and baseline UFC (29-45 mg/day), mixed models analysis revealed no significant differences among groups in UFC over the course of ventilatory support.\"\nQuestion:\n\"Does music influence stress in mechanically ventilated patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19757704": {
                "source": [
                    "\"To determine the practices and knowledge of harmful effects regarding use of Chaalia and Pan Masala in three schools of Mahmoodabad and Chanesar Goth, Jamshed Town, Karachi, Pakistan.\nTo achieve the objective a cross-sectional design was used in three government schools of Mahmoodabad and Chanesar Goth, Jamshed Town, Karachi. Students of either gender drawn from these schools fulfilling the inclusion and exclusion criteria were interviewed using a pre-coded structured questionnaire. Along with demographic data, questions regarding frequency of Chaalia and Pan Masala use, practices of this habit in friends and family and place of procurement of these substances, were inquired. Knowledge was assessed about harmful effects and its source of information. In addition, practices in relation to that knowledge were assessed.\nA total of 370 students were interviewed over a period of six weeks, of which 205 (55.4%) were boys. The ages of the students were between 10 and 15 years. Thirty one percent of the fathers and 62% of the mothers were uneducated. The frequency of use of any brand of Chaalia was found to be 94% and that of Pan Masala was 73.8%. Eighty five percent of them were regular users. A large majority (88%) procured the substances themselves from near their homes. Ninety five percent of the children had friends with the same habits. Eighty four percent were using the substances in full knowledge of their families. Chaalia was considered harmful for health by 96% and Pan Masala by 60%. Good taste was cited as a reason for continuing the habit by 88.5% of the children and use by friends by 57%. Knowledge about established harmful effects was variable. Knowledge about harmful effects was high in both \"daily\" and \"less than daily users\".\"\nQuestion:\n\"Is Chaalia/Pan Masala harmful for health?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17105833": {
                "source": [
                    "\"Current guidelines for the treatment of uncomplicated urinary tract infection (UTI) in women recommend empiric therapy with antibiotics for which local resistance rates do not exceed 10-20%. We hypothesized that resistance rates of Escherichia coli to fluoroquinolones may have surpassed this level in older women in the Israeli community setting.\nTo identify age groups of women in which fluoroquinolones may no longer be appropriate for empiric treatment of UTI.\nResistance rates for ofloxacin were calculated for all cases of uncomplicated UTI diagnosed during the first 5 months of 2005 in a managed care organization (MCO) in Israel, in community-dwelling women aged 41-75 years. The women were without risk factors for fluoroquinolone resistance. Uncomplicated UTI was diagnosed with a urine culture positive for E. coli. The data set was stratified for age, using 5 year intervals, and stratum-specific resistance rates (% and 95% CI) were calculated. These data were analyzed to identify age groups in which resistance rates have surpassed 10%.\nThe data from 1291 urine cultures were included. The crude resistance rate to ofloxacin was 8.7% (95% CI 7.4 to 10.2). Resistance was lowest among the youngest (aged 41-50 y) women (3.2%; 95% CI 1.11 to 5.18), approached 10% in women aged 51-55 years (7.1%; 95% CI 3.4 to 10.9), and reached 19.86% (95% CI 13.2 to 26.5) among the oldest women (aged 56-75 y).\"\nQuestion:\n\"Empiric treatment of uncomplicated urinary tract infection with fluoroquinolones in older women in Israel: another lost treatment option?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23448747": {
                "source": [
                    "\"To examine whether a history of cancer increased the likelihood of a fall in community-dwelling older adults, and if cancer type, stage, or time since diagnosis increased falls.\nA longitudinal, retrospective, cohort study.\nA home- and community-based waiver program in Michigan.\n862 older adults aged 65 years or older with cancer compared to 8,617 older adults without cancer using data from the Minimum Data Set-Home Care and Michigan cancer registry.\nReports of falls were examined for 90-180 days. Generalized estimating equations were used to compare differences between the groups.\nCancer, falls, patient characteristics, comorbidities, medications, pain, weight loss, vision, memory recall, and activities, as well as cancer type, stage, and time since diagnosis.\nA fall occurred at a rate of 33% in older adults with cancer compared to 29% without cancer (p<0.00). Those with a history of cancer were more likely to fall than those without cancer (adjusted odds ratio 1.16; 95% confidence interval [1.02, 1.33]; p = 0.03). No differences in fall rates were determined by cancer type or stage, and the odds of a fall did not increase when adding time since cancer diagnosis.\"\nQuestion:\n\"Do older adults with cancer fall more often?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25280365": {
                "source": [
                    "\"Clinical pathologists (CPs) report RBC morphologic (RBC-M) changes to assist clinicians in prioritizing differential diagnoses. However, reporting is subjective, semiquantitative, and potentially biased. Reporting decisions vary among CPs, and reports may not be interpreted by clinicians as intended.\nThe aims of this study were to survey clinicians and CPs about RBC-M terms and their clinical value, and identify areas of agreement and discordance.\nOnline surveys were distributed to small animal clinicians via the Veterinary Information Network and to CPs via the ASVCP listserv. A quiz assessed understanding of RBC-M terms among respondent groups. Descriptive statistics were used to analyze responses to survey questions, and quiz scores were compared among groups.\nAnalyzable responses were obtained from 1662 clinicians and 82 CPs. Both clinicians and CPs considered some terms, e.g., agglutination, useful, whereas only CPs considered other terms, e.g., ghost cells, useful. All groups interpreted certain terms, e.g., Heinz bodies, correctly, whereas some clinicians misinterpreted others, e.g., eccentrocytes. Responses revealed that CPs often do not report RBC-M they consider insignificant, when present in low numbers. Twenty-eight percent of clinicians think CPs review all blood smears while only 19% of CPs report reviewing all smears.\"\nQuestion:\n\"Reporting and interpreting red blood cell morphology: is there discordance between clinical pathologists and clinicians?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22382608": {
                "source": [
                    "\"The differential diagnosis between essential tremor (ET) and Parkinson's disease (PD) may be, in some cases, very difficult on clinical grounds alone. In addition, it is accepted that a small percentage of ET patients presenting symptoms and signs of possible PD may progress finally to a typical pattern of parkinsonism. Ioflupane, N-u-fluoropropyl-2a-carbomethoxy-3a-(4-iodophenyl) nortropane, also called FP-CIT, labelled with (123)I (commercially known as DaTSCAN) has been proven to be useful in the differential diagnosis between PD and ET and to confirm dopaminergic degeneration in patients with parkinsonism. The aim of this study is to identify dopaminergic degeneration in patients with PD and distinguish them from others with ET using semi-quantitative SPECT (123)I-Ioflupane (DaTSCAN) data in comparison with normal volunteers (NV), in addition with the respective ones of patients referred as suffering from ET, as well as, of patients with a PD diagnosis at an initial stage with a unilateral presentation of motor signs.\nTwenty-eight patients suffering from ET (10 males plus 18 females) and 28 NV (12 males and 16 females) were enroled in this study. In addition, 33 patients (11 males and 22 females) with an established diagnosis of PD with unilateral limb involvement (12 left hemi-body and 21 right hemi-body) were included for comparison with ET. We used DaTSCAN to obtain SPECT images and measure the radiopharmaceutical uptake in the striatum (S), as well as the caudate nucleus (CN) and putamen (P) in all individuals.\nQualitative (Visual) interpretation of the SPECT data did not find any difference in the uptake of the radiopharmaceutical at the level of the S, CN and P between NV and ET patients. Reduced accumulation of the radiopharmaceutical uptake was found in the P of all PD patients. Semiquantitative analysis revealed significant differences between NV and ET patients in the striatum, reduced in the latter. There was also a significant reduction in the tracer accumulation in the left putamen of patients with right hemi-parkinsonism compared to ET and NV. Patients with left hemi-parkinsonism, demonstrated reduced radioligand uptake in the right putamen in comparison with ET and NV. Clinical follow-up of 20 patients with ET at (so many months afterwards) revealed no significant change in clinical presentation, particularly no signs of PD. Follow-up DaTSCAN performed in 10 of them (so many months afterwards) was negative in all but one. This one had an equivocal baseline study which deteriorated 12\u00a0months later.\"\nQuestion:\n\"SPECT study with I-123-Ioflupane (DaTSCAN) in patients with essential tremor. Is there any correlation with Parkinson's disease?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19546588": {
                "source": [
                    "\"Although the mechanism of muscle wasting in end-stage renal disease is not fully understood, there is increasing evidence that acidosis induces muscle protein degradation and could therefore contribute to the loss of muscle protein stores of patients on hemodialysis, a prototypical state of chronic metabolic acidosis (CMA). Because body protein mass is controlled by the balance between synthesis and degradation, protein loss can occur as result of either increased breakdown, impaired synthesis, or both. Correction of acidosis may therefore help to maintain muscle mass and improve the health of patients with CMA. We evaluated whether alkalizing patients on hemodialysis might have a positive effect on protein synthesis and on nutritional parameters.\nEight chronic hemodialysis patients were treated daily with oral sodium bicarbonate (NaHCO(3)) supplementation for 10-14 days, yielding a pre-dialytic plasma bicarbonate concentration of 28.6 +/-1.6 mmol/l. The fractional synthesis rates (FSR) of muscle protein and albumin were obtained by the L-[(2)H(5)ring]phenylalanine flooding technique.\nOral NaHCO(3 )supplementation induced a significant increase in serum bicarbonate (21.5 +/- 3.4 vs. 28.6 +/- 1.6 mmol/l; p = 0.018) and blood pH (7.41 vs. 7.46; p = 0.041). The FSR of muscle protein and the FSR of albumin did not change significantly (muscle protein: 2.1 +/- 0.2 vs. 2.0 +/- 0.5% per day, p = 0.39; albumin: 8.3 +/- 2.2 vs. 8.6 +/- 2.5% per day, p = 0.31). Plasma concentrations of insulin-like growth factor 1 decreased significantly (33.4 +/- 21.3 vs. 25.4 +/- 12.3 nmol/l; p = 0.028), whereas thyroid-stimulating hormone, free thyroxin and free triiodothyronine did not change significantly and nutritional parameters showed no improvement.\"\nQuestion:\n\"Does increasing blood pH stimulate protein synthesis in dialysis patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21951591": {
                "source": [
                    "\"Chronic low back pain (CLBP) is often accompanied by an abnormal motor performance. However, it has not been clarified yet whether these deviations also occur during motor tasks not involving the back and whether the performance is influenced by pain and pain-related cognitions. Therefore, the aim of the present study is to get insight in the contribution of both pain experience and pain-related cognitions to general motor task performance in CLBP.\n13 CLBP patients and 15 healthy subjects performed a hand-function task in three conditions: sitting, lying prone (lying) and lying prone without trunk support (provoking). The last condition was assumed to provoke pain-related cognitions, which was considered successful when a patients' pain expectancy on a numeric rating scale was at least 1 point higher than actual pain experienced. Subjects' performance was expressed in reaction time and movement time. Repeated measures analysis of variance was performed to detect main effect for group and condition. Special interest was given to group*condition interaction, since significant interaction would indicate that patients and healthy subjects performed differently throughout the three conditions.\nPatients were slower throughout all conditions compared to healthy subjects. With respect to the provoking condition, patients showed deteriorated performance compared to lying while healthy subjects' performance remained equal between these two conditions. Further analysis of patients' data showed that provocation was successful in 54% of the patients. Especially this group showed deteriorated performance in the provoking condition.\"\nQuestion:\n\"Motor performance in chronic low back pain: is there an influence of pain-related cognitions?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24958650": {
                "source": [
                    "\"Our aim in this study was to investigate whether mean platelet volume (MPV) value could be used as an early marker to predict pelvic inflammatory disease (PID).\nOverall, 44 patients with PID and 44 healthy women were included in the study. The control group consisted of 44 women who applied to the clinic for a routine gynaecological check-up, without chronic disease or a history of medication use. Owing to the fact that it would affect thrombocyte function, women who have the following conditions were excluded from the study: women who were taking anticoagulant therapy, oral contraceptives, nonsteroid anti-inflammatory medications and who had chronic diseases. The leukocyte count, platelet count, neutrophil ratio and MPV values were collected from PID and the control group. C reactive protein values of patients with PID were also noted.\nMPV values in patients with PID were lower than those in the control group. This reduction in MPV is statistically significant when the PID patient group is compared with the control group (p\u2009<\u20090.001). A negative correlation was discovered between platelet count and MPV values (p\u2009=\u20090.019, r\u2009=\u2009-\u20090.425). Receiver-operating curve analysis pointed out that MPV has greater area under curve value than neutrophil rate, leukocyte and platelet count (0.73, 0.64, 0.72 and 0.49 respectively).\"\nQuestion:\n\"May mean platelet volume levels be a predictor in the diagnosis of pelvic inflammatory disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21592383": {
                "source": [
                    "\"An increasingly significant public health issue in Canada, and elsewhere throughout the developed world, pertains to the provision of adequate palliative/end-of-life (P/EOL) care. Informal caregivers who take on the responsibility of providing P/EOL care often experience negative physical, mental, emotional, social and economic consequences. In this article, we specifically examine how Canada's Compassionate Care Benefit (CCB)--a contributory benefits social program aimed at informal P/EOL caregivers--operates as a public health response in sustaining informal caregivers providing P/EOL care, and whether or not it adequately addresses known aspects of caregiver burden that are addressed within the population health promotion (PHP) model.\nAs part of a national evaluation of Canada's Compassionate Care Benefit, 57 telephone interviews were conducted with Canadian informal P/EOL caregivers in 5 different provinces, pertaining to the strengths and weaknesses of the CCB and the general caregiving experience. Interview data was coded with Nvivo software and emerging themes were identified by the research team, with such findings published elsewhere. The purpose of the present analysis was identified after comparing the findings to the literature specific to caregiver burden and public health, after which data was analyzed using the PHP model as a guiding framework.\nInformal caregivers spoke to several of the determinants of health outlined in the PHP model that are implicated in their burden experience: gender, income and social status, working conditions, health and social services, social support network, and personal health practises and coping strategies. They recognized the need for improving the CCB to better address these determinants.\"\nQuestion:\n\"Canada's Compassionate Care Benefit: is it an adequate public health response to addressing the issue of caregiver burden in end-of-life care?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17179167": {
                "source": [
                    "\"Pterygium is a disease of unknown origin and pathogenesis that might be vision threatening. It is characterised by a wing-like conjunctival overgrowth of the cornea. Several studies have investigated human papillomavirus (HPV) as a risk factor for the development of pterygia, but the results are inconclusive.AIM: To investigate a large sample of pterygia for the presence of HPV in order to clarify the putative association between pterygia and HPV.\n100 specimens of pterygium from Danish patients and 20 normal conjunctival biopsy specimens were investigated for the presence of HPV with PCR technique using beta-globin primers to access the quality of the extracted DNA and the HPV primers MY09/11 and GP5+/6+. HPV-positive specimens underwent subsequent HPV typing with type-specific HPV primers and further investigation with DNA in situ hybridisation (ISH).\n90 of 100 investigated pterygia proved suitable for HPV analysis by PCR. As beta-globin could not be amplified, 10 specimens were excluded from the study. 4 of 90 pterygia harboured HPV. HPV type 6 was identified in all four HPV-positive pterygia. The 20 normal conjunctival biopsy specimens were beta-globin positive and HPV negative. All four pterygia that were HPV type 6 positive were DNA ISH negative.\"\nQuestion:\n\"Human papillomavirus and pterygium. Is the virus a risk factor?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19299238": {
                "source": [
                    "\"Aromatase inhibitors (AIs) are an effective treatment for postmenopausal women with hormone receptor-positive breast cancer. However, patients receiving AIs report a higher incidence of musculoskeletal symptoms and bone fractures; the mechanism and risk factors for this correlation are not well studied. The aim of this study was to correlate these musculoskeletal symptoms and bone fractures in patients receiving AIs with bone mineral density (BMD), previous tamoxifen use, and administration of calcium/bisphosphonate (Ca/Bis).\nWe reviewed charts of 856 patients with hormone receptor-positive nonmetastatic breast cancer seen at our institution between January 1999 and October 2007. A total of 316 patients met the inclusion criteria of treatment with one of the AIs for>or = 3 months and availability of a dualenergy X-ray absorptiometry (DEXA) during this treatment. Arthralgia, generalized bone pain and/or myalgia, bone fracture after beginning AIs, any tamoxifen treatment, and Ca/Bis therapy were recorded.\nOur study demonstrates a significant association between symptoms and DEXA-BMD results (P<.001). Similarly, the group receiving tamoxifen before AIs had fewer patients with arthralgia or generalized bone pain/myalgia or bone fracture (P<.001). Furthermore, the group receiving AIs plus Ca/Bis had more patients without musculoskeletal symptoms and had fewer fractures. Finally, the group receiving steroidal AIs compared with nonsteroidal AIs had more patients with arthralgia or generalized bone pain and/or myalgia, and bone fractures (P<.001).\"\nQuestion:\n\"Aromatase inhibitor-related musculoskeletal symptoms: is preventing osteoporosis the key to eliminating these symptoms?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24153338": {
                "source": [
                    "\"With the advancement of an aging society in the world, an increasing number of elderly patients have been hospitalized due to aneurysmal subarachnoid hemorrhage (aSAH). There is no study that compares the elderly cases of aSAH who receive the definitive treatment with those who treated conservatively. The aim of this study was to investigate the feasibility of the definitive surgery for the acute subarachnoid cases aged 80 or older.\nWe reviewed 500 consecutive cases with acute aSAH with surgical indication for aneurysm repair. Inoperable cases such as dead-on-arrival and the cases with both pupils dilated were excluded. We compared the cases aged 80 or older that received clipping or coil embolization with the controls that the family selected conservative treatment.\n69 cases were included in this study (ranged 80-98, male:female=9:60). 56 cases (81.2%) had an aneurysm in the anterior circulation. 23 cases received clipping, 20 cases coil embolization and 26 cases treated conservatively. The cases with aneurysm repair showed significantly better clinical outcome than the controls, while World Federation of Neurological Surgeons (WFNS) grade on admission and premorbid modified Rankin Scale showed no difference between them.\"\nQuestion:\n\"Is aneurysm repair justified for the patients aged 80 or older after aneurysmal subarachnoid hemorrhage?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20337874": {
                "source": [
                    "\"Infants referred to our institution with a final diagnosis of ARM were retrospectively reviewed between 2001 and 2009. The first cohort consisted of patients that were referred between November 2001 and November 2006 with the diagnosis of an ARM that had been delayed for more than 48 h. The second cohort was those referred between December 2006 and May 2009 with whom the diagnosis of ARM had not been made within 24 h of birth.\nNineteen infants were referred with delayed diagnosis of an ARM over the 7.5 years of the study. Of 44 patients referred to our institution between December 2006 and May 2009, diagnosis of an ARM was delayed more than 24 h in 14 (32%). There was no difference in gender, birth weight, prematurity, type of malformation or presence of associated anomalies between those with timely and delayed diagnosis of their ARM. A significantly greater proportion of those with a delayed diagnosis presented with obstructive symptoms (86% vs. 27%, P<0.001), including abdominal distension (57%) and delayed passage of meconium or stool (29%). Despite undergoing neonatal examination, the diagnosis of ARM was missed in 12 patients overall.\"\nQuestion:\n\"Delayed diagnosis of anorectal malformations: are current guidelines sufficient?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22042121": {
                "source": [
                    "\"Demonstrate that the rabbit may be used in the training of surgery, in addition to present its perioperative care.\nThirty two animals, with age and weight, respectively, from 3 to 5.5 months old and 3000 to 4200 grams, were undergone different periods of pre-operative fasting, exclusive intramuscular anesthesia (ketamine+xylazine), laparotomy with total gastrectomy and total splenectomy. It was dosed the pre-operative (initial) and post-surgical (end) serum blood glucose, in addition to quantify the gastric content after the resection of the part.\nThe anesthetical-surgical procedure presented a mortality rate of 3.125% (1:32) and a morbidity rate of 6.25% (2:32). It was evidenced an initial mean blood glucose = 199.4 mg/dl and the end = 326.1 mg/dl. In spite of extended fasting (minimum of 2 hours for the absolute fasting and maximum of 8.5 hours for liquids, and 20.5 hours for solids) all animals presented at the end of the surgical procedure any gastric content and a blood glucose increase. Those with fasting for liquids and solids when compared to the quantity of solid gastric content, presented a moderate negative degree of correlation.\"\nQuestion:\n\"Perioperative care in an animal model for training in abdominal surgery: is it necessary a preoperative fasting?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21739621": {
                "source": [
                    "\"To examine longitudinal patterns in body mass index (BMI) over 14 years and its association with knee pain in the Chingford Study.\nWe studied a total of 594 women with BMI data from clinic visits at years (Y) 1, 5, 10, and 15. Knee pain at Y15 was assessed by questionnaire. Associations between BMI over 14 years and knee pain at Y15 were examined using logistic regression.\nBMI significantly increased from Y1 to Y15 (P<0.0005) with medians (interquartile ranges) of 24.5 kg/m(2)  (22.5-27.2 kg/m(2) ) and 26.5 kg/m(2)  (23.9-30.1 kg/m(2) ), respectively. At Y15, 45.1% of subjects had knee pain. A greater BMI at Y1 (odds ratio [OR] 1.34, 95% confidence interval [95% CI]1.05-1.69), at Y15 (OR 1.34, 95% CI 1.10-1.61), and change in BMI over 15 years (OR 1.40, 95% CI 1.00-1.93) were significant predictors of knee pain at Y15 (P<0.05). BMI change was associated with bilateral (OR 1.61, 95% CI 1.05-1.76, P = 0.024) but not unilateral knee pain (OR 1.22, 95% CI 0.73-1.76, P = 0.298). The association between BMI change and knee pain was independent of radiographic knee osteoarthritis (OA). The strength of association between BMI and knee pain at Y15 was similar during followup measurements.\"\nQuestion:\n\"Does obesity predict knee pain over fourteen years in women, independently of radiographic changes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9920954": {
                "source": [
                    "\"\"America's Best Hospitals,\" an influential list published annually by U.S. News and World Report, assesses the quality of hospitals. It is not known whether patients admitted to hospitals ranked at the top in cardiology have lower short-term mortality from acute myocardial infarction than those admitted to other hospitals or whether differences in mortality are explained by differential use of recommended therapies.\nUsing data from the Cooperative Cardiovascular Project on 149,177 elderly Medicare beneficiaries with acute myocardial infarction in 1994 or 1995, we examined the care and outcomes of patients admitted to three types of hospitals: those ranked high in cardiology (top-ranked hospitals); hospitals not in the top rank that had on-site facilities for cardiac catheterization, coronary angioplasty, and bypass surgery (similarly equipped hospitals); and the remaining hospitals (non-similarly equipped hospitals). We compared 30-day mortality; the rates of use of aspirin, beta-blockers, and reperfusion; and the relation of differences in rates of therapy to short-term mortality.\nAdmission to a top-ranked hospital was associated with lower adjusted 30-day mortality (odds ratio, 0.87; 95 percent confidence interval, 0.76 to 1.00; P=0.05 for top-ranked hospitals vs. the others). Among patients without contraindications to therapy, top-ranked hospitals had significantly higher rates of use of aspirin (96.2 percent, as compared with 88.6 percent for similarly equipped hospitals and 83.4 percent for non-similarly equipped hospitals; P<0.01) and beta-blockers (75.0 percent vs. 61.8 percent and 58.7 percent, P<0.01), but lower rates of reperfusion therapy (61.0 percent vs. 70.7 percent and 65.6 percent, P=0.03). The survival advantage associated with admission to top-ranked hospitals was less strong after we adjusted for factors including the use of aspirin and beta-blockers (odds ratio, 0.94; 95 percent confidence interval, 0.82 to 1.08; P=0.38).\"\nQuestion:\n\"Do \"America's Best Hospitals\" perform better for acute myocardial infarction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27136599": {
                "source": [
                    "\"Patient data were retrospectively collected from a database for gynaecological cancer procedures carried out between January 2013 and July 2015. All patients who underwent a colorectal resection during cytoreduction were included in the study. The primary outcome was anastomotic leakage in the presence or absence of a diverting stoma. Secondary outcome parameters were complications and reoperations.\nIn the period of study, 43 major colorectal procedures were performed on 37 women. The most common colorectal procedure was low rectal resection (n\u00a0=\u00a022; 59%) followed by anterior rectal resection (n\u00a0=\u00a07; 19%) and sigmoid resection (n\u00a0=\u00a04; 11%). Five (14%) patients underwent Hartmann's procedure. In three (8%) patients, a diverting loop ileostomy was created.\"\nQuestion:\n\"Is it safe to perform rectal anastomosis in gynaecological debulking surgery without a diverting stoma?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22522271": {
                "source": [
                    "\"Forty obese patients with T2DM without clinical features of Cushing's syndrome were recruited. Plasma, urinary and salivary cortisol were measured directly by an enzyme-linked immunosorbent assay using monoclonal antibodies. The specificities of the three tests using various cutoffs were calculated and compared, employing the assumption that none of the patients had hypercortisolism.\nThe patients had a mean age and BMI of 56 years (range 31-75) and 37 kg/m\u00b2 (31-56) respectively. All 40 provided late-night salivary cortisol samples. Thirty-eight patients completed all three tests. Two patients only completed two screening tests. The specificities of late-night salivary cortisol (cutoff 10 nmol/L), 24hr UFC (400 nmol) and 1mg DST (50 nmol/L) were 70% (95% CI 53-83%), 90% (76-97%) and 72% (55-85%) respectively. The specificity of late-night salivary cortisol was significantly less than 24 hr UFC (P=0.039) but not 1mg DST (P>0.99).\"\nQuestion:\n\"Is late-night salivary cortisol a better screening test for possible cortisol excess than standard screening tests in obese patients with Type 2 diabetes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24014276": {
                "source": [
                    "\"Studies examining predictors of survival among the oldest-old have primarily focused on objective measures, such as physical function and health status. Only a few studies have examined the effect of personality traits on survival, such as optimism. The aim of this study was to examine whether an optimistic outlook predicts survival among the oldest-old.\nThe Danish 1905 Cohort Survey is a nationwide, longitudinal survey comprising all individuals born in Denmark in 1905. At baseline in 1998, a total of 2,262 persons aged 92 or 93 agreed to participate in the intake survey. The baseline in-person interview consisted of a comprehensive questionnaire including physical functioning and health, and a question about whether the respondent had an optimistic, neutral or pessimistic outlook on his or her own future.\nDuring the follow-up period of 12 years (1998-2010) there were 2,239 deaths (99 %) in the 1905 Cohort Survey. Univariable analyses revealed that optimistic women and men were at lower risk of death compared to their neutral counterparts [HR 0.82, 95 % CI (0.73-0.93) and 0.81, 95 % CI (0.66-0.99), respectively]. When confounding factors such as baseline physical and cognitive functioning and disease were taken into account the association between optimism and survival weakened in both sexes, but the general pattern persisted. Optimistic women were still at lower risk of death compared to neutral women [HR 0.85, 95 % CI (0.74-0.97)]. The risk of death was also decreased for optimistic men compared to their neutral counterparts, but the effect was non-significant [HR 0.91, 95 % CI (0.73-1.13)].\"\nQuestion:\n\"Optimism and survival: does an optimistic outlook predict better survival at advanced ages?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24019262": {
                "source": [
                    "\"Epidemiological studies have suggested inverse relationships between blood pressure and prevalence of conditions such as migraine and headache. It is not yet clear whether similar relationships can be established for back pain in particular in prospective studies.\nAssociations between blood pressure and chronic low back pain were explored in the cross-sectional HUNT 2 survey of a Norwegian county in 1995-1997, including 39,872 individuals who never used antihypertensive medication. A prospective study, comprising 17,209 initially back pain-free individuals and 5740 individuals reporting low back pain, was established by re-examinations in the HUNT 3 survey in 2006-2008. Associations were assessed by logistic regression with respect to systolic, diastolic and pulse pressure, with adjustment for education, work status, physical activity, smoking, body mass and lipid levels.\nIn the cross-sectional study, all three blood pressure measures showed inverse relationships with prevalence of low back pain in both sexes. In the prospective study of disease-free women, baseline pulse pressure and systolic pressure were inversely associated with risk of low back pain [odds ratio (OR) 0.93 per 10\u2009mm\u2009Hg increase in pulse pressure, 95% confidence interval (CI) 0.89-0.98, p\u2009=\u20090.007; OR 0.95 per 10\u2009mm Hg increase in systolic pressure, 95% CI 0.92-0.99, p\u2009=\u20090.005]. Results among men were equivocal. No associations were indicated with the occurrence of pain in individuals with low back pain at baseline.\"\nQuestion:\n\"Does high blood pressure reduce the risk of chronic low back pain?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26556589": {
                "source": [
                    "\"Achilles tendon structure deteriorates 2-days after maximal loading in elite athletes. The load-response behaviour of tendons may be altered in type 1 diabetes mellitus (T1DM) as hyperglycaemia accelerates collagen cross-linking. This study compared Achilles tendon load-response in participants with T1DM and controls.\nAchilles tendon structure was quantified at day-0, day-2 and day-4 after a 10\u00a0km run. Ultrasound tissue characterisation (UTC) measures tendon structural integrity by classifying pixels as echo-type I, II, III or IV. Echo-type I has the most aligned collagen fibrils and IV has the least.\nParticipants were 7 individuals with T1DM and 10 controls. All regularly ran distances greater than 5\u00a0km and VISA-A scores indicated good tendon function (T1DM\u2009=\u200994\u2009\u00b1\u200911, control\u2009=\u200994\u2009\u00b1\u200910). There were no diabetic complications and HbA1c was 8.7\u2009\u00b1\u20092.6\u00a0mmol/mol for T1DM and 5.3\u2009\u00b1\u20090.4\u00a0mmol/mol for control groups. Baseline tendon structure was similar in T1DM and control groups - UTC echo-types (I-IV) and anterior-posterior thickness were all p\u2009>\u20090.05. No response to load was seen in either T1DM or control group over the 4-days post exercise.\"\nQuestion:\n\"Does type 1 diabetes mellitus affect Achilles tendon response to a 10\u00a0km run?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27896825": {
                "source": [
                    "\"Current guidelines recommend total thyroidectomy for nearly all children with well-differentiated thyroid cancer (WDTC). These guidelines, however, derive from older data accrued prior to current high-resolution imaging. We speculate that there is a subpopulation of children who may be adequately treated with lobectomy.\nRetrospective analysis of prospectively maintained database.\nSeventy-three children with WDTC treated between 2004 and 2015.\nWe applied two different risk-stratification criteria to this population. First, we determined the number of patients meeting American Thyroid Association (ATA) 'low-risk' criteria, defined as disease grossly confined to the thyroid with either N0/Nx or incidental microscopic N1a disease. Second, we defined a set of 'very-low-risk' histopathological criteria, comprising unifocal tumours \u22644 cm without predefined high-risk factors, and determined the proportion of patients that met these criteria.\nTwenty-seven (37%) males and 46 (63%) females were included in this study, with a mean age of 13\u00b74 years. Ipsilateral- and contralateral multifocality were identified in 27 (37\u00b70%) and 19 (26\u00b70%) of specimens. Thirty-seven (51%) patients had lymph node metastasis (N1a = 18/N1b = 19). Pre-operative ultrasound identified all cases with clinically significant nodal disease. Of the 73 patients, 39 (53\u00b74%) met ATA low-risk criteria and 16 (21\u00b79%) met 'very-low-risk' criteria. All 'very-low-risk' patients demonstrated excellent response to initial therapy without persistence/recurrence after a mean follow-up of 36\u00b74 months.\"\nQuestion:\n\"Is it time to reconsider lobectomy in low-risk paediatric thyroid cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18693227": {
                "source": [
                    "\"This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center.\nBetween June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan.\nPatient characteristics included a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.\"\nQuestion:\n\"Does a geriatric oncology consultation modify the cancer treatment plan for elderly patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26044262": {
                "source": [
                    "\"Rates of active travel vary by socio-economic position, with higher rates generally observed among less affluent populations. Aspects of both social and built environments have been shown to affect active travel, but little research has explored the influence of physical environmental characteristics, and less has examined whether physical environment affects socio-economic inequality in active travel. This study explored income-related differences in active travel in relation to multiple physical environmental characteristics including air pollution, climate and levels of green space, in urban areas across England. We hypothesised that any gradient in the relationship between income and active travel would be least pronounced in the least physically environmentally-deprived areas where higher income populations may be more likely to choose active transport as a means of travel.\nAdults aged 16+ living in urban areas (n\u2009=\u200920,146) were selected from the 2002 and 2003 waves of the UK National Travel Survey. The mode of all short non-recreational trips undertaken by the sample was identified (n\u2009=\u2009205,673). Three-level binary logistic regression models were used to explore how associations between the trip being active (by bike/walking) and three income groups, varied by level of multiple physical environmental deprivation.\nLikelihood of making an active trip among the lowest income group appeared unaffected by physical environmental deprivation; 15.4% of their non-recreational trips were active in both the least and most environmentally-deprived areas. The income-related gradient in making active trips remained steep in the least environmentally-deprived areas because those in the highest income groups were markedly less likely to choose active travel when physical environment was 'good', compared to those on the lowest incomes (OR\u2009=\u20090.44, 95% CI\u2009=\u20090.22 to 0.89).\"\nQuestion:\n\"Are income-related differences in active travel associated with physical environmental characteristics?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12913878": {
                "source": [
                    "\"Nd:YAG laser-induced thermo therapy (LITT) of rat brains is associated with blood-brain barrier (BBB) permeability changes. We address the question of whether LITT-induced locoregional disruption of the BBB could possibly allow a locoregional passage of chemotherapeutic agents into brain tissue to treat malignant glioma.STUDY DESIGN/\nCD Fischer rats were subject to LITT of the left forebrain. Disruption of the BBB was analyzed using Evans blue and immunohistochemistry (IH). Animals were perfused with paclitaxel, and high-pressure liquid chromatography (HPLC) was employed to analyze the content of paclitaxel in brain and plasma samples.\nLITT induces an opening of the BBB as demonstrated by locoregional extravasation of Evans blue, C3C, fibrinogen, and IgM. HPLC proved the passage of paclitaxel across the disrupted BBB.\"\nQuestion:\n\"Locoregional opening of the rodent blood-brain barrier for paclitaxel using Nd:YAG laser-induced thermo therapy: a new concept of adjuvant glioma therapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25070942": {
                "source": [
                    "\"In the Philippines, the current national control strategy for schistosomiasis is annual mass drug administration (MDA) with 40 mg/kg of praziquantel in all schistosomiasis-endemic villages with a prevalence \u226510%.\nA cross-sectional survey of schistosomiasis was conducted in 2012 on 18 221 individuals residing in 22 schistosomiasis-endemic villages in the province of Northern Samar. The prevalence of schistosomiasis, intensity of Schistosoma infection, and morbidity of disease were assessed.\nDespite an active schistosomiasis-control program in Northern Samar for>30 years, which included a MDA campaign in the last 5 years, the mean prevalence of schistosomiasis among 10 435 evaluated subjects was 27.1% (95% confidence interval [CI], 26.3%-28.0%), and the geometric mean intensity of infection among 2832 evaluated subjects was 17.2 eggs per gram of feces (95% CI, 16.4-18.1). Ultrasonography revealed high levels of schistosomiasis-induced morbidity in the schistosomiasis-endemic communities. Left lobe liver enlargement (\u226570 mm) was evident in 89.3% of subjects. Twenty-five percent of the study population had grade II/III liver parenchyma fibrosis, and 13.3% had splenomegaly (\u2265100 mm).\"\nQuestion:\n\"Can mass drug administration lead to the sustainable control of schistosomiasis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18251357": {
                "source": [
                    "\"To evaluate the degree to which histologic chorioamnionitis, a frequent finding in placentas submitted for histopathologic evaluation, correlates with clinical indicators of infection in the mother.\nA retrospective review was performed on 52 cases with a histologic diagnosis of acute chorioamnionitis from 2,051 deliveries at University Hospital, Newark, from January 2003 to July 2003. Third-trimester placentas without histologic chorioamnionitis (n = 52) served as controls. Cases and controls were selected sequentially. Maternal medical records were reviewed for indicators of maternal infection.\nHistologic chorioamnionitis was significantly associated with the usage of antibiotics (p = 0.0095) and a higher mean white blood cell count (p = 0.018). The presence of 1 or more clinical indicators was significantly associated with the presence of histologic chorioamnionitis (p = 0.019).\"\nQuestion:\n\"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23422012": {
                "source": [
                    "\"Vancomycin is the primary treatment for infections caused by methicilin-resistant Staphylococcus aureus (MRSA). The association of vancomycin treatment failures with increased vancomycin minimum inhibitory concentration (MIC) is a well-recognized problem. A number of single-centre studies have identified progressive increases in glycopeptide MICs for S. aureus strains over recent years - a phenomenon known as vancomycin MIC creep. It is unknown if this is a worldwide phenomenon or if it is localized to specific centers.\nThe aim of this study was to evaluate the trend of vancomycin MIC for isolates of MRSA over a 3-year period in a tertiary university hospital in Portugal. MRSA isolates from samples of patients admitted from January 2007 to December 2009 were assessed. Etest method was used to determine the respective vancomycin MIC. Only one isolate per patient was included in the final analysis.\nA total of 93 MRSA isolates were studied. The vancomycin MICs were 0.75, 1, 1.5 and 2 mg/L for 1 (1.1%), 19 (20.4%), 38 (40.9%), 35 (37.6%) isolates, respectively. During the 3 year period, we observed a significant fluctuation in the rate of MRSA with a vancomycin MIC\u2009>\u20091 mg/L (2007: 86.2%; 2008: 93.3%; 2009: 58.8%, p\u2009=\u20090.002). No MRSA isolate presented a MIC\u2009>\u20092 mg/L.\"\nQuestion:\n\"Is vancomycin MIC creep a worldwide phenomenon?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17919952": {
                "source": [
                    "\"(i) To examine the association between self-reported mechanical factors and chronic oro-facial pain. (ii) To test the hypothesis that this relationship could be explained by: (a) reporting of psychological factors, (b) common association of self-reported mechanical factors with other unexplained syndromes.\nA population based cross-sectional study of 4200 randomly selected adults registered with a General Medical Practice in North West, England. The study examined the association of chronic oro-facial pain with a variety of self-reported mechanical factors: teeth grinding, facial trauma, missing teeth and the feeling that the teeth did not fit together properly. Information was also collected on demographic factors, psychological factors and the reporting of other frequently unexplained syndromes.\nAn adjusted response rate of 72% was achieved. Only two mechanical factors: teeth grinding (odds ratio (OR) 2.0, 95% CI 1.3-3.0) and facial trauma (OR 2.0; 95% CI 1.3-2.9) were independently associated with chronic oro-facial pain after adjusting for psychological factors. However, these factors were also commonly associated with the reporting of other frequently unexplained syndromes: teeth grinding (odds ratio (OR) 1.8, 95% CI 1.5-2.2), facial trauma (OR 2.1; 95% CI 1.7-2.6).\"\nQuestion:\n\"Are reports of mechanical dysfunction in chronic oro-facial pain related to somatisation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11729377": {
                "source": [
                    "\"To assess and compare the value of split-liver transplantation (SLT) and living-related liver transplantation (LRT).\nThe concept of SLT results from the development of reduced-size transplantation. A further development of SLT, the in situ split technique, is derived from LRT, which itself marks the optimized outcome in terms of postoperative graft function and survival. The combination of SLT and LRT has abolished deaths on the waiting list, thus raising the question whether living donor liver transplantation is still necessary.\nOutcomes and postoperative liver function of 43 primary LRT patients were compared with those of 49 primary SLT patients (14 ex situ, 35 in situ) with known graft weight performed between April 1996 and December 2000. Survival rates were analyzed using the Kaplan-Meier method.\nAfter a median follow-up of 35 months, actual patient survival rates were 82% in the SLT group and 88% in the LRT group. Actual graft survival rates were 76% and 81%, respectively. The incidence of primary nonfunction was 12% in the SLT group and 2.3% in the LRT group. Liver function parameters (prothrombin time, factor V, bilirubin clearance) and surgical complication rates did not differ significantly. In the SLT group, mean cold ischemic time was longer than in the LRT group. Serum values of alanine aminotransferase during the first postoperative week were significantly higher in the SLT group. In the LRT group, there were more grafts with signs of fatty degeneration than in the SLT group.\"\nQuestion:\n\"Is there still a need for living-related liver transplantation in children?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23147106": {
                "source": [
                    "\"We analyzed the pharmacokinetic-pharmacodynamic relationship of vancomycin to determine the drug exposure parameters that correlate with the efficacy and nephrotoxicity of vancomycin in patients with methicillin-resistant Staphylococcus aureus pneumonia and evaluated the need to use peak concentration in therapeutic drug monitoring (TDM).\nSerum drug concentrations of 31 hospitalized patients treated with vancomycin for methicillin-resistant S. aureus pneumonia were collected.\nSignificant differences in trough concentration (Cmin)/minimum inhibitory concentration (MIC) and area under the serum concentration-time curve (AUC0-24)/MIC were observed between the response and non-response groups. Significant differences in Cmin and AUC0-24 were observed between the nephrotoxicity and non-nephrotoxicity groups. Receiver operating characteristic curves revealed high predictive values of Cmin/MIC and AUC0-24/MIC for efficacy and of Cmin and AUC0-24 for safety of vancomycin.\"\nQuestion:\n\"Is peak concentration needed in therapeutic drug monitoring of vancomycin?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17062225": {
                "source": [
                    "\"The impact of different strategies for management of intercostal and lumbar arteries during repair of thoracic and thoracoabdominal aortic aneurysms (TAA/A) on the prevention of paraplegia remains poorly understood.\nOne hundred consecutive patients with intraoperative monitoring of motor evoked potentials (MEP) and somatosensory evoked potentials (SSEP) during TAA/A repair involving serial segmental artery sacrifice (October 2002 to December 2004) were reviewed.\nOperative mortality was 6%. The median intensive care unit stay was 2.5 days (IQ range: 1-4 days), and the median hospital stay 10.0 days (IQ range: 8-17 days). Potentials remained unchanged during the course of serial segmental artery sacrifice, or could be returned to baseline levels by anesthetic and blood pressure manipulation, in 99 of 100 cases. An average of 8.0 +/- 2.6 segmental artery pairs were sacrificed overall, with an average of 4.5 +/- 2.1 segmental pairs sacrificed between T7 and L1, where the artery of Adamkiewicz is presumed to arise. Postoperative paraplegia occurred in 2 patients. In 1, immediate paraplegia was precipitated by an intraoperative dissection, resulting in 6 hours of lower body ischemia. A second ambulatory patient had severe paraparesis albeit normal cerebral function after resuscitation from a respiratory arrest.\"\nQuestion:\n\"Thoracic and thoracoabdominal aneurysm repair: is reimplantation of spinal cord arteries a waste of time?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24577079": {
                "source": [
                    "\"Older adults typically perform worse on measures of working memory (WM) than do young adults; however, age-related differences in WM performance might be reduced if older adults use effective encoding strategies.\nThe purpose of the current experiment was to evaluate WM performance after training individuals to use effective encoding strategies.\nParticipants in the training group (older adults: n = 39; young adults: n = 41) were taught about various verbal encoding strategies and their differential effectiveness and were trained to use interactive imagery and sentence generation on a list-learning task. Participants in the control group (older: n = 37; young: n = 38) completed an equally engaging filler task. All participants completed a pre- and post-training reading span task, which included self-reported strategy use, as well as two transfer tasks that differed in the affordance to use the trained strategies - a paired-associate recall task and the self-ordered pointing task.\nBoth young and older adults were able to use the target strategies on the WM task and showed gains in WM performance after training. The age-related WM deficit was not greatly affected, however, and the training gains did not transfer to the other cognitive tasks. In fact, participants attempted to adapt the trained strategies for a paired-associate recall task, but the increased strategy use did not benefit their performance.\"\nQuestion:\n\"Does strategy training reduce age-related deficits in working memory?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16956164": {
                "source": [
                    "\"First, to establish whether a deprivation gradient in all-cause mortality exists for all ethnic groups within New Zealand; second, if such gradients do exist, whether their absolute slopes are the same; and third, if such gradients exist, what impact the unequal deprivation distributions of the different ethnic groups have on the observed ethnic inequalities in life expectancy at birth.\nAbridged lifetables for the period 1999-2003 were constructed using standard demographic methods for each of four ethnic groups (Asian, Pacific, Maori and European) by NZDep2001 quintile and sex. Gradients were estimated by fitting generalised linear models to the quintile-specific life expectancy estimates for each ethnic group (by sex). The contribution of variation in deprivation distributions to inter-ethnic inequalities in life expectancy was estimated by re-weighting the quintile-specific mortality rates for each ethnic group using weights derived from the European deprivation distribution and recalculating the lifetable.\nAll four ethnic groups exhibit deprivation gradients in all-cause mortality (life expectancy). Maori show the steepest gradients, with slopes approximately 25% steeper than those of Europeans for both males and females. By contrast, gradients among Asian and Pacific peoples are shallower than those of their European counterparts.\"\nQuestion:\n\"Do all ethnic groups in New Zealand exhibit socio-economic mortality gradients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25675614": {
                "source": [
                    "\"Diabetes mellitus (DM) is undiagnosed in approximately half of the patients actually suffering from the disease. In addition, the prevalence of DM is more than twice as high as in patients with periodontitis when compared to periodontally healthy subjects. Thus, a high number of patients with periodontitis may have undiagnosed DM. The purpose of the present study was to evaluate whether blood oozing from a gingival crevice during routine periodontal examination can be used for determining glucose levels.\nObservational cross-sectional studies were carried out in 75 patients (43 males and 32 females) with chronic periodontitis who were divided into two groups: Group I and Group II, respectively. Blood oozing from the gingival crevices of anterior teeth following periodontal probing was collected with the stick of glucose self-monitoring device, and the blood glucose levels were measured. At the same time, finger-prick blood was taken for glucometric analysis and subsequent readings were recorded.\nThe patient's blood glucose values ranged from 74 to 256 mg/dl. The comparison between gingival crevicular blood and finger-prick blood showed a very strong correlation, with a t value of 3.97 (at P value = 0.001).\"\nQuestion:\n\"Can gingival crevicular blood be relied upon for assessment of blood glucose level?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20354380": {
                "source": [
                    "\"To assess gender differences among residents regarding their plans to have children during residency and determine the most influential reasons for these differences.\nUsing the Health Belief Model as a framework, the authors created an instrument to survey 424 residents from 11 residency programs at three academic medical institutions about their intentions to have children during residency. The authors developed a scale to assess the perceived career threats of having children during residency, evaluated its psychometric properties, and calculated the effect of the mediators.\nThe response rate was 77% (328/424). Forty-one percent of men versus 27% of women planned to have children during residency (P = .01). The instrument measured four career threats-extended training, loss of fellowship positions, pregnancy complications, and interference with career plans-on a five-point Likert scale. The scale had a Cronbach alpha of 0.84 and an eigenvalue of 2.2. Compared with men, women had higher scores for each item and a higher mean score (2.9 versus 2.1, P = .001), signifying greater belief in the potential of pregnancy to threaten careers. After adjusting for age, institution, postgraduate year, and knowledge of parental leave policies, women were less likely to plan to have children during residency (odds ratio 0.46 [95% confidence interval 0.25-0.84]). In mediation analysis, threats to career explained 67% of the gender variance.\"\nQuestion:\n\"Do women residents delay childbearing due to perceived career threats?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11882828": {
                "source": [
                    "\"Desmopressin releases tissue-type plasminogen activator, which augments cardiopulmonary bypass--associated hyperfibrinolysis, causing excessive bleeding. Combined use of desmopressin with prior administration of the antifibrinolytic drug tranexamic acid may decrease fibrinolytic activity and might improve postoperative hemostasis.\nThis prospective randomized study was carried out with 100 patients undergoing coronary artery bypass operations between April 1999 and November 2000 in G\u00fclhane Military Medical Academy. Patients were divided into 2 groups. Desmopressin (0.3 microg/kg) was administrated just after cardiopulmonary bypass and after protamine infusion in group 1 (n = 50). Both desmopressin and tranexamic acid (before the skin incision at a loading dose of 10 mg/kg over 30 minutes and followed by 12 hours of 1 mg.kg(-1).h(-1)) were administrated in group 2 (n = 50).\nSignificantly less drainage was noted in group 2 (1010 +/- 49.9 mL vs 623 +/- 41.3 mL, P =.0001). Packed red blood cells were transfused at 2.1 +/- 0.5 units per patient in group 1 versus 0.9 +/- 0.3 units in group 2 (P =.0001). Fresh frozen plasma was transfused at 1.84 +/- 0.17 units per patient in group 1 versus 0.76 +/- 0.14 units in group 2 (P =.0001). Only 24% of patients in group 2 required donor blood or blood products compared with 74% of those in the isolated desmopressin group (group 1, P =.00001). Group 1 and group 2 findings were as follows: postoperative fibrinogen, 113 +/- 56.3 mg/dL versus 167 +/- 45.8 mg/dL (P =.0001); fibrin split product, 21.2 +/- 2.3 ng/mL versus 13.5 +/- 3.4 ng/mL (P =.0001); and postoperative hemoglobin level, 7.6 plus minus 1.2 g/dL versus 9.1 plus minus 1.2 g/dL (P =.0001).\"\nQuestion:\n\"Does tranexamic acid reduce desmopressin-induced hyperfibrinolysis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21873082": {
                "source": [
                    "\"Despite rapid adoption of the Hirsch index (h-index) as a measure of academic success, the correlations between the h-index and other metrics of productivity remain poorly understood. The aims of this study were to determine whether h-indices were associated with greater National Institutes of Health (NIH) funding success among academic radiologists.\nUsing the Scopus database, h-indices were calculated for a random sample of academic radiologists with the rank of professor. Using the NIH tool Research Portfolio Online Reporting Tools Expenditures and Reports, we determined the number, classification, and total years of NIH grant funding as principal investigator for each radiologist. Differences in h-index, sorted by funding status, were determined using Wilcoxon's tests. Associations between h-index and funding status were determined using logistic regression. Significant correlations between h-index and grant metrics were determined using Spearman's \u03c1.\nAmong 210 professors of radiology, 48 (23%) secured at least one NIH grant. The mean h-index was significantly higher among individuals who secured at least one NIH grant (19.1) compared to those who did not (10.4) (P<.0001). Professors with h-indices<10 compared to those with h-indices>10 were significantly less likely to receive NIH funding (odds ratio, 0.07; P = .0321). However, h-indices>10 were not significantly predictive of greater funding. No significant relationships were observed between h-index and the number of grant awards, years of prior funding, the amounts of grant awards, or grant classification.\"\nQuestion:\n\"Is the h-index predictive of greater NIH funding success among academic radiologists?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14992556": {
                "source": [
                    "\"Ambulatory 24-h dual-channel pharyngeal and oesophageal pH monitoring is the standard test for measuring gastro-oesophageal and gastropharyngeal reflux. Artefacts caused by the intake of food may result in falsely positive gastropharyngeal reflux, which necessitates a manual review of 24-h pH data. The purpose of the study was to investigate the influence of meals and whether leaving out meals affected the reliability of the test.\nPatients referred for otolaryngological complaints, suspected to have been caused by gastro-oesophageal reflux, underwent 24-h dual-channel pH monitoring. The raw unprocessed pH data were corrected by visual inspection of the 24-h tracings (corrected data), by leaving out meals or meals plus a 2-h postprandrial period.\nThe raw pH data were substantially influenced by artefacts of food intake and pseudoreflux. Data obtained by leaving out meals agreed best with manually corrected data. Many of the falsely positive reflux episodes could be removed, thereby inducing a 9%-18% chance of undetected reflux. When examining the fraction of time supine, manually corrected data and data leaving out meals were fully concordant and detected 79% of patients with gastropharyngeal reflux. However, leaving out meals plus a 2-h postprandrial period resulted in 21%-50% falsely negative tests.\"\nQuestion:\n\"Artefacts in 24-h pharyngeal and oesophageal pH monitoring: is simplification of pH data analysis feasible?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16872243": {
                "source": [
                    "\"In this study, an expert panel assessed causality of adverse reports by using the WHO global introspection (GI) method. The same reports were independently assessed using 15 published algorithms. The causality assessment level 'possible' was considered the lower limit for a report to be considered to be drug related. For a given algorithm, sensitivity was determined by the proportion of reports simultaneously classified as drug related by the algorithm and the GI method. Specificity was measured as the proportion of reports simultaneously considered non-drug related. The analysis was performed for the total sample and within serious or unexpected events.\nFive hundred adverse reports were studied. Algorithms presented high rates of sensitivity (average of 93%, positive predictive value of 89%) and low rates of specificity (average of 7%, negative predictive value of 31%).\"\nQuestion:\n\"Can decisional algorithms replace global introspection in the individual causality assessment of spontaneously reported ADRs?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11138995": {
                "source": [
                    "\"Alexithymia is presumed to play an important predisposing role in the pathogenesis of medically unexplained physical symptoms. However, no research on alexithymia has been done among general medical outpatients who present with medically unexplained physical symptoms as their main problem and in which anxiety and depression have been considered as possible confounding factors. This study investigated whether patients with medically unexplained physical symptoms are more alexithymic than those with explained symptoms and whether, in patients with unexplained symptoms, alexithymia is associated with subjective health experience and use of medical services.\nWe conducted a cross-sectional study among patients attending an internal medicine outpatient clinic. All patients were given a standardized interview and completed a number of questionnaires.\nAfter complete physical examinations, 169 of 321 patients had unexplained physical symptoms according to two independent raters. Patients with medically unexplained symptoms more often had a mental disorder, but overall they were not more alexithymic. In patients with unexplained physical symptoms, alexithymia was not associated with subjective health experience or use of medical services. However, patients with both unexplained symptoms and a mental disorder who also denied any possible connection between emotional problems and their physical symptoms did have more alexithymic traits.\"\nQuestion:\n\"Is alexithymia a risk factor for unexplained physical symptoms in general medical outpatients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11034241": {
                "source": [
                    "\"Routine intraoperative frozen section (FS) of sentinel lymph nodes (SLN) can detect metastatic disease, allowing immediate axillary dissection and avoiding the need for reoperation. Routine FS is also costly, increases operative time, and is subject to false-negative results. We examined the benefit of routine intraoperative FS among the first 1000 patients at Memorial Sloan Kettering Cancer Center who had SLN biopsy for breast cancer.\nWe performed SLN biopsy with intraoperative FS in 890 consecutive breast cancer patients, none of whom had a back-up axillary dissection planned in advance. Serial sections and immunohistochemical staining for cytokeratins were performed on all SLN that proved negative on FS. The sensitivity of FS was determined as a function of (1) tumor size and (2) volume of metastatic disease in the SLN, and the benefit of FS was defined as the avoidance of a reoperative axillary dissection.\nThe sensitivity of FS ranged from 40% for patients with Tla to 76% for patients with T2 cancers. The volume of SLN metastasis was highly correlated with tumor size, and FS was far more effective in detecting macrometastatic disease (sensitivity 92%) than micrometastases (sensitivity 17%). The benefit of FS in avoiding reoperative axillary dissection ranged from 4% for Tla (6 of 143) to 38% for T2 (45 of 119) cancers.\"\nQuestion:\n\"Is routine intraoperative frozen-section examination of sentinel lymph nodes in breast cancer worthwhile?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19459018": {
                "source": [
                    "\"From 2000 to 2007, 344 patients underwent pancreatoduodenectomy for PA. Fifty-three patients (elevated group) had preoperatively elevated serum CA 19-9 levels (>400 IU/ml) after resolution of obstructive jaundice. Of these, 27 patients had high levels (400-899 IU/ml (HL)) and 26 patients had very high levels>or=900 IU/ml (VHL). Fifty patients with normal preoperative serum CA 19-9 levels (<37 IU/ml) comprised the control group.\nMedian survival of the control group (n = 50) versus elevated group (n = 53) was 22 versus 15 months (p = 0.02) and overall 3-year survival was 32% versus 14% (p = 0.03). There was no statistical difference in the median and 3-year overall survival between patients with HL and VHL. Patients in the elevated group who normalized their CA 19-9 levels after surgery (n = 11) had a survival equivalent to patients in the control group.\"\nQuestion:\n\"Very high serum CA 19-9 levels: a contraindication to pancreaticoduodenectomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23848044": {
                "source": [
                    "\"This study represents a subset of a complete data set, considering only those children aged admitted to the Pediatric Surgery and Pediatric Nephrology Clinics during the period January 2011 to July 2012.\nIn this study, we have determined that the QT interval changes significantly depending on the use of oxybutynin. The QT changes increased cardiac arrhythmia in children.\"\nQuestion:\n\"Does oxybutynin hydrochloride cause arrhythmia in children with bladder dysfunction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10808977": {
                "source": [
                    "\"Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening?\nThree-year randomized field trial.\nOne thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina.\nWomen were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling.\nAdherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997.\nCompared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year.\"\nQuestion:\n\"Can tailored interventions increase mammography use among HMO women?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24964725": {
                "source": [
                    "\"Utilization of the Recovery Knowledge Inventory (RKI) and Recovery Attitudes Questionnaire (RAQ) in southeastern Australia raised questions about the RAQ, including links between attitudes, faith, and culture in supporting the recovery journey. These questions are particularly important when considered in the context of people with mental illness who live in secular multicultural societies.\"\nQuestion:\n\"Recovery Outcome Measures: Is There a Place for Culture, Attitudes, and Faith?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9044116": {
                "source": [
                    "\"The management of noncorrectable extra hepatic biliary atresia includes portoenterostomy, although the results of the surgery are variable. This study was done to develop criteria that could successfully predict the outcome of surgery based on preoperative data, including percutaneous liver biopsy, allowing a more selective approach to the care of these babies.\nThe charts and biopsy results of 31 patients who underwent a Kasai procedure for biliary atresia between 1984 and 1994 were reviewed. Values for preoperative albumin, bilirubin, age of patient at Kasai, and lowest postoperative bilirubin were recorded. Surgical success was defined as postoperative bilirubin that returned to normal. A pathologist blinded to the child's eventual outcome graded the pre-Kasai needle liver biopsy results according to duct proliferation, ductal plate lesion, bile in ducts, lobular inflammation, giant cells, syncitial giant cells, focal necrosis, bridging necrosis, hepatocyte ballooning, bile in zone 1, 2, and 3, cholangitis, and end-stage cirrhosis. Clinical outcome was then predicted.\nSuccess after portoenterostomy could not reliably be predicted based on gender, age at Kasai, preoperative bilirubin or albumin levels. Histological criteria, however, predicted outcome in 27 of 31 patients (P<.01). Fifteen of 17 clinical successes were correctly predicted; as were 12 of 14 clinical failures (sensitivity, 86%; specificity, 88%). Individually, the presence of syncitial giant cells, lobular inflammation, focal necrosis, bridging necrosis, and cholangitis, were each associated with failure of the portoenterostomy (P<.05). Bile in zone 1 was associated with clinical success of the procedure (P<.05).\"\nQuestion:\n\"Biliary atresia: should all patients undergo a portoenterostomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22012962": {
                "source": [
                    "\"Men appear to benefit more from being married than women with respect to mortality in middle age. However, there is some uncertainty about gender differences in mortality risks in older individuals, widowed, divorced and single individuals and about the impact of living arrangements.\nLongitudinal data with 1990 census records being linked to mortality data up to 2005 were used (Swiss National Cohort). The sample comprised all residents over age 44 years in Switzerland (n=2,440,242). All-cause mortality HRs for marital status and living arrangements were estimated by Cox regression for men and women and different age groups with adjustment for education and socio-professional category.\nThe benefit of being married was stronger for men than for women; however, mortality patterns were similar, with higher mortality in divorced and single individuals compared with widowed individuals (<80 years). After adjustment for living arrangements, the gender difference by marital status disappeared. Stratification by living arrangement revealed that mortality risks were highest for 45-64-year-old divorced (HR 1.72 (95% CI 1.67 to 1.76)) and single men (HR 1.67 (95% CI 1.63 to 1.71)) who lived alone. In women of the same age, the highest mortality risk was observed for those who were single and living with a partner (HR 1.70 (95% CI 1.58 to 1.82)). In older age groups, the impact of marital status decreased.\"\nQuestion:\n\"Marital status, living arrangement and mortality: does the association vary by gender?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21900017": {
                "source": [
                    "\"Ischemia-reperfusion (IR) injury remains a major cause of early morbidity and mortality after lung transplantation with poorly documented extrapulmonary repercussions. To determine the hemodynamic effect due to lung IR injury, we performed a quantitative coronary blood-flow analysis in a swine model of in situ lung ischemia and reperfusion.\nIn 14 healthy pigs, blood flow was measured in the ascending aorta, left anterior descending (LAD), circumflex (Cx), right coronary artery (RCA), right common carotid artery (RCCA), and left internal mammary artery (LIMA), along with left-and right-ventricular pressures (LVP and RVP), aortic pressure (AoP), and pulmonary artery pressure (PAP). Cardiac Troponin (cTn), interleukin 6 and 10 (IL-6 and IL-10), and tumor necrosis factor A (TNF-A) were measured in coronary sinus blood samples. The experimental (IR) group (n=10) underwent 60 min of lung ischemia followed by 60 min of reperfusion by clamping and releasing the left pulmonary hilum. Simultaneous measurements of all parameters were made at baseline and during IR. The control group (n=4) had similar measurements without lung IR.\nIn the IR group, total coronary flow (TCF=LAD+Cx+RCA blood-flow) decreased precipitously and significantly from baseline (113\u00b141 ml min\"1) during IR (p<0.05), with the lowest value observed at 60 min of reperfusion (-37.1%, p<0.003). Baseline cTn (0.08\u00b10.02 ng ml(-1)) increased during IR and peaked at 45 min of reperfusion (+138%, p<0.001). Baseline IL-6 (9.2\u00b12.17 pg ml(-1)) increased during IR and peaked at 60 min of reperfusion (+228%, p<0.0001). Significant LVP drop at 5 min of ischemia (p<0.05) was followed by a slow return to baseline at 45 min of ischemia. A second LVP drop occurred at reperfusion (p<0.05) and persisted. Conversely, RVP increased throughout ischemia (p<0.05) and returned toward baseline during reperfusion. Coronary blood flow and hemodynamic profile remained unchanged in the control group. IL-10 and TNF-A remained below the measurable range for both the groups.\"\nQuestion:\n\"Does lung ischemia and reperfusion have an impact on coronary flow?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10732884": {
                "source": [
                    "\"To study the relationship between coronary angiography and in-hospital mortality in patients undergoing emergency surgery of the aorta without a history of coronary revascularization or coronary angiography before the onset of symptoms.\nIn the setting of acute ascending aortic dissection warranting emergency aortic repair, coronary angiography has been considered to be desirable, if not essential. The benefits of defining coronary anatomy have to be weighed against the risks of additional delay before surgical intervention.\nRetrospective analysis of patient charts and the Cardiovascular Information Registry (CVIR) at the Cleveland Clinic Foundation.\nWe studied 122 patients who underwent emergency surgery of the aorta between January 1982 and December 1997. Overall, in-hospital mortality was 18.0%, and there was no significant difference between those who had coronary angiography on the day of surgery compared with those who had not (No: 16%, n = 81 vs. Yes: 22%, n = 41, p = 0.46). Multivariate analysis revealed that a history of myocardial infarction (MI) was the only predictor of in-hospital mortality (relative risk: 4.98 95% confidence interval: 1.48-16.75, p = 0.009); however, coronary angiography had no impact on in-hospital mortality in patients with a history of MI. Furthermore, coronary angiography did not significantly affect the incidence of coronary artery bypass grafting (CABG) during aortic surgery (17% vs. 25%, Yes vs. No). Operative reports revealed that 74% of all CABG procedures were performed because of coronary dissection, and not coronary artery disease.\"\nQuestion:\n\"Does coronary angiography before emergency aortic surgery affect in-hospital mortality?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27394685": {
                "source": [
                    "\"The prevalence of combined humeral and glenoid defects varies between 79 and 84\u00a0% in case of chronic posttraumatic anterior shoulder instability. The main goal of this study was to evaluate the relationship between humeral and glenoid defects based on quantitative radiological criteria.\nA retrospective study was performed between 2000 and 2011 including patients who underwent primary surgical shoulder stabilization for chronic posttraumatic anterior shoulder instability, with bone defects in both the glenoid and humerus and a healthy contralateral shoulder. The following measurements were taken: D/R ratio (Hill-Sachs lesion depth/humeral head radius) on an AP X-ray in internal rotation and the D1/D2 ratio [diameter of the involved glenoid articular surfaces (D1)/the healthy one (D2)] on a comparative Bernageau glenoid profile view. Measurements were taken by two observers. Correlations were determined by the Spearman correlation coefficients (r), Bland and Altman diagrams, and intra-class correlation coefficients (ICC). A sample size calculation was done.\nThirty patients were included, 25 men/5 women, mean age 29.8\u00a0\u00b1\u00a011.2\u00a0years. The mean D/R was 23\u00a0\u00b1\u00a012\u00a0% for observer 1 and 23\u00a0\u00b1\u00a010\u00a0% for observer 2. The mean D1/D2 was 95\u00a0\u00b1\u00a04\u00a0% for observer 1 and 94\u00a0\u00b1\u00a06\u00a0% for observer 2. No significant correlation was found between humeral and glenoid bone defects by observer 1 (r\u00a0=\u00a00.23, p\u00a0=\u00a00.22) or observer 2 (r\u00a0=\u00a00.05, p\u00a0=\u00a00.78). Agreement of the observers for the D/R ratio was excellent (ICC\u00a0=\u00a00.89\u00a0\u00b1\u00a00.04, p\u00a0<\u00a00.00001) and good for the D1/D2 ratio (ICC\u00a0=\u00a00.54\u00a0\u00b1\u00a00.14, p\u00a0=\u00a00.006).\"\nQuestion:\n\"Bony defects in chronic anterior posttraumatic dislocation of the shoulder: Is there a correlation between humeral and glenoidal lesions?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26399179": {
                "source": [
                    "\"To report three cases illustrating that it is not unusual for a primary eyelid tumour to metastasise to the parotid gland and vice versa.\nTwo patients with malignant parotid tumours underwent radical parotidectomy and presented subsequently with eyelid lesions. Biopsy showed that both eyelid lesions were histologically similar to the primary parotid tumour. A third patient was noted to have ipsilateral upper eyelid and parotid gland tumours. Histology and immunocytochemistry were used to differentiate the primary tumour and the metastasis.\"\nQuestion:\n\"Eyelid-parotid metastasis: do we screen for coexisting masses?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16968876": {
                "source": [
                    "\"The aim of this prognostic factor analysis was to investigate if a patient's self-reported health-related quality of life (HRQOL) provided independent prognostic information for survival in non-small cell lung cancer (NSCLC) patients.\nPretreatment HRQOL was measured in 391 advanced NSCLC patients using the EORTC QLQ-C30 and the EORTC Lung Cancer module (QLQ-LC13). The Cox proportional hazards regression model was used for both univariate and multivariate analyses of survival. In addition, a bootstrap validation technique was used to assess the stability of the outcomes.\nThe final multivariate Cox regression model retained four parameters as independent prognostic factors for survival: male gender with a hazard ratio (HR) = 1.32 (95% CI 1.03-1.69; P = 0.03); performance status (0 to 1 versus 2) with HR = 1.63 (95% CI 1.04-2.54; P = 0.032); patient's self-reported score of pain with HR= 1.11 (95% CI 1.07-1.16; P<0.001) and dysphagia with HR = 1.12 (95% CI 1.04-1.21; P = 0.003). A 10-point shift worse in the scale measuring pain and dysphagia translated into an 11% and 12% increased in the likelihood of death respectively. A risk group categorization was also developed.\"\nQuestion:\n\"Is a patient's self-reported health-related quality of life a prognostic factor for survival in non-small-cell lung cancer patients?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26485091": {
                "source": [
                    "\"Several atypical antipsychotics (AAPs) are used as second-line agents for treatment resistant depression. AAPs can be expensive compared to other treatment options and can cause several side effects.\nTo estimate healthcare costs and utilization of AAPs compared to other second-line agents.\nObservational study using Medicaid claims data (2006-2011). Subjects were depression-diagnosed adult members with at least two prescriptions of antidepressant medications followed by a second-line agent. Gamma generalized linear models (GLM) produced estimates of the difference in mean expenditures among treatment groups after adjusting for individual baseline characteristics using propensity scores. Negative binomial models produced estimates of the difference in number of hospitalizations and emergency department (ED) visits.\nA total of 3910 members received second-line treatment. Treatment groups were AAPs (n\u2009=\u20092211), augmentation agents other than AAPs (n\u2009=\u20091008), and antidepressant switching (n\u2009=\u2009691). AAPs resulted in higher mean adjusted pharmacy costs and higher mean adjusted total mental health-related costs. Mean adjusted total healthcare costs and number of inpatient and ED visits were not different among treatments.\"\nQuestion:\n\"Does the use of atypical antipsychotics as adjunctive therapy in depression result in cost savings?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24433626": {
                "source": [
                    "\"Medicare beneficiaries who have chronic conditions are responsible for a disproportionate share of Medicare fee-for-service expenditures. The objective of this study was to analyze the change in the health of Medicare beneficiaries enrolled in Part A (hospital insurance) between 2008 and 2010 by comparing the prevalence of 11 chronic conditions.\nWe conducted descriptive analyses using the 2008 and 2010 Chronic Conditions Public Use Files, which are newly available from the Centers for Medicare and Medicaid Services and have administrative (claims) data on 100% of the Medicare fee-for-service population. We examined the data by age, sex, and dual eligibility (eligibility for both Medicare and Medicaid).\nMedicare Part A beneficiaries had more chronic conditions on average in 2010 than in 2008. The percentage increase in the average number of chronic conditions was larger for dual-eligible beneficiaries (2.8%) than for nondual-eligible beneficiaries (1.2%). The prevalence of some chronic conditions, such as congestive heart failure, ischemic heart disease, and stroke/transient ischemic attack, decreased. The deterioration of average health was due to other chronic conditions: chronic kidney disease, depression, diabetes, osteoporosis, rheumatoid arthritis/osteoarthritis. Trends in Alzheimer's disease, cancer, and chronic obstructive pulmonary disease showed differences by sex or dual eligibility or both.\"\nQuestion:\n\"Prevalence of chronic conditions among Medicare Part A beneficiaries in 2008 and 2010: are Medicare beneficiaries getting sicker?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22023714": {
                "source": [
                    "\"To explore the impact of delivery mode on women's postpartum quality of life in rural China and probe factors influencing postnatal quality of life.\nChildbirth significantly affects puerpera's physical, psychological and social domains of quality of life. Under the circumstance of increasing high caesarean section rate in rural China, the impact of delivery mode on postnatal quality of life remains unclear.\nCross-sectional study design.\nWomen residing in rural areas and in their 0-12 months after childbirth from 30 rural townships participated in a household survey. A structured questionnaire was used to evaluate women's socio-demographic characteristics, previous pregnant experiences, foetal characteristics and use of maternal health services. The scale for rural postnatal quality of life was adopted to assess postnatal quality of life from six dimensions: physical complaints and pain, sleep and energy, sex satisfaction, interpersonal communication, self-evaluated living stress and perceived life satisfaction.\nThe overall caeserean section rate was 70\u00b70% (962/1375), and most of them (59\u00b77%) were selected by maternal request. None of six dimensions and total score of quality of life displayed significant difference between women with normal delivery and cesaerean section. It was found that postnatal home visit related to good postnatal quality of life and lower husband education level, male gender of infant were associated with poor quality of life.\"\nQuestion:\n\"Does delivery mode affect women's postpartum quality of life in rural China?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22453060": {
                "source": [
                    "\"Bystander resuscitation plays an important role in lifesaving cardiopulmonary resuscitation (CPR). A significant reduction in the \"no-flow-time\", quantitatively better chest compressions and an improved quality of ventilation can be demonstrated during CPR using supraglottic airway devices (SADs). Previous studies have demonstrated the ability of inexperienced persons to operate SADs after brief instruction. The aim of this pilot study was to determine whether an instruction manual consisting of four diagrams enables laypersons to operate a Laryngeal Mask Supreme\u00ae (LMAS) in the manikin.\nAn instruction manual of four illustrations with speech bubbles displaying the correct use of the LMAS was designed. Laypersons were handed a bag containing a LMAS, a bag mask valve device (BMV), a syringe prefilled with air and the instruction sheet, and were asked to perform and ventilate the manikin as displayed. Time to ventilation was recorded and degree of success evaluated.\nA total of 150 laypersons took part. Overall 145 participants (96.7%) inserted the LMAS in the manikin in the right direction. The device was inserted inverted or twisted in 13 (8.7%) attempts. Eight (5.3%) individuals recognized this and corrected the position. Within the first 2 minutes 119 (79.3%) applicants were able to insert the LMAS and provide tidal volumes greater than 150 ml (estimated dead space). Time to insertion and first ventilation was 83.2 \u00b1 29 s. No significant difference related to previous BLS training (P = 0.85), technical education (P = 0.07) or gender could be demonstrated (P = 0.25).\"\nQuestion:\n\"Does a 4 diagram manual enable laypersons to operate the Laryngeal Mask Supreme\u00ae?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14518645": {
                "source": [
                    "\"Deaths from injury and poisoning (suicide, accidents, undetermined deaths, and homicide) are the major cause of death among young men aged 15-39 years in England and Wales and have been increasing in recent years.AIM: To describe common characteristics among young men who die from injury and poisoning.\nWe employed a retrospective survey methodology to investigate factors associated with deaths by injury and poisoning among young men aged 15-39 years (n = 268) in Merseyside and Cheshire during 1995. Data were collected from Coroner's inquest notes and General Practitioner records.\nThe most common cause of death was poisoning by alcohol and drugs (29.1%, n = 78). A high proportion of cases were unemployed (39.4%, n = 106). Cases were also more likely to be single compared to the general population (74.2% vs 55.5%). Self-destructive behaviour was evident in 77% of deaths (n = 206).\"\nQuestion:\n\"Injury and poisoning mortality among young men--are there any common factors amenable to prevention?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23072266": {
                "source": [
                    "\"This study examined changes in the use of complementary and alternative medicine (CAM) therapies by U.S. adults aged 18 years or older with chronic disease-related functional limitations between 2002 and 2007.\nThe study was a cross-sectional survey.SETTING/\nThe study was conducted in the United States.\nThe study comprised adults aged 18 years or older with chronic disease-related functional limitations.\nData were obtained from the 2002 and 2007 U.S. National Health Interview Survey to compare the use of 22 CAM therapies (n=9313 and n=7014, respectively). Estimates were age adjusted to the year 2000 U.S. standard population.\nThe unadjusted and age-standardized prevalence of overall CAM use (22 therapies comparable between both survey years) was higher in 2007 than in 2002 (30.6% versus 26.9%, p<0.001 and 34.4% versus 30.6%, p<0.001, respectively). Adults with functional limitations that included changing and maintaining body position experienced a significant increase in CAM use between 2002 and 2007 (31.1%-35.0%, p<0.01). The use of deep breathing exercises was the most prevalent CAM therapy in both 2002 and 2007 and increased significantly during this period (from 17.9% to 19.9%, p<0.05). The use of meditation, massage, and yoga also increased significantly from 2002 and 2007 (11.0%-13.5%, p<0.01; 7.0%-10.9%, p<0.0001; and 5.1% to 6.6%, p<0.05, respectively), while the use of the Atkins diet decreased (2.2%- 1.4%, p<0.01).\"\nQuestion:\n\"Has the use of complementary and alternative medicine therapies by U.S. adults with chronic disease-related functional limitations changed from 2002 to 2007?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11713724": {
                "source": [
                    "\"Cancer of the buccal mucosa is an uncommon and aggressive neoplasm of the oral cavity. Less than 2% of patients treated for cancer of the oral cavity at Roswell Park Cancer Institute (RPCI) from 1971 to 1997 had primary buccal cancers. Because the majority of these patients did not undergo any adjuvant treatment, this group provided us with the opportunity to assess the relationship between margin status and local recurrence for both small (T1-T2) and large (T3-T4) tumors treated with surgery alone.\nThe RPCI tumor registry database reported 104 patients who were treated for buccal carcinoma. A retrospective chart review identified 27 patients who met our criteria for a buccal mucosal primary tumor (epicenter of the mass in the buccal mucosa). There were 13 men and 14 women, ranging in age from 34 to 94 years (mean, 75). Data were collected regarding patient demographics, presenting symptoms, stage, treatment received, and outcome.\nAll patients underwent surgical resection of their primary lesion; 21 (75%) had T1 or T2 tumors. The rate of local recurrence was 56% for the group as a whole. Patients with close or positive margins had a 66% local failure rate as compared with 52% when surgical margins were negative (greater than or equal to 5 mm from the resection margin after tissue fixation; P = ns). Among those in whom negative margins were achieved, patients with T1-T2 disease had a 40% local failure rate with surgical resection alone.\"\nQuestion:\n\"Cancer of the buccal mucosa: are margins and T-stage accurate predictors of local control?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21864397": {
                "source": [
                    "\"Nasopharyngeal carcinoma (NPC) with lung metastasis alone has been reported as a relatively favorable prognostic group, and combined modality treatment might be indicated for selected cases. However, the prognostic factors determining survival of this group and the indication of combined therapy have not been thoroughly studied.\nWe retrospectively reviewed 246 patients of NPC with lung metastasis(es) alone presented at diagnosis or as the first failure after primary treatment from 1993 to 2008 in an academic tertiary hospital. Univariate and multivariate survival analyses of post-metastasis survival (PMS) and overall survival (OS) were carried out to determine the prognostic factors.\nThe 3-year, 5-year, and 10-year of PMS and OS for the whole cohort were 34.3%, 17.0%, 8.6% and 67.8%, 45.4%, 18.5%, respectively. The median PMS (45.6 months vs. 23.7 months) and OS (73.7 months vs. 46.2 months) of patients treated with combined therapy was significantly longer than that of those treated with chemotherapy alone (P<0.001). Age, disease-free interval (DFI) and treatment modality were evaluated as independent prognostic factors of OS, while only age and treatment modality retain their independent significance in PMS analysis. In stratified survival analysis, compared to chemotherapy alone, combined therapy could benefit the patients with DFI>1 year, but not those with DFI \u2264 1 year.\"\nQuestion:\n\"Factors determining the survival of nasopharyngeal carcinoma with lung metastasis alone: does combined modality treatment benefit?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20363841": {
                "source": [
                    "\"The US Preventive Services Task Force recommends against spirometry in the absence of symptoms. However, as much as 50% of COPD cases in the United States remain undiagnosed.\nReport of symptoms, smoking history, and spirometric data were collected from subjects screened for a work-related medical evaluation (N = 3,955). Prevalence of airflow obstruction and respiratory symptoms were assessed. Sensitivity, specificity, positive and negative predictive values, and relative risks of predicting symptoms and smoking history for COPD were calculated.\nForty-four percent of smokers in our sample had airways obstruction (AO). Of these, 36% reported a diagnosis of or treatment for COPD. Odds ratio (95% CI) for AO with smoking (>or = 20 pack-years) was 3.73 (3.12- 4.45), 1.98 (1.73-2.27) for cough, 1.79 (1.55-2.08) for dyspnea, 1.95 (1.70-2.34) for sputum, and 2.59 (2.26-2.97) for wheeze. Respiratory symptoms were reported by 92% of smokers with AO, 86% smokers with restriction, 76% smokers with normal spirometry, and 73% of nonsmokers. Sensitivity (92% vs 90%), specificity (19% vs 22%), positive (47% vs 40%) and negative (75% vs 80%) predictive values for the presence of one or more symptoms were similar between smokers and all subjects.\"\nQuestion:\n\"Do symptoms predict COPD in smokers?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24449622": {
                "source": [
                    "\"This study aimed to show the relationship between serum paraoxonase 1 level and the epicardial fat tissue thickness.\nTwo hundred and seven patients without any atherosclerotic disease history were included in this cross-sectional observational study. Correlation analysis was performed to determine the correlation between epicardial fat tissue thickness, which was measured by echocardiography and serum paraoxonase 1 level. Also correlation analysis was performed to show correlation between patients' clinical and laboratory findings and the level of serum paraoxonase 1 (PON 1) and the epicardial fat tissue thickness. Pearson and Spearman test were used for correlation analysis.\nNo linear correlation between epicardial fat tissue thickness and serum PON 1 found (correlation coefficient: -0.127, p=0.069). When epicardial fat tissue thickness were grouped as 7 mm and over, and below, and 5 mm and over, and below, serum PON 1 level were significantly lower in \u22657 mm group (PON1 : 168.9 U/L) than<7 mm group (PON 1: 253.9 U/L) (p<0.001). Also hypertension prevalence was increased in \u22657 mm group (p=0.001). Serum triglyceride was found to be higher in \u22657 mm group (p=0.014), body mass index was found higher in \u22655 mm group (p=0.006).\"\nQuestion:\n\"Is there a relationship between serum paraoxonase level and epicardial fat tissue thickness?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25756710": {
                "source": [
                    "\"To validate a clinical diagnostic tool, used by emergency physicians (EPs), to diagnose the central cause of patients presenting with vertigo, and to determine interrater reliability of this tool.\nA convenience sample of adult patients presenting to a single academic ED with isolated vertigo (i.e. vertigo without other neurological deficits) was prospectively evaluated with STANDING (SponTAneousNystagmus, Direction, head Impulse test, standiNG) by five trained EPs. The first step focused on the presence of spontaneous nystagmus, the second on the direction of nystagmus, the third on head impulse test and the fourth on gait. The local standard practice, senior audiologist evaluation corroborated by neuroimaging when deemed appropriate, was considered the reference standard. Sensitivity and specificity of STANDING were calculated. On the first 30 patients, inter-observer agreement among EPs was also assessed.\nFive EPs with limited experience in nystagmus assessment volunteered to participate in the present study enrolling 98 patients. Their average evaluation time was 9.9 \u00b1 2.8\u2009min (range 6-17). Central acute vertigo was suspected in 16 (16.3%) patients. There were 13 true positives, three false positives, 81 true negatives and one false negative, with a high sensitivity (92.9%, 95% CI 70-100%) and specificity (96.4%, 95% CI 93-38%) for central acute vertigo according to senior audiologist evaluation. The Cohen's kappas of the first, second, third and fourth steps of the STANDING were 0.86, 0.93, 0.73 and 0.78, respectively. The whole test showed a good inter-observer agreement (k = 0.76, 95% CI 0.45-1).\"\nQuestion:\n\"Can emergency physicians accurately and reliably assess acute vertigo in the emergency department?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11601252": {
                "source": [
                    "\"We have reported previously that cerulein-induced edematous pancreatitis would transform into hemorrhagic pancreatitis by administration of endothelin-1 in rats. In the present study, we tried to protect rat model from developing into hemorrhagic pancreatitis with BQ123 (an ETA receptor antagonist).\nThe rat model was made by 5-hour restraint water-immersion stress and two intraperitoneal injections of cerulein (40 micrograms/kg) at hourly interval. BQ123 (3 or 6 mg/kg) was administered intravenously 30 minutes before and 2 hours after the first cerulein injection.\nAcute hemorrhagic pancreatitis was induced in all rats treated with cerulin + stress. The score for pancreatic hemorrhage was 2.4 +/- 0.2 in this group. In the rats pretreated with BQ123, the score was reduced to 1.0 +/- 0.0, pancreas wet weight and serum amylase activity were significantly reduced, and histologic alterations in the pancreas lightened, also the local pancreatic blood flow improved without affecting the systemic blood pressure.\"\nQuestion:\n\"Is endothelin-1 an aggravating factor in the development of acute pancreatitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25255719": {
                "source": [
                    "\"This prospective case-control study consisted of 33 patients with pre-eclampsia and 32 normotensive pregnant patients as controls. All of the subjects underwent otoscopic examinations - pure tone audiometry (0.25-16\u2009kHz) and transient evoked otoacoustic emission (1-4\u2009kHz) tests - during their third trimester of pregnancy.\nThe mean ages of the patients with pre-eclampsia and the control subjects were 29.6\u2009\u00b1\u20095.7 and 28.6\u2009\u00b1\u20095.3 years, respectively. The baseline demographic characteristics, including age, gravidity, parity number, and gestational week, were similar between the two patient groups. Hearing thresholds in the right ear at 1, 4, 8, and 10\u2009kHz and in the left ear at 8 and 10\u2009kHz were significantly higher in the patients with pre-eclampsia compared to the control subjects. The degree of systolic blood pressure measured at the time of diagnosis had a deteriorating effect on hearing at 8, 10, and 12\u2009kHz in the right ear and at 10\u2009kHz in the left ear.\"\nQuestion:\n\"Hearing loss: an unknown complication of pre-eclampsia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24359102": {
                "source": [
                    "\"Skin diseases are the most frequently recognized occupational diseases in Denmark. The prognosis for occupational contact dermatitis is often poor.\nTo investigate the prognosis, assessed by eczema, job status and skin-related quality of life, among patients allergic to rubber chemicals and latex (ubiquitous allergens) and epoxy (nonubiquitous allergen), 2\u00a0years after recognition of occupational allergic contact dermatitis.\nFrom a cohort of all patients recognized as having occupational dermatitis by the Danish National Board of Industrial Injuries in 2010, 199 patients with relevant rubber allergy (contact allergy to rubber chemicals or contact urticaria from latex) or epoxy allergy were identified. Follow-up consisted of a questionnaire covering current severity of eczema, employment, exposure and quality of life.\nThe response rate was 75%. Clearance of eczema was reported by 11% of patients and 67% reported improvement. Overall 22% of patients with allergy to a nonubiquitous allergen had total clearance of eczema compared with 10% of cases allergic to ubiquitous allergens and 0% of those with contact urticaria (P\u00a0=\u00a00\u00b7116). Improvement was significantly more frequent in those who had changed jobs compared with those who had not (P\u00a0=\u00a00\u00b701).\"\nQuestion:\n\"Two-year follow-up survey of patients with allergic contact dermatitis from an occupational cohort: is the prognosis dependent on the omnipresence of the allergen?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23860049": {
                "source": [
                    "\"To evaluate the role of clinical assessment with selective use of imaging studies in the management of suspected acute appendicitis in children.\nMedical records of children referred to Emergency Room in 2010 for suspected appendicitis were retrospectively reviewed. Diagnostic investigations divided by age and sex were related to pathological findings. Negative appendectomy and complication rates were calculated.\n923 children needed surgical assessment : In 75.7% of them surgical indication was excluded and 24.3% were admitted to surgical ward for observation. Appendectomy was eventually performed in 137 patients (61.9%), 82.4% of them without any preoperative imaging while 17.6% underwent selective studies, mainly abdominal ultrasonography (14.6%). Imaging was requested twice as frequently in not operated admitted children (39.3%) than in the operated ones (17.5%, P<0.001). Overall complicated appendicitis rate (peritonitis and abscess) resulted 26.4% and negative appendectomy rate 8.8%. Females older than 10 years presented histologically not-confirmed appendicitis in 22.2% of cases, while the younger ones presented more frequently complicated appendicitis (29.3%).\"\nQuestion:\n\"Do we need imaging to diagnose appendicitis in children?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22668852": {
                "source": [
                    "\"The high prevalence of obesity in African American (AA) women may result, in part, from a lower resting metabolic rate (RMR) than non-AA women. If true, AA women should require fewer calories than non-AA women to maintain weight. Our objective was to determine in the setting of a controlled feeding study, if AA women required fewer calories than non-AA women to maintain weight.\nThis analysis includes 206 women (73% AA), aged 22-75 years, who participated in the Dietary Approaches to Stop Hypertension (DASH) trial-a multicenter, randomized, controlled, feeding study comparing the effects of 3 dietary patterns on blood pressure in individuals with prehypertension or stage 1 hypertension. After a 3-week run-in, participants were randomized to 1 of 3 dietary patterns for 8 weeks. Calorie intake was adjusted during feeding to maintain stable weight. The primary outcome of this analysis was average daily calorie (kcal) intake during feeding.\nAA women had higher baseline weight and body mass index than non-AA women (78.4 vs 72.4 kg, P<.01; 29.0 vs 27.6 kg/m(2), P<.05, respectively). During intervention feeding, mean (SD) kcal was 2168 (293) in AA women and 2073 (284) in non-AA women. Mean intake was 94.7 kcal higher in AA women than in non-AA women (P<.05). After adjustment for potential confounders, there was no difference in caloric intake between AA and non-AA women (\u0394 = -2.8 kcal, P = .95).\"\nQuestion:\n\"Do African American women require fewer calories to maintain weight?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25735444": {
                "source": [
                    "\"A multicentre, retrospective study was conducted of patients with rectal cancer threatening or affecting the prostatic plane, but not the bladder, judged by magnetic resonance imaging (MRI). The use of preoperative chemoradiotherapy and the type of urologic resection were correlated with the status of the pathological circumferential resection margin (CRM) and local recurrence.\nA consecutive series of 126 men with rectal cancer threatening (44) or affecting (82) the prostatic plane on preoperative staging and operated with local curative intent between 1998 and 2010 was analysed. In patients who did not have chemoradiotherapy but had a preoperative threatened anterior margin the CRM-positive rate was 25.0%. In patients who did not have preoperative chemoradiotherapy but did have an affected margin, the CRM-positive rate was 41.7%. When preoperative radiotherapy was given, the respective CRM infiltration rates were 7.1 and 20.7%. In patients having preoperative chemoradiotherapy followed by prostatic resection the rate of CRM positivity was 2.4%. Partial prostatectomy after preoperative chemoradiotherapy resulted in a free anterior CRM in all cases, but intra-operative urethral damage occurred in 36.4% of patients who underwent partial prostatectomy, resulting in a postoperative urinary fistula in 18.2% of patients.\"\nQuestion:\n\"Rectal cancer threatening or affecting the prostatic plane: is partial prostatectomy oncologically adequate?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21481154": {
                "source": [
                    "\"Our hypothesis is that the adoption of Department of Health (DH) guidance has led to an improvement in outcome in gynaecological cancer survival.\nIn 1999 the DH in England introduced the Improving Outcomes in Gynaecological Cancer guidance, advising case management by multidisciplinary teams with surgical concentration in specialist hospitals. This guidance was rapidly adopted in the East of England, with a population of 2.5 million.\nThe population of the Anglia Cancer Network was approximately 2.3 million.\nFrom 1996 to 2003, details of 3406 cases of gynaecological cancer were identified in the Anglia region of England. Survival analysis was performed by Cox proportional hazards regression, relative to cases diagnosed in 1996.\nPrimary endpoint was survival.\nThe survival rates for cases diagnosed between 1996 and 1999 were broadly the same across the time period, with a marked improvement taking place in 2000, and continuing to 2003 (HR 0.71, 95% CI 0.64-0.79, comparing 2000-03 with 1996-99 diagnoses), for all gynaecological sites combined. Adjustment for treatments or method of case follow-up did not attenuate these improvements. There was a concurrent change towards major surgery being performed in specialist centres from 2000.\"\nQuestion:\n\"Improvements in survival of gynaecological cancer in the Anglia region of England: are these an effect of centralisation of care and use of multidisciplinary management?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15223725": {
                "source": [
                    "\"To determine whether there are differences between blood pressure (BP) measured by the nurse (NBP), BP measured by the physician (PBP) and self-measured BP in treated hypertensive patients and, if found, to evaluate their clinical importance.\nAn observational study is carried out with hypertensive patients recruited from two village-based community health centres in Catalonia (Spain) serving an area with a total population of 2800 inhabitants. All patients treated for hypertension visiting the health centre on a specific day of the week and during the same timetable between October 2000 and May 2001 were included.\nThe difference between physician-systolic BP and nurse-systolic BP was 5.16 mmHg (95% CI 2.62-7.7; p<0.001). The difference between physician-systolic BP and self-measured systolic BP was 4.67 mmHg (95% CI 0.89-8.44; p=0.016). The differences between nurse-systolic BP and self-measured systolic BP were not significant (0.49 mmHg; 95% CI 3.71-2.71; p=0.758). With regards to diastolic BP, no significant differences were found between the different ways of measurement. NBP gave the following values: sensitivity (Sn) of 92% and specificity (Sp) of 60%; positive predictive value (PPV) of 65.7% and negative predictive value (NPV) of 90% with a positive coefficient of probability (CP+) of 2.3 and a negative coefficient of probability (CP-) of 0.133. PBP gave the following results: Sn=72%; Sp=66.7%; PPV=64.3%; NPV=74.1%; CP+=2.16 and CP- = 0.420.\"\nQuestion:\n\"Does blood pressure change in treated hypertensive patients depending on whether it is measured by a physician or a nurse?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23735520": {
                "source": [
                    "\"To determine the potential prognostic value of using functional magnetic resonance imaging (fMRI) to identify patients with disorders of consciousness, who show potential for recovery.\nObservational study.\nUnit for acute rehabilitation care.\nPatients (N=22) in a vegetative state (VS; n=10) and minimally conscious state (MCS; n=12) during the first 200 days after the initial incident.\nNot applicable.\nFurther course on the Coma Recovery Scale-Revised.\nParticipants performed a mental imagery fMRI paradigm. They were asked to alternately imagine playing tennis and navigating through their home. In 14 of the 22 examined patients (VS, n=5; MCS, n=9), a significant activation of the regions of interest (ROIs) of the mental imagery paradigm could be found. All 5 patients with activation of a significant blood oxygen level dependent signal, who were in a VS at the time of the fMRI examination, reached at least an MCS at the end of the observation period. In contrast, 5 participants in a VS who failed to show activation in ROIs, did not (sensitivity 100%, specificity 100%). Six of 9 patients in an MCS with activation in ROIs emerged from an MCS. Of 3 patients in an MCS who did not show activation, 2 patients stayed in an MCS and 1 patient emerged from the MCS (sensitivity 85%, specificity 40%).\"\nQuestion:\n\"Can mental imagery functional magnetic resonance imaging predict recovery in patients with disorders of consciousness?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10973547": {
                "source": [
                    "\"It is generally assumed, that patients with Werlhof's disease (WD) are at increased risk for bleeding complications when undergoing cardiac surgery with extracorporeal circulation. Therefore we performed this case control study to estimate the real risk for bleeding complications of these patients.\nBetween 05/95 and 07/98, ten patients with WD (eight males, two females) underwent cardiac surgery employing extracorporeal circulation (WD-group). Five of these patients with platelet counts below 80/nl were treated by immunoglobulins preoperatively. Each patient with WD was matched to five patients without WD (no-WD-group) using diagnosis, age, gender, ejection fraction, number of distal anastomosis and body-mass-index as matching criteria.\nMean number of platelet counts were significant lower in the WD-group than in the no-WD-group despite a significant increase of platelet counts after immunoglobulin treatment (54/nl-->112/nl, P=0.018). On the day before, directly after and on the first day after surgery they were 141/nl vs. 215/nl (P=0.012), 75/nl vs. 147/nl (P=0.001) and 93/nl vs. 136/nl (P=0.009). Accordingly, patients of the WD-group received significantly more platelet concentrates than patients of the no-WD-group (mean number of platelet concentrates: 2.3 versus 0.7, P=0.007). Total drainage loss via the mediastinal chest tubes was almost identical (1197 ml in the no-WD-group and 1140 ml in the WD-group). One patient of each group suffered from a bleeding complication requiring reexploration. Three patients of the no-WD-group (6%) and one patient of the WD-group (10%) expired postoperatively unrelated to WD.\"\nQuestion:\n\"Are patients with Werlhof's disease at increased risk for bleeding complications when undergoing cardiac surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24995509": {
                "source": [
                    "\"To investigate the association between age-related macular degeneration (AMD) and the polymorphisms of HIF1A, a major vascular epithelial growth factor regulator under hypoxic conditions. The associations of AMD and polymorphisms of genes CFH, SKIV2L and MYRIP were also studied.\nProspective study.\nEighty-seven AMD patients and 80 healthy subjects admitted to the Department of Ophthalmology at Pamukkale University Hospital, Denizli, Turkey, were included: 45 (52%) had wet type AMD, and 42 (48%) had dry type AMD.\nPolymorphisms rs1061170 (CFH), rs429608 (SKIV2L), rs2679798 (MYRIP) and both rs11549465 and rs11549467 (HIF1A) were investigated in DNA isolated from peripheral blood samples of the cases and controls by dye-termination DNA sequencing.\nGenotype distribution of rs1061170 (CFH), rs429608 (SKIV2L), rs2679798 (MYRIP) and both rs11549465 and rs11549467 (HIF1A) in AMD cases and healthy controls; association between genotypes and AMD subtypes.\nGiven the significant difference between the mean age of case and control groups (72.13\u2009\u00b1\u20095.77 vs. 62.80\u2009\u00b1\u20095.22, respectively) (P\u2009=\u2009.000), subsequent analyses were adjusted for age. We found that having at least one C allele for polymorphism rs1061170 increases AMD risk independent of age (OR\u2009=\u20092.42, 95% confidence interval [CI], 1.22-4.81). The ancestral T allele for polymorphism rs1061170 has a protective effect for AMD (OR\u2009=\u20090.53, 95% CI, 0.34-0.83). No statistically significant difference for distributions of other single nucleotide polymorphisms (SNPs) emerged between patients and healthy subjects.\"\nQuestion:\n\"HIF1A as a major vascular endothelial growth factor regulator: do its polymorphisms have an association with age-related macular degeneration?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22348433": {
                "source": [
                    "\"The optimum protocol for expander volume adjustment with respect to the timing and application of radiotherapy remains controversial.\nEighteen New Zealand rabbits were divided into three groups. Metallic port integrated anatomic breast expanders of 250 cc were implanted on the back of each animal and controlled expansion was performed. Group I underwent radiotherapy with full expanders while in Group II, expanders were partially deflated immediately prior to radiotherapy. Control group did not receive radiotherapy.The changes in blood flow at different volume adjustments were investigated in Group II by laser Doppler flowmetry. Variations in the histopathologic properties of the irradiated tissues including the skin, capsule and the pocket floor, were compared in the biopsy specimens taken from different locations in each group.\nA significant increase in skin blood flow was detected in Group II with partial expander deflation. Overall, histopathologic exam revealed aggravated findings of chronic radiodermatitis (epidermal atrophy, dermal inflammation and fibrosis, neovascularisation and vascular changes as well as increased capsule thickness) especially around the lower expander pole, in Group II.\"\nQuestion:\n\"Does partial expander deflation exacerbate the adverse effects of radiotherapy in two-stage breast reconstruction?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16713745": {
                "source": [
                    "\"Cytokine concentration in pancreatic juice of patients with pancreatic disease is unknown. Secretin stimulation allows endoscopic collection of pancreatic juice secreted into the duodenum. We aimed to evaluate the cytokine concentrations in pancreatic juice of patients with abdominal pain to discriminate presence from absence of pancreatic disease.\nFrom January 2003-December 2004, consecutive patients with abdominal pain compatible with pancreatic origin were enrolled. Patients underwent upper endoscopy. Intravenous secretin (0.2 mug/kg) was given immediately before scope intubation. Pancreatic juice collected from the duodenum was immediately snap-frozen in liquid nitrogen until assays were performed. Pancreatic juice levels of interleukin-8, interleukin-6, intercellular adhesion molecule 1, and transforming growth factor-beta 1 were measured by modified enzyme-linked immunosorbent assays. The final diagnosis was made by the primary gastroenterologist on the basis of medical history; laboratory, endoscopic, and imaging studies; and clinical follow-up. Fisher exact test and Kruskal-Wallis rank sum test were used for statistical analysis.\nOf 130 patients screened, 118 met the inclusion criteria. Multivariate analysis revealed that only interleukin-8 was able to discriminate between normal pancreas and chronic pancreatitis (P = .011), pancreatic cancer (P = .044), and the presence of pancreatic diseases (P = .007). Individual cytokine concentrations were not significantly different in chronic pancreatitis compared with pancreatic cancer.\"\nQuestion:\n\"Do cytokine concentrations in pancreatic juice predict the presence of pancreatic diseases?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15000338": {
                "source": [
                    "\"To determine the cost of 46 commonly used investigations and therapies and to assess British Columbia family doctors' awareness of these costs.\nMailed survey asking about costs of 23 investigations and 23 therapies relevant to family practice. A random sample of 600 doctors was asked to report their awareness of costs and to estimate costs of the 46 items.\nBritish Columbia.\nSix hundred family physicians.\nEstimates within 25% of actual cost were considered correct. Associations between cost awareness and respondents'characteristics (eg, sex, practice location) were sought. Degree of error in estimates was also assessed.\nOverall, 283 (47.2%) surveys were returned and 259 analyzed. Few respondents estimated costs within 25% of true cost, and estimates were highly variable. Physicians underestimated costs of expensive drugs and laboratory investigations and overestimated costs of inexpensive drugs. Cost awareness did not correlate with sex, practice location, College certification, faculty appointment, or years in practice.\"\nQuestion:\n\"Do family physicians know the costs of medical care?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25371231": {
                "source": [
                    "\"The aetiology of osteochondritis dissecans is still unclear. The aim of this prospective pilot study was to analyse whether vitamin D insufficiency, or deficiency, might be a contributing etiological factor in the development of an OCD lesion.\nThe serum level of vitamin D3 in 23 consecutive patients (12 male and 11 female) suffering from a stage III, or stages III and IV, OCD lesion (mostly stage III) admitted for surgery was measured.\nThe patients' mean age was 31.3\u00a0years and most of them already exhibited closed epiphyseal plates. In the majority of patients (18/23), a distinct vitamin D3 deficiency was found, two patients were vitamin D3-insufficient and, in three patients, the vitamin D3 level reached the lowest normal value.\"\nQuestion:\n\"Is vitamin D insufficiency or deficiency related to the development of osteochondritis dissecans?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16647887": {
                "source": [
                    "\"The purpose of this study was to investigate the outcomes that are associated with pregnancy and treated hypothyroidism.\nThis was a retrospective cohort study of all women who received prenatal care and were delivered at the University of California, San Francisco, between 1989 and 2001. All patients with hypothyroidism diagnosed before pregnancy or early in pregnancy were identified. Maternal, fetal, and obstetric outcomes were then collected and analyzed for women with hypothyroidism and compared with women without hypothyroidism.\nAmong 20,499 deliveries, there were 419 women (2.1%) who were treated for hypothyroidism during the study period. Hypothyroidism was more common among women>or =35 years old, white women, and women without Medicaid insurance. Treated hypothyroidism was not associated with any increase in maternal, fetal, or neonatal complications. In addition, hypothyroidism did not affect mode of delivery.\"\nQuestion:\n\"Are women who are treated for hypothyroidism at risk for pregnancy complications?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10781708": {
                "source": [
                    "\"Most studies on thrombosis prophylaxis focus on postoperative venous thrombosis. In medical wards thrombosis prophylaxis is generally restricted to patients who are immobilised. Our primary aim was to investigate the incidence of venous thrombosis in a general internal ward, to assess whether more rigorous prophylaxis would be feasible.\nWe investigated the incidence of venous thrombosis in patients hospitalised from 1992 to 1996 and related our findings to literature reports.\nThe incidence of symptomatic venous thrombosis in internal patients during hospitalisation was 39/6332 (0.6%). Among these 39 patients, 24 had a malignancy, whereas 876 out of all 6332 patients had a known malignancy. So, the incidence in this group with cancer was 2.7% compared with 0.3% (15/5456) in the non-cancer group (relative risk for venous thrombosis due to malignancy was 10.0 (95%C.I. 5.3-18.9).\"\nQuestion:\n\"Thrombosis prophylaxis in hospitalised medical patients: does prophylaxis in all patients make sense?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20602101": {
                "source": [
                    "\"Studies have indicated that hypoalbuminemia is associated with decreased survival of patients with gastric cancer. However, the prognostic value of albumin may be secondary to an ongoing systemic inflammatory response. The aim of the study was to assess the relation between hypoalbuminemia, the systemic inflammatory response, and survival in patients with gastric cancer.\nPatients diagnosed with gastric carcinoma attending the upper gastrointestinal surgical unit in the Royal Infirmary, Glasgow between April 1997 and December 2005 and who had a pretreatment measurement of albumin and C-reactive protein (CRP) were studied.\nMost of the patients had stage III/IV disease and received palliative treatment. The minimum follow-up was 15 months. During follow-up, 157 (72%) patients died of their cancer. On univariate analysis, stage (p<0.001), treatment (p<0.001), albumin level (p<0.001), and CRP level (p<0.001) were significant predictors of survival. On multivariate analysis, stage (p<0.001), treatment (p<0.001), and CRP level (p<0.001) remained significant predictors of survival. Albumin was no longer an independent predictor of survival.\"\nQuestion:\n\"Is hypoalbuminemia an independent prognostic factor in patients with gastric cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25007420": {
                "source": [
                    "\"Francophones may experience poorer health due to social status, cultural differences in lifestyle and attitudes, and language barriers to health care. Our study sought to compare mental health indicators between Francophones and non-Francophones living in the province of Manitoba.\nTwo populations were used: one from administrative datasets housed at the Manitoba Centre for Health Policy and the other from representative survey samples. The administrative datasets contained data from physician billings, hospitalizations, prescription drug use, education, and social services use, and surveys included indicators on language variables and on self-rated health.\nOutside urban areas, Francophones had lower rates of diagnosed substance use disorder (rate ratio [RR] = 0.80; 95% CI 0.68 to 0.95) and of suicide and suicide attempts (RR = 0.59; 95% CI 0.43 to 0.79), compared with non-Francophones, but no differences were found between the groups across the province in rates of diagnosed mood disorders, anxiety disorders, dementia, or any mental disorders after adjusting for age, sex, and geographic area. When surveyed, Francophones were less likely than non-Francophones to report that their mental health was excellent, very good, or good (66.9%, compared with 74.2%).\"\nQuestion:\n\"Are there mental health differences between francophone and non-francophone populations in manitoba?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23283159": {
                "source": [
                    "\"To investigate the effect of obesity at the start of adolescence on the prevalence, incidence and maintenance of chest wheezing among individuals aged 11-15 years in a birth cohort in a developing country.\nThe seventh follow-up of the 1993 Pelotas birth cohort occurred in 2004 (individuals aged 10-11 years). Between January and August 2008, the eighth follow-up of the cohort was conducted. All the individuals of the original cohort who were alive (who were then adolescents aged between 14 and 15 years) were targets for the study. The International Study of Asthma and Allergies in Childhood (ISAAC) questionnaire was used to define wheezing. In addition to the body mass index (BMI), used to define obesity by the World Health Organization (WHO) criteria, we assessed skinfold thickness.\nFrom the original cohort, 4,349 individuals were located (85.7% follow-up rate). The prevalence of chest wheezing at 11 and 15 years were 13.5% (95% CI: 12.5%-14.5%) and 12.1% (95% CI: 11.1%-13.1%), respectively. The prevalence of wheezing at both times was 4.5% (95% CI: 3.9%-5.1%) and the incidence of wheezing was 7.5% (95% CI: 6.7%-8.3%). Independent of the effect of various confounding variables, the prevalence of wheezing at 15 years was 50% greater among obese individuals than among eutrophic individuals at 11 years (RR 1.53; 95% CI: 1.14-2.05). The greater the skinfold tertile at 11 years, the higher the prevalence of wheezing at 15 years was (p = .011). Weight status and skinfolds did not present any association with incident wheezing. After controlling for confounding factors, the risk of persistent wheezing among obese individuals at 11 years was 1.82 (95% CI: 1.30-2.54).\"\nQuestion:\n\"Is obesity a risk factor for wheezing among adolescents?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24922528": {
                "source": [
                    "\"To explore the extent to which parent-adolescent emotional closeness, family conflict, and parental permissiveness moderate the association of puberty and alcohol use in adolescents (aged 10-14).\nCross-sectional survey of 7631 adolescents from 231 Australian schools. Measures included pubertal status, recent (30day) alcohol use, parent-adolescent emotional closeness, family conflict, parental permissiveness of alcohol use and peer alcohol use. The analysis was based on a two-level (individuals nested within schools) logistic regression model, with main effects entered first, and interaction terms added second.\nThe interaction of family factors and pubertal stage did not improve the fit of the model, so a main effect model of family factors and pubertal stage was adopted. There were significant main effects for pubertal stage with boys in middle puberty at increased odds of alcohol use, and girls in advanced puberty at increased odds of alcohol use.\"\nQuestion:\n\"The association of puberty and young adolescent alcohol use: do parents have a moderating role?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20488985": {
                "source": [
                    "\"To determine the effect of the 2008 English public antibiotic campaigns.\nEnglish and Scottish (acting as controls) adults aged>or = 15 years were questioned face to face about their attitudes to and use of antibiotics, in January 2008 (1888) before and in January 2009 (1830) after the antibiotic campaigns.\nAmong English respondents, there was a small increase in recollection of campaign posters (2009 23.7% versus 2008 19.2%; P = 0.03), but this increase was only 2.3% higher in England than in Scotland. We did not detect any improvement in either England or Scotland, or any differences between England and Scotland in the understanding of the lack of benefit of antibiotics for coughs and colds, and we found no improvement in antibiotic use. We detected a significant increase in respondents retaining leftover antibiotics. Over 20% reported discussing antibiotics with their general practitioner (GP) or nurse in the year to January 2009. The offer of a delayed antibiotic prescription was reported significantly more often by English respondents (19% versus 8% Scottish in 2009; P = 0.01), and English respondents were advised to use other remedies for coughs and colds significantly more often in the year to January 2009 (12.7% in 2009 versus 7.4% in 2008; P<0.001).\"\nQuestion:\n\"The English antibiotic awareness campaigns: did they change the public's knowledge of and attitudes to antibiotic use?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11759976": {
                "source": [
                    "\"To determine survival among patients with epithelial ovarian carcinoma (EOC) who underwent a second-look laparotomy (SLL) and those refusing the procedure. Also to analyze factor(s) influencing the survival of the patients.\nMedical records were reviewed of patients with advanced EOC who were clinically free of disease after primary surgery and platinum-based chemotherapy between January 1, 1992, and December 31, 1998. All of them were offered SLL. Measurement outcomes include patient survival and disease-free survival.\nThere were 50 patients with clinically complete remission after chemotherapy. Sixteen patients underwent SLL, and thirty-four patients refused the procedure (NSLL). Seven patients (43.8%) were reported to have positive SLL. After the median follow-up time of 35 months, 12 patients had died, and 5 patients were lost to follow-up. The median survival time for patients with SLL was about 60 months. Five-year survival rates of patients in the SLL, and NSLL groups were 37 per cent (95%CI = 7%-69%), and 88 per cent (95%CI = 65%-96%) respectively (P<0.001). The median time to relapse was about 25 months for patients with negative SLL. Five-year disease-free survival rates of patients in the negative SLL, and NSLL groups were 28 per cent (95%CI = 4%-59%), and 54 per cent (95%CI = 34%-70%) respectively (P=0.251). By Cox regression analysis, tumor grade was the only significant prognostic factor influencing patients' survival (HR = 6, 95%CI of HR = 1.2-34.2).\"\nQuestion:\n\"Advanced epithelial ovarian carcinoma in Thai women: should we continue to offer second-look laparotomy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18235194": {
                "source": [
                    "\"In a prospective study 218 preschool children were enrolled (stratified in 2 training programs, one specialized for phonologic awareness in order to prevent dyslexia, the other consisting in training of general perception) during the last year of kindergarten. After finishing the first grade 131 children were compared in their reading and writing abilities.\nIn the whole group only a slight difference was found between both training modalities concerning their writing abilities. However, children with a history of hearing loss, actual hearing loss or pathologic middle ear findings profited most from the specialized training program compared to the control in their reading abilities.\"\nQuestion:\n\"Is a specialised training of phonological awareness indicated in every preschool child?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24599411": {
                "source": [
                    "\"To compare the characteristics and prognoses of gastric cancers by tumor location in Korean and U.S. subjects after curative-intent (R0) resection for gastric cancer (GC).\nData were collected for all patients who had undergone R0 resection at one U.S. institution (n = 567) and one South Korean institution (n = 1,620). Patients with gastroesophageal junction tumors or neoadjuvant therapy were excluded. Patient, surgical, and pathologic variables were compared by tumor location. Factors associated with disease-specific survival (DSS) were determined via multivariate analysis.\nIn the Korean cohort, significantly more upper third GC (UTG) patients had undifferentiated, diffuse type, and advanced stage cancers compared to lower third GC (LTG) and middle third GC (MTG) patients. In the U.S. cohort, however, T stage was relatively evenly distributed among UTG, MTG, and LTG patients. The independent predictors of DSS in the Korean cohort were T stage, tumor size, retrieved and positive lymph node counts, and age, but in the U.S. cohort, the only independent predictors were T stage and positive lymph node count. Tumor size significantly affected DSS of Korean UTG patients but not U.S. UTG patients.\"\nQuestion:\n\"Is gastric cancer different in Korea and the United States?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27549226": {
                "source": [
                    "\"The \"health workforce\" crisis has led to an increased interest in health professional education, including MPH programs. Recently, it was questioned whether training of mid- to higher level cadres in public health prepared graduates with competencies to strengthen health systems in low- and middle-income countries. Measuring educational impact has been notoriously difficult; therefore, innovative methods for measuring the outcome and impact of MPH programs were sought. Impact was conceptualized as \"impact on workplace\" and \"impact on society,\" which entailed studying how these competencies were enacted and to what effect within the context of the graduates' workplaces, as well as on societal health.\nThis is part of a larger six-country mixed method study; in this paper, the focus is on the qualitative findings of two English language programs, one a distance MPH program offered from South Africa, the other a residential program in the Netherlands. Both offer MPH training to students from a diversity of countries. In-depth interviews were conducted with 10 graduates (per program), working in low- and middle-income health systems, their peers, and their supervisors.\nImpact on the workplace was reported as considerable by graduates and peers as well as supervisors and included changes in management and leadership: promotion to a leadership position as well as expanded or revitalized management roles were reported by many participants. The development of leadership capacity was highly valued amongst many graduates, and this capacity was cited by a number of supervisors and peers. Wider impact in the workplace took the form of introducing workplace innovations such as setting up an AIDS and addiction research center and research involvement; teaching and training, advocacy, and community engagement were other ways in which graduates' influence reached a wider target grouping. Beyond the workplace, an intersectoral approach, national reach through policy advisory roles to Ministries of Health, policy development, and capacity building, was reported. Work conditions and context influenced conduciveness for innovation and the extent to which graduates were able to have effect. Self-selection of graduates and their role in selecting peers and supervisors may have resulted in some bias, some graduates could not be traced, and social acceptability bias may have influenced findings.\"\nQuestion:\n\"Impact of MPH programs: contributing to health system strengthening in low- and middle-income countries?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20813740": {
                "source": [
                    "\"48 cases of SbCC were analysed immunohistochemically using monoclonal \u03b2-catenin antibody and the results correlated with tumour size, histopathological differentiation, orbital invasion and pagetoid spread.\nCytoplasmic overexpression of \u03b2-catenin was seen in 66% cases of SbCC which correlated positively with tumour size, orbital invasion and pagetoid spread. This correlation was found to be significant in tumour size>2 cm (p = 0.242). Nuclear staining was not observed in any of the cases.\"\nQuestion:\n\"Does \u03b2-catenin have a role in pathogenesis of sebaceous cell carcinoma of the eyelid?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21801416": {
                "source": [
                    "\"It is now widely accepted that AMP-activated protein kinase (AMPK) is a critical regulator of energy homeostasis. Recently, it has been shown to regulate circadian clocks. In seasonal breeding species such as sheep, the circadian clock controls the secretion of an endogenous rhythm of melatonin and, as a consequence, is probably involved in the generation of seasonal rhythms of reproduction. Considering this, we identified the presence of the subunits of AMPK in different hypothalamic nuclei involved in the pre- and post-pineal pathways that control seasonality of reproduction in the ewe and we investigated if the intracerebroventricular (i.c.v.) injection of two activators of AMPK, metformin and AICAR, affected the circadian rhythm of melatonin in ewes that were housed in constant darkness. In parallel the secretion of insulin was monitored as a peripheral metabolic marker. We also investigated the effects of i.c.v. AICAR on the phosphorylation of AMPK and acetyl-CoA carboxylase (ACC), a downstream target of AMPK, in brain structures along the photoneuroendocrine pathway to the pineal gland.\nAll the subunits of AMPK that we studied were identified in all brain areas that were dissected but with some differences in their level of expression among structures. Metformin and AICAR both reduced (p<0.001 and p<0.01 respectively) the amplitude of the circadian rhythm of melatonin secretion independently of insulin secretion. The i.c.v. injection of AICAR only tended (p = 0.1) to increase the levels of phosphorylated AMPK in the paraventricular nucleus but significantly increased the levels of phosphorylated ACC in the paraventricular nucleus (p<0.001) and in the pineal gland (p<0.05).\"\nQuestion:\n\"The effect of an intracerebroventricular injection of metformin or AICAR on the plasma concentrations of melatonin in the ewe: potential involvement of AMPK?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15388567": {
                "source": [
                    "\"To examine the evidence base of sports medicine research and assess how relevant and applicable it is to everyday practice.\nOriginal research articles, short reports, and case reports published in four major sport and exercise medicine journals were studied and classified according to the main topic of study and type of subjects used.\nThe most common topic was sports science, and very few studies related to the treatment of injuries and medical conditions. The majority of published articles used healthy subjects sampled from the sedentary population, and few studies have been carried out on injured participants.\"\nQuestion:\n\"Are sports medicine journals relevant and applicable to practitioners and athletes?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26561905": {
                "source": [
                    "\"To compare the dose intensity and toxicity profiles for patients undergoing chemotherapy at the Townsville Cancer Centre (TCC), a tertiary cancer centre in northern Queensland, with those for patients treated in Mount Isa, supervised by the same medical oncologists via teleoncology.\nA quasi-experimental design comparing two patient groups.\nTCC and Mount Isa Hospital, which both operate under the auspices of the Townsville Teleoncology Network (TTN).\nEligible patients who received chemotherapy at TCC or Mt Isa Hospital between 1 May 2007 and 30 April 2012.\nTeleoncology model for managing cancer patients in rural towns.\nDose intensity (doses, number of cycles and lines of treatment) and toxicity rates (rate of serious side effects, hospital admissions and mortality).\nOver 5 years, 89 patients received a total of 626 cycles of various chemotherapy regimens in Mount Isa. During the same period, 117 patients who received a total of 799 cycles of chemotherapy at TCC were eligible for inclusion in the comparison group. There were no significant differences between the Mount Isa and TCC patients in most demographic characteristics, mean numbers of treatment cycles, dose intensities, proportions of side effects, and hospital admissions. There were no toxicity-related deaths in either group.\"\nQuestion:\n\"Do teleoncology models of care enable safe delivery of chemotherapy in rural towns?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18843057": {
                "source": [
                    "\"One of the problems with manual resuscitators is the difficulty in achieving accurate volume delivery. The volume delivered to the patient varies by the physical characteristics of the person and method. This study was designed to compare tidal volumes delivered by the squeezing method, physical characteristics and education and practice levels.\n114 individuals trained in basic life support and bag-valve-mask ventilation participated in this study. Individual characteristics were obtained by the observer and the education and practice level were described by the subjects. Ventilation was delivered with a manual resuscitator connected to a microspirometer and volumes were measured. Subjects completed three procedures: one-handed, two-handed and two-handed half-compression.\nThe mean (standard deviation) volumes for the one-handed method were 592.84 ml (SD 117.39), two-handed 644.24 ml (SD 144.7) and two-handed half-compression 458.31 ml (SD 120.91) (p<0.01). Tidal volume delivered by two hands was significantly greater than that delivered by one hand (r = 0.398, p<0.01). The physical aspects including hand size, volume and grip power had no correlation with the volume delivered. There were slight increases in tidal volume with education and practice, but correlation was weak (r = 0.213, r = 0.281, r = 0.131, p<0.01).\"\nQuestion:\n\"Can you deliver accurate tidal volume by manual resuscitator?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16776337": {
                "source": [
                    "\"A retrospective analysis of a contemporary series of patients with pituitary apoplexy was performed to ascertain whether the histopathological features influence the clinical presentation or the outcome.\nA retrospective analysis was performed in 59 patients treated for pituitary apoplexy at the University of Virginia Health System, Charlottesville, Virginia, or Groote Schuur Hospital, University of Cape Town, South Africa. The patients were divided into two groups according to the histological features of their disease: one group with infarction alone, comprising 22 patients; and the other with hemorrhagic infarction and/or frank hemorrhage, comprising 37 patients. The presenting symptoms, clinical features, endocrinological status, and outcome were compared between the two groups.\"\nQuestion:\n\"Pituitary apoplexy: do histological features influence the clinical presentation and outcome?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17051586": {
                "source": [
                    "\"Several studies have suggested a protective effect of folic acid (FA) on congenital heart anomalies. Down syndrome (DS) infants are known to have a high frequency of heart anomalies. Not all children with DS suffer from heart anomalies, which raises the question whether maternal factors might affect the risk of these anomalies. Our objectives were to investigate whether first-trimester FA use protects against heart anomalies among DS children.\nWomen with liveborn DS children participating in the Slone Epidemiology Center Birth Defects Study between 1976 and 1997 were included. We performed case-control analyses using DS, with heart anomalies as cases and DS, without heart anomalies as controls. Subanalyses were performed for defects that have been associated with FA in non-DS populations (conotruncal, ventricular septal [VSD]) and for those that are associated with DS (ostium secundum type atrial septal defects [ASD]and endocardial cushion defects [ECD]). Exposure was defined as the use of any FA-containing product for an average of at least 4 days per week during the first 12 weeks of pregnancy, whereas no exposure was defined as no use of FA in these 12 weeks.\nOf the 223 cases, 110 (49%) were exposed versus 84 (46%) of the 184 controls. After adjustment for possible confounders, no protective effect of FA was found on heart anomalies overall (OR 0.95, 95% CI: 0.61-1.47) nor separately for conotruncal defects, VSDs, ASDs, or ECDs.\"\nQuestion:\n\"Can folic acid protect against congenital heart defects in Down syndrome?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16253970": {
                "source": [
                    "\"Controlled ovarian stimulation (COS) with intrauterine insemination (IUI) is a common treatment in couples with unexplained non-conception. Induction of multifollicular growth is considered to improve pregnancy outcome, but it contains an increased risk of multiple pregnancies and ovarian hyperstimulation syndrome. In this study the impact of the number of follicles (>14 mm) on the ongoing pregnancy rate (PR) and multiple PR was evaluated in the first four treatment cycles.\nA retrospective cohort study was performed in all couples with unexplained non-conception undergoing COS-IUI in the Academic Hospital of Maastricht. The main outcome measure was ongoing PR. Secondary outcomes were ongoing multiple PR, number of follicles of>or=14 mm, and order of treatment cycle.\nThree hundred couples were included. No significant difference was found in ongoing PR between women with one, two, three or four follicles respectively (P=0.54), but in women with two or more follicles 12/73 pregnancies were multiples. Ongoing PR was highest in the first treatment cycle and declined significantly with increasing cycle order (P=0.006), while multiple PR did not change.\"\nQuestion:\n\"Is controlled ovarian stimulation in intrauterine insemination an acceptable therapy in couples with unexplained non-conception in the perspective of multiple pregnancies?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12947068": {
                "source": [
                    "\"Prior decision-analytic models are based on outdated or suboptimal efficacy, patient preference, and comorbidity data. We estimated life expectancy (LE) and quality-adjusted life expectancy (QALE) associated with available treatments for localized prostate cancer in men aged>/= 65 years, adjusting for Gleason score, patient preferences, and comorbidity.\nWe evaluated three treatments, using a decision-analytic Markov model: radical prostatectomy (RP), external beam radiotherapy (EBRT), and watchful waiting (WW). Rates of treatment complications and pretreatment incontinence and impotence were derived from published studies. We estimated treatment efficacy using three data sources: cancer registry cohort data, pooled case series, and modern radiotherapy studies. Utilities were obtained from 141 prostate cancer patients and from published studies.\nFor men with well-differentiated tumors and few comorbidities, potentially curative therapy (RP or EBRT) prolonged LE up to age 75 years but did not improve QALE at any age. For moderately differentiated cancers, potentially curative therapy resulted in LE and QALE gains up to age 75 years. For poorly differentiated disease, potentially curative therapy resulted in LE and QALE gains up to age 80 years. Benefits of potentially curative therapy were restricted to men with no worse than mild comorbidity. When cohort and pooled case series data were used, RP was preferred over EBRT in all groups but was comparable to modern radiotherapy.\"\nQuestion:\n\"Do older men benefit from curative therapy of localized prostate cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18570208": {
                "source": [
                    "\"To determine the association between fetal biometry in the first or early second trimester and severe macrosomia at delivery.\nThis case-control study included 30 term severely macrosomic neonates; 90 appropriate-for-gestational age (AGA) neonates served as controls. All pregnancies underwent nuchal translucency (NT) screening at 11-14 weeks' gestation. Pregnancies were dated by accurate last menstrual period consistent with crown-rump length (CRL) measurements at the time of screening, early pregnancy CRL or date of fertilization. The association between birth weight and the difference between the measured and the expected CRL at the time of NT screening was analyzed.\nThe difference between measured and expected CRL, expressed both in mm and in days of gestation, was statistically greater in the severely macrosomic neonates compared with controls (mean, 6.66 +/- 4.78 mm vs. 1.17 +/- 4.6 mm, P<0.0001 and 3 +/- 2.2 days vs. 0.5 +/- 2.3 days, P<0.0001, respectively). Furthermore, there were significant correlations between the extent of macrosomia and the discrepancy between expected and measured fetal size at the time of NT screening (r = 0.47, P<0.01 and r = 0.48, P<0.01, respectively).\"\nQuestion:\n\"Is severe macrosomia manifested at 11-14 weeks of gestation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19327500": {
                "source": [
                    "\"It remains controversial whether there is a gender difference in survival of patients with resected non-small cell lung cancer.\nWe retrospectively analyzed 2770 patients (1689 men and 1081 women) with non-small cell lung cancer who underwent pulmonary resection between 1995 and 2005 at the National Cancer Center Hospital, Tokyo. A gender difference in survival was studied in all patients, in those divided according to histology or pathologic stage, and in propensity-matched gender pairs.\nThere were no differences in background, such as preoperative pulmonary function, operation procedures, or operative mortality. The proportions of adenocarcinoma and pathologic stage I in women were greater than those in men (93.6% vs 61.7% and 71.4% vs 58.6%, respectively) (P<.001). Overall 5-year survival of women was better than that of men (81% vs 70%, P<.001). In adenocarcinoma, the overall 5-year survival for women was better than that for men in pathologic stage I (95% vs 87%, P<.001) and in pathologic stage II or higher (58% vs 51%, P = .017). In non-adenocarcinoma, there was no significant gender difference in survival in pathologic stage I (P = .313) or pathologic stage II or higher (P = .770). The variables such as age, smoking status, histology, and pathologic stage were used for propensity score matching, and survival analysis of propensity score-matched gender pairs did not show a significant difference (P = .69).\"\nQuestion:\n\"Gender difference in survival of resected non-small cell lung cancer: histology-related phenomenon?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26209118": {
                "source": [
                    "\"Children with sickle cell disease (SCD) are at risk of bone infarcts and acute osteomyelitis. The clinical differentiation between a bone infarct and acute osteomyelitis is a diagnostic challenge. Unenhanced T1-W fat-saturated MR images have been proposed as a potential tool to differentiate bone infarcts from osteomyelitis.\nTo evaluate the reliability of unenhanced T1-W fat-saturated MRI for differentiation between bone infarcts and acute osteomyelitis in children with SCD.\nWe retrospectively reviewed the records of 31 children (20 boys, 11 girls; mean age 10.6 years, range 1.1-17.9 years) with SCD and acute bone pain who underwent MR imaging including unenhanced T1-W fat-saturated images from 2005 to 2010. Complete clinical charts were reviewed by a pediatric hematologist with training in infectious diseases to determine a clinical standard to define the presence or absence of osteomyelitis. A pediatric radiologist reviewed all MR imaging and was blinded to clinical information. Based on the signal intensity in T1-W fat-saturated images, the children were further classified as positive for osteomyelitis (low bone marrow signal intensity) or positive for bone infarct (high bone marrow signal intensity).\nBased on the clinical standard, 5 children were classified as positive for osteomyelitis and 26 children as positive for bone infarct (negative for osteomyelitis). The bone marrow signal intensity on T1-W fat-saturated imaging was not significant for the differentiation between bone infarct and osteomyelitis (P\u2009=\u20090.56). None of the additional evaluated imaging parameters on unenhanced MRI proved reliable in differentiating these diagnoses.\"\nQuestion:\n\"Utility of unenhanced fat-suppressed T1-weighted MRI in children with sickle cell disease -- can it differentiate bone infarcts from acute osteomyelitis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18019905": {
                "source": [
                    "\"To illustrate how maternal mortality audit identifies different causes of and contributing factors to maternal deaths in different settings in low- and high-income countries and how this can lead to local solutions in reducing maternal deaths.\nDescriptive study of maternal mortality from different settings and review of data on the history of reducing maternal mortality in what are now high-income countries.\nKalabo district in Zambia, Farafenni division in The Gambia, Onandjokwe district in Namibia, and the Netherlands.\nPopulation of rural areas in Zambia and The Gambia, peri-urban population in Namibia and nationwide data from The Netherlands.\nData from facility-based maternal mortality audits from three African hospitals and data from the latest confidential enquiry in The Netherlands.\nMaternal mortality ratio (MMR), causes (direct and indirect) and characteristics.\nMMR ranged from 10 per 100,000 (the Netherlands) to 1540 per 100,000 (The Gambia). Differences in causes of deaths were characterized by HIV/AIDS in Namibia, sepsis and HIV/AIDS in Zambia, (pre-)eclampsia in the Netherlands and obstructed labour in The Gambia.\"\nQuestion:\n\"The use of audit to identify maternal mortality in different settings: is it just a difference between the rich and the poor?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16971978": {
                "source": [
                    "\"Coronary atherosclerotic burden is excessive in diabetic patients. Diabetes mellitus (DM) is an independent predictor for both death and myocardial infarction. It is not known whether the prevalence of complex coronary lesions, such as bifurcation and ostial lesions, is different in diabetics from nondiabetics.\nThe aim of present study was to investigate the prevalence of these lesions in patients with DM.\nOne thousand fourteen consecutive patients (mean age 61.3+/-10.7 years) were investigated. Coronary angiograms were examined for bifurcation and ostial lesions using a digital quantitative system. Patients were classified as diabetic (n=281) or nondiabetic (n=733).\nPatient mean age, and rates of hypertension and hyperlipidemia were significantly higher in the diabetic group than in the nondiabetic group (P<0.0001), although smoking was significantly lower (P=0.001). Reasons for coronary angiography and treatment were comparable between the two groups. The prevalence of bifurcation lesions and ostial lesions was significantly greater in the diabetic group than in the nondiabetic group (9.8% versus 4.3% [P=0.001] and 38.4% versus 29.2% [P=0.003]in the diabetic group versus the nondiabetic group). The presence of DM and greater age were found to be independent predictors for bifurcation lesions (OR=2.27 [P=0.004] and OR=1.03 [P=0.01], for DM and age, respectively) and ostial lesions (OR=1.40 [P=0.027] and OR=1.02 [P=0.001], for DM and age, respectively) in multivariate analysis.\"\nQuestion:\n\"Are complex coronary lesions more frequent in patients with diabetes mellitus?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12380309": {
                "source": [
                    "\"To evaluate prepuce development and retractibility in a group of boys. To point out the value of circumcision and prepucial forced dilation during childhood.\nPrepuce development and retractibility were evaluated in 400 boys ages between 0-16 year old.\nIn boys under 1 year prepuce retractibility (assessed only in children who did not undergo forced dilation previously) was type I (non retractile) in 71.5% whereas type V (completely retractile) was only 5.5%. In adolescent boys type I prepuce was observed in 1 boy only, 1.6%, whereas type V was observed in 82.3%. Furthermore, it was observed that at the time of examination for the study 106 boys who had undergone forced dilation at an earlier age had balano-prepucial adhesions again, which demonstrates that prepuce adheres again to glans penis in many boys after a forced dilation is performed. Only 11 boys were considered in need for circumcision, three of them for prepucial orifice stenosis, which prevented normal micturition, causing a prepucial sac, one case due to a constrictive ring below the prepucial edge that would have prevented ulterior retractability, two cases with repetitive balanopostitis, and five cases secondary to xerosol balanitis, accounting for 2.7% of all examined boys.\"\nQuestion:\n\"Should circumcision be performed in childhood?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12419743": {
                "source": [
                    "\"To determine whether patients with high-risk metastatic breast cancer draw benefit from combination chemotherapy as first-line treatment.\nA total of 260 women with measurable metastatic breast cancer fulfilling high-risk criteria, previously untreated with chemotherapy for their metastatic disease, were randomized to receive either mitoxantrone 12 mg/m(2) or the combination of fluorouracil 500 mg/m(2), epirubicin 50 mg/m(2) and cyclophosphamide 500 mg/m(2) (FEC) every 3 weeks. Treatment was continued until complete remission plus two cycles, or until disease progression. In the case of partial remission or stable disease, treatment was stopped after 12 cycles. Second-line treatment was vindesine, mitomycin and prednisolone. Gain from treatment was estimated using a modified Brunner's score composed of time to progression, patients' rating of the treatment benefit, alopecia, vomiting and performance status.\nAfter recruitment from 1992 to 1997 and observation from 1997 to 1999, the final evaluation showed that single-agent treatment with mitoxantrone does not differ significantly from combination treatment with FEC in terms of response, objective remission rate, remission duration, time to response, time to best response, time to progression or overall survival. There was, however, a significant difference in gain from treatment using a modified Brunner's score favoring the single-agent treatment arm. There was no evidence that any subgroup would fare better with combination treatment.\"\nQuestion:\n\"Is first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20736672": {
                "source": [
                    "\"To assess whether perspective-taking, which researchers in other fields have shown to induce empathy, improves patient satisfaction in encounters between student-clinicians and standardized patients (SPs).\nIn three studies, randomly assigned students (N = 608) received either a perspective-taking instruction or a neutral instruction prior to a clinical skills examination in 2006-2007. SP satisfaction was the main outcome in all three studies. Study 1 involved 245 third-year medical students from two universities. Studies 2 and 3 extended Study 1 to examine generalizability across student and SP subpopulations. Study 2 (105 physician assistant students, one university) explored the effect of perspective-taking on African American SPs' satisfaction. Study 3 (258 third-year medical students, two universities) examined the intervention's effect on students with high and low baseline perspective-taking tendencies.\nIntervention students outscored controls in patient satisfaction in all studies: Study 1: P = .01, standardized effect size = 0.16; Study 2: P = .001, standardized effect size = 0.31; Study 3: P = .009, standardized effect size = 0.13. In Study 2, perspective-taking improved African American SPs' satisfaction. In Study 3, intervention students with high baseline perspective-taking tendencies outscored controls (P = .0004, standardized effect size = 0.25), whereas those with low perspective-taking tendencies did not (P = .72, standardized effect size = 0.00).\"\nQuestion:\n\"Does perspective-taking increase patient satisfaction in medical encounters?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27078715": {
                "source": [
                    "\"Digital tomosynthesis (DT) is a new X-ray-based imaging technique that allows image enhancement with minimal increase in radiation exposure. The purpose of this study was to compare DT with noncontrast computed tomography (NCCT) and to evaluate its potential role for the follow-up of patients with nephrolithiasis in a nonemergent setting.\nA retrospective review of patients with nephrolithiasis at our institution that underwent NCCT and DT from July 2012 to September 2013 was performed. Renal units (RUs) that did not undergo treatment or stone passage were randomly assigned to two blinded readers, who recorded stone count, size area (mm(2)), maximum stone length (mm), and location, for both DT and NCCT. Mean differences per RU were compared. Potential variables affecting stone detection rate, including stone size and body mass index (BMI), were evaluated. Interobserver agreement was determined using the intraclass correlation coefficient to measure the consistency of measurements made by the readers.\nDT and NCCT demonstrated similar stone detection rates in terms of stone counts and stone area mm(2). Of the 79 RUs assessed, 41 RUs showed exact stone counts on DT and NCCT. The mean difference in stone area was 16.5\u2009mm(2) (-4.6 to 38.5), p\u2009=\u20090.121. The mean size of the largest stone on NCCT and DT was 9.27 and 8.87\u2009mm, respectively. Stone size and BMI did not cause a significant difference in stone detection rates. Interobserver agreement showed a strong correlation between readers and adequate reproducibility.\"\nQuestion:\n\"Digital Tomosynthesis: A Viable Alternative to Noncontrast Computed Tomography for the Follow-Up of Nephrolithiasis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21850494": {
                "source": [
                    "\"Hepatorenal syndrome (HRS) is the functional renal failure associated with advanced cirrhosis and has also been described in fulminant hepatic failure. Without liver transplantation its prognosis is dismal. Our study included patients with type 1 HRS associated with cirrhosis, who were not liver transplant candidates.AIM: To identify variables associated with improved survival.\nSixty-eight patients fulfilled the revised Ascites Club Criteria for type 1 HRS. None of them was suitable for liver transplantation. All the patients were treated with combinations of: albumin, midodrine and octreotide, pressors, and hemodialysis.\nMedian survival was 13 days for the whole group. Survival varied with the end-stage liver disease (ESLD) etiology: autoimmune, 49 days, cardiac cirrhosis, 22 days, idiopathic, 15.5 days, viral, 15 days, hepatitis C and alcohol, 14.5 days, alcohol 8 days, and neoplasia 4 days (p = 0.048). Survival of HRS associated with alcoholic liver disease versus other etiologies was not statistically significant (p = 0.1). Increased serum creatinine (p = 0.02) and urinary sodium 6-10 mEq/l (p = 0.027) at the initiation of therapy were prognostic factors for mortality. HRS treatment modalities (p = 0.73), use of dialysis (p = 0.56), dialysis modality (p = 0.35), use of vasopressors (p = 0.26), pre-existing renal disease (p = 0.49), gender (p = 0.90), and age (p = 0.57) were not associated with survival.\"\nQuestion:\n\"Hepatorenal syndrome: are we missing some prognostic factors?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16195477": {
                "source": [
                    "\"Obesity is associated with an increased risk for cardiovascular disease. Although it is known that white adipose tissue (WAT) produces numerous proinflammatory and proatherogenic cytokines and chemokines, it is unclear whether adipose-derived chemotactic signals affect the chronic inflammation in atherosclerosis.\nHistological examination showed that perivascular WAT (pWAT) is in close proximity to vascular walls, particularly at sites that have a tendency to develop atherosclerosis. In rodents, the amount of pWAT is markedly increased by a high-fat diet. At a functional level, supernatant from subcutaneous and pWAT strongly induced the chemotaxis of peripheral blood leukocytes. The migration of granulocytes and monocytes was mostly mediated by interleukin-8 and monocyte chemoattractant protein-1, respectively, whereas both chemokines contributed to the migration of activated T cells. Moreover, pWAT produces these chemokines, as shown by immunohistochemistry and by explant culture. The accumulation of macrophages and T cells at the interface between pWAT and the adventitia of human atherosclerotic aortas may reflect this prochemotactic activity of pWAT.\"\nQuestion:\n\"Production of chemokines by perivascular adipose tissue: a role in the pathogenesis of atherosclerosis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19664156": {
                "source": [
                    "\"Web search engines are an important tool in communication and diffusion of knowledge. Among these, Google appears to be the most popular one: in August 2008, it accounted for 87% of all web searches in the UK, compared with Yahoo's 3.3%. Google's value as a diagnostic guide in general medicine was recently reported. The aim of this comparative cross-sectional study was to evaluate whether searching Google with disease-related terms was effective in the identification and diagnosis of complex immunological and allergic cases.\nForty-five case reports were randomly selected by an independent observer from peer-reviewed medical journals. Clinical data were presented separately to three investigators, blinded to the final diagnoses. Investigator A was a Consultant with an expert knowledge in Internal Medicine and Allergy (IM&A) and basic computing skills. Investigator B was a Registrar in IM&A. Investigator C was a Research Nurse. Both Investigators B and C were familiar with computers and search engines. For every clinical case presented, each investigator independently carried out an Internet search using Google to provide a final diagnosis. Their results were then compared with the published diagnoses.\nCorrect diagnoses were provided in 30/45 (66%) cases, 39/45 (86%) cases, and in 29/45 (64%) cases by investigator A, B, and C, respectively. All of the three investigators achieved the correct diagnosis in 19 cases (42%), and all of them failed in two cases.\"\nQuestion:\n\"Search engine as a diagnostic tool in difficult immunological and allergologic cases: is Google useful?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15939071": {
                "source": [
                    "\"Since insulin therapy might have an atherogenic effect, we studied the relationship between cumulative insulin dose and atherosclerosis in type 1 diabetes. We have focused on patients with type 1 diabetes instead of type 2 diabetes to minimise the effect of insulin resistance as a potential confounder.\nAn observational study was performed in 215 subjects with type 1 diabetes treated with multiple insulin injection therapy. Atherosclerosis was assessed by measurement of carotid intima-media thickness (CIMT).\nThe cumulative dose of regular insulin showed a positive and significant relation with CIMT: increase of 21 microm in CIMT per S.D. of insulin use (95% CI: 8-35 adjusted for gender and age), which remained unchanged after adjustment for duration of diabetes, HbA1c, BMI, pulse pressure, physical activity and carotid lumen diameter. A similar relation was found for intermediate-acting insulin: 15.5 microm per S.D. (2-29), which was no longer present after further adjustment.\"\nQuestion:\n\"High cumulative insulin exposure: a risk factor of atherosclerosis in type 1 diabetes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25489696": {
                "source": [
                    "\"Radiotherapy reduces local recurrence rates but is also capable of short- and long-term toxicity. It may also render treatment of local recurrence more challenging if it develops despite previous radiotherapy.\nThis study examined the impact of radiotherapy for the primary rectal cancer on outcomes after pelvic exenteration for local recurrence.\nWe conducted a retrospective review of exenteration databases.\nThe study took place at a quaternary referral center that specializes in pelvic exenteration.\nPatients referred for pelvic exenteration from October 1994 to November 2012 were reviewed. Patients who did and did not receive radiotherapy as part of their primary rectal cancer treatment were compared.\nThe main outcomes of interest were resection margins, overall survival, disease-free survival, and surgical morbidities.\nThere were 108 patients, of which 87 were eligible for analysis. Patients who received radiotherapy for their primary rectal cancer (n = 41) required more radical exenterations (68% vs 44%; p = 0.020), had lower rates of clear resection margins (63% vs 87%; p = 0.010), had increased rates of surgical complications per patient (p = 0.014), and had a lower disease-free survival (p = 0.022). Overall survival and disease-free survival in patients with clear margins were also lower in the primary irradiated patients (p = 0.049 and p<0.0001). This difference in survival persisted in multivariate analysis that corrected for T and N stages of the primary tumor.\nThis study is limited by its retrospective nature and heterogeneous radiotherapy regimes among radiotherapy patients.\"\nQuestion:\n\"Does radiotherapy of the primary rectal cancer affect prognosis after pelvic exenteration for recurrent rectal cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11247896": {
                "source": [
                    "\"In familial adenomatous polyposis (FAP), correlations between site of mutation in the adenomatous polyposis coli (APC) gene and severity of colonic polyposis or extracolonic manifestations are well known. While mutation analysis is important for predictive diagnosis in persons at risk, its relevance for clinical management of individual patients is open to question.\nWe examined 680 unrelated FAP families for germline mutations in the APC gene. Clinical information was obtained from 1256 patients.\nAPC mutations were detected in 48% (327/680) of families. Age at diagnosis of FAP based on bowel symptoms and age at diagnosis of colorectal cancer in untreated patients were used as indicators of the severity of the natural course of the disease. A germline mutation was detected in 230 of 404 patients who were diagnosed after onset of bowel symptoms (rectal bleeding, abdominal pain, diarrhoea). When these patients were grouped according to the different sites of mutations, mean values for age at onset of disease differed significantly: patients carrying APC mutations at codon 1309 showed a disease onset 10 years earlier (mean age 20 years) compared with patients with mutations between codons 168 and 1580 (except codon 1309) (mean age 30 years), whereas patients with mutations at the 5' end of codon 168 or the 3' end of codon 1580 were diagnosed at a mean age of 52 years. Within each group of patients however large phenotypic variation was observed, even among patients with identical germline mutations. A higher incidence of desmoids was found in patients with mutations between codons 1445 and 1580 compared with mutations at other sites, while no correlation between site of mutation and presence of duodenal adenomas was observed.\"\nQuestion:\n\"Can APC mutation analysis contribute to therapeutic decisions in familial adenomatous polyposis?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27757987": {
                "source": [
                    "\"Recent studies have found a choroidal thickening in amblyopic eyes and suggested that there might be a relationship between the choroid and amblyopia. The present study aimed to evaluate the effect of a six-month treatment of amblyopia on choroidal thickness in anisometropic hyperopic amblyopic children.\nThirty-two anisometropic hyperopic children with unilateral amblyopia were included in this prospective study. Subfoveal choroidal thickness was measured as the distance between the retinal pigment epithelium and the chorioscleral edge, by using spectral domain enhanced depth imaging optical coherence tomography. The treatment of amblyopia was performed based on the full correction of the refractive error with eyeglasses, a refractive adaptation phase and occlusion by patching the fellow eye.\nThe mean visual acuity of the amblyopic eyes significantly increased from 0.35\u2009\u00b1\u20090.3 to 0.16\u2009\u00b1\u20090.2 logMAR after the treatment (p\u2009<\u20090.001). The mean initial choroidal thickness was significantly higher in the amblyopic eyes than in the fellow eyes (p\u2009=\u20090.019). There were no significant differences between the pre- and post-treatment mean choroidal thickness in the amblyopic eyes (p\u2009=\u20090.428) and in the fellow eyes (p\u2009=\u20090.343). The mean choroidal thickness was still higher in the amblyopic eyes than in the fellow eyes after the treatment (p\u2009=\u20090.006).\"\nQuestion:\n\"Does the treatment of amblyopia normalise subfoveal choroidal thickness in amblyopic children?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            }
        }
    }
}
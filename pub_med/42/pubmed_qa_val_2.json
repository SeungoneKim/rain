{
    "pubmed_qa": {
        "pubmed_qa": {
            "25499207": {
                "source": [
                    "\"Current evidence suggests that neck pain is negatively associated with health-related quality of life (HRQoL). However, these studies are cross-sectional and do not inform the association between neck pain and future HRQoL.\nThe purpose of this study was to investigate the association between increasing grades of neck pain severity and HRQoL 6 months later. In addition, this longitudinal study examines the crude association between the course of neck pain and HRQoL.\nThis is a population-based cohort study.\nEleven hundred randomly sampled Saskatchewan adults were included.\nOutcome measures were the mental component summary (MCS) and physical component summary (PCS) of the Short-Form-36 (SF-36) questionnaire.\nWe formed a cohort of 1,100 randomly sampled Saskatchewan adults in September 1995. We used the Chronic Pain Questionnaire to measure neck pain and its related disability. The SF-36 questionnaire was used to measure physical and mental HRQoL 6 months later. Multivariable linear regression was used to measure the association between graded neck pain and HRQoL while controlling for confounding. Analysis of variance and t tests were used to measure the crude association among four possible courses of neck pain and HRQoL at 6 months. The neck pain trajectories over 6 months were no or mild neck pain, improving neck pain, worsening neck pain, and persistent neck pain. Finally, analysis of variance was used to examine changes in baseline to 6-month PCS and MCS scores among the four neck pain trajectory groups.\nThe 6-month follow-up rate was 74.9%. We found an exposure-response relationship between neck pain and physical HRQoL after adjusting for age, education, arthritis, low back pain, and depressive symptomatology. Compared with participants without neck pain at baseline, those with mild (\u03b2=-1.53, 95% confidence interval [CI]=-2.83, -0.24), intense (\u03b2=-3.60, 95% CI=-5.76, -1.44), or disabling (\u03b2=-8.55, 95% CI=-11.68, -5.42) neck pain had worse physical HRQoL 6 months later. We did not find an association between neck pain and mental HRQoL. A worsening course of neck pain and persistent neck pain were associated with worse physical HRQoL.\"\nQuestion:\n\"Is neck pain associated with worse health-related quality of life 6 months later?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26452334": {
                "source": [
                    "\"The aim of this study was to assess the reproducibility of different measurement methods and define the most workable technique for measuring head and neck paragangliomas, to determine the best method for evaluating tumour growth. The evaluation of tumour growth is vital for a 'wait-and-scan' policy, a management strategy that became increasingly important.\nMethod comparison study.\nThirty tumours, including carotid body, vagal body, jugulotympanic tumours and conglomerates of multiple tumours, were measured in duplicate, using linear dimensions, manual area tracing and an automated segmentation method.\nReproducibility was assessed using the Bland-Altman method.\nThe smallest detectable difference using the linear dimension method was 11% for carotid body and 27% for vagal body tumours, compared with 17% and 20% for the manual area tracing method. Due to the irregular shape of paragangliomas in the temporal bone and conglomerates, the manual area tracing method showed better results in these tumours (26% and 8% versus 54% and 47%). The linear dimension method was significantly faster (median 4.27 versus 18.46 minutes, P<0.001). The automatic segmentation method yielded smallest detectable differences between 39% and 75%, and although fast (2.19 \u00b1 1.49 minutes), it failed technically.\"\nQuestion:\n\"Measurement of head and neck paragangliomas: is volumetric analysis worth the effort?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14631523": {
                "source": [
                    "\"The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading.\nChildren aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems.\nThe overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.\"\nQuestion:\n\"Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8916748": {
                "source": [
                    "\"To assess the risk of death associated with work based and non-work based measures of socioeconomic status before and after retirement age.\nFollow up study of mortality in relation to employment grade and car ownership over 25 years.\nThe first Whitehall study.\n18,133 male civil servants aged 40-69 years who attended a screening examination between 1967 and 1970.\nDeath.\nGrade of employment was a strong predictor of mortality before retirement. For men dying at ages 40-64 the lowest employment grade had 3.12 times the mortality of the highest grade (95% confidence interval 2.4 to 4.1). After retirement the ability of grade to predict mortality declined (rate ratio 1.86; 1.6 to 2.2). A non-work based measure of socioeconomic status (car ownership) predicted mortality less well than employment grade before retirement but its ability to predict mortality declined less after retirement. Using a relative index of inequality that was sensitive to the distribution among socioeconomic groups showed employment grade and car ownership to have independent associations with mortality that were of equal magnitude after retirement. The absolute difference in death rates between the lowest and highest employment grades increased with age from 12.9 per 1000 person years at ages 40-64 to 38.3 per 1000 at ages 70-89.\"\nQuestion:\n\"Do socioeconomic differences in mortality persist after retirement?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23048048": {
                "source": [
                    "\"This study examined the extent to which ADHD was associated with risky sexual behaviors (RSBs) in a sample of 92 undergraduates with (n = 44) and without (n = 48) ADHD. Mother-child relationship quality was examined as a potential moderator.\nWe conducted comprehensive assessments for ADHD and comorbid conditions and collected measures of RSB and mother-child relationship quality.\nFemale students with ADHD were least likely to use condoms than males overall and females without ADHD. An interaction between ADHD and mother-child relationship quality accounted for significant variance in the number of past-year sexual partners, such that a high-quality relationship was protective only for students with ADHD. No other significant associations were found between ADHD and RSB.\"\nQuestion:\n\"Risky sexual behavior among college students With ADHD: is the mother-child relationship protective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12607666": {
                "source": [
                    "\"The aim of this study was to evaluate the effectiveness of our surgical strategy for acute aortic dissection based on the extent of the dissection and the site of the entry, with special emphasis on resection of all dissected aortic segments if technically possible.\nBetween January 1995 and March 2001, 43 consecutive patients underwent operations for acute aortic dissection. In all patients the distal repair was performed under circulatory arrest without the use of an aortic cross-clamp. Fifteen patients underwent aortic arch replacement with additional reconstruction of supra-aortic vessels in 3 patients. Complete replacement of all dissected tissue could be achieved in 21 patients (group 1). Because of the distal extent of the dissection beyond the aortic arch, replacement of all the dissected tissue was not possible in 22 patients (group 2).\nEarly mortality was 4.7% (2 patients), and the incidence of perioperative cerebrovascular events was 7.0% (3 patients). All of these events occurred in group 2 (p<0.025). During the follow-up period of 6 years or less, 5 patients died, all from causes not related to the aorta or the aortic valve. A persisting patent false lumen was observed in 14 of the 36 surviving patients (39%).\"\nQuestion:\n\"Is extended aortic replacement in acute type A dissection justifiable?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26708803": {
                "source": [
                    "\"Treatment of neonatal testicular torsion has two objectives: salvage of the involved testicle (which is rarely achieved) and preservation of the contralateral gonad. The second goal universally involves contralateral testicular scrotal fixation to prevent the future occurrence of contralateral torsion. However, there is controversy with regards to management of a synchronous contralateral hydrocele. It has been our policy not to address the contralateral hydrocele through an inguinal incision to minimize potential injury to the spermatic cord. Our objective in this study was to determine whether the decision to manage a contralateral hydrocele in cases of neonatal testicular torsion solely through a scrotal approach is safe and effective.\nWe reviewed all cases of neonatal testicular torsion occurring at our institution between the years 1999 and 2006. Age at presentation, physical examination, ultrasonographic and intraoperative findings were recorded. Patients were followed after initial surgical intervention to determine the likelihood of developing a subsequent hydrocele or hernia.\nThirty-seven patients were identified as presenting with neonatal torsion. Age of presentation averaged 3.5 days (range 1-14 days). Left-sided pathology was seen more commonly than the right, with a 25:12 distribution. All torsed testicles were nonviable. Twenty-two patients were noted to have a contralateral hydrocele at presentation. All hydroceles were opened through a scrotal approach at the time of contralateral scrotal fixation. No patient underwent an inguinal exploration to examine for a patent process vaginalis. None of the patients who presented with a hydrocele have developed a clinical hydrocele or hernia after an average 7.5 years (range 4.3-11.2) follow-up.\"\nQuestion:\n\"Treatment of contralateral hydrocele in neonatal testicular torsion: Is less more?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23361217": {
                "source": [
                    "\"There are a number of factors responsible for the longevity of unicompartmental knee replacements (UKR). These include the magnitude of postoperative alignment and the type of material used. The effect of component design and material on postoperative alignment, however, has not been explored.\nWe retrospectively reviewed 89 patients who underwent UKR with robotic guidance. Patients were divided into two groups, according to whether they had received an all-polyethylene inlay component (Inlay group) or a metal-backed onlay component (Onlay group). We explored the magnitude of mechanical alignment correction obtained in both groups.\nMean postoperative mechanical alignment was significantly closer to neutral in the Onlay group (mean=2.8\u00b0; 95% CI=2.4\u00b0, 3.2\u00b0) compared to the Inlay group (mean=3.9\u00b0; 95% CI=3.4\u00b0, 4.4\u00b0) (R2=0.65; P=0.003), adjusting for gender, BMI, age, side and preoperative mechanical alignment (Fig. 2). Further exploration revealed that the thickness of the tibial polyethyelene insert had a significant effect on postoperative alignment when added to the model (R2=0.68; P=0.01).\"\nQuestion:\n\"Does the type of tibial component affect mechanical alignment in unicompartmental knee replacement?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19130332": {
                "source": [
                    "\"Uncontrolled hemorrhage is the leading cause of fatality. The aim of this study was to evaluate the effect of zeolite mineral (QuikClot - Advanced Clotting Sponge [QC-ACS]) on blood loss and physiological variables in a swine extremity arterial injury model.\nSixteen swine were used. Oblique groin incision was created and a 5 mm incision was made. The animals were allocated to: control group (n: 6): Pressure dressing was applied with manual pressure over gauze sponge; or QC group (n: 10): QC was directly applied over lacerated femoral artery. Mean arterial pressure, blood loss and physiological parameters were measured during the study period.\nApplication of QC led to a slower drop in blood pressure. The control group had a significantly higher increase in lactate within 60 minutes. The mean prothrombin time in the control group was significantly increased at 60 minutes. The application of QC led to decreased total blood loss. The QC group had significantly higher hematocrit levels. QC application generated a significant heat production. There were mild edematous and vacuolar changes in nerve samples.\"\nQuestion:\n\"Is the zeolite hemostatic agent beneficial in reducing blood loss during arterial injury?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25604390": {
                "source": [
                    "\"Dickkopf-3 (DKK3) may act as a tumor suppressor as it is down-regulated in various types of cancer. This study assessed the DKK3 protein expression in gastric cancer and its potential value as a prognostic marker.\nDKK3 expression was evaluated by immunohistochemistry in 158 gastric cancer samples from patients who underwent gastrectomy from 2002 to 2008. Clinicopathological parameters and survival data were analyzed.\nLoss of DKK3 expression was found in 64 of 158 (40.5%) samples, and it was associated with advanced T stage (p<0.001), lymph node metastasis (p<0.001), UICC TNM stage (p<0.001), tumor location (p = 0.029), lymphovascular invasion (p = 0.035), and perineural invasion (p = 0.032). Patients without DKK3 expression in tumor cells had a significantly worse disease-free and overall survival than those with DKK3 expression (p<0.001, and p = 0.001, respectively). TNM stage (p = 0.028 and p<0.001, respectively) and residual tumor (p<0.001 and p = 0.003, respectively) were independent predictors of disease-free and overall survival. Based on the preoperative clinical stage assessed by computed tomography (CT), loss of DKK3 expression was predominantly associated with worse prognosis in patients with clinically node-negative advanced gastric cancer (AGC). The combination of DKK3 expression status and CT increased the accuracy of CT staging for predicting lymph node involvement from 71.5 to 80.0% in AGC patients.\"\nQuestion:\n\"Aberrant loss of dickkopf-3 in gastric cancer: can it predict lymph node metastasis preoperatively?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16296668": {
                "source": [
                    "\"To investigate the ability of a bedside swallowing assessment to reliably exclude aspiration following acute stroke.\nConsecutive patients admitted within 24 h of stroke onset to two hospitals.\nA prospective study. Where possible, all patients had their ability to swallow assessed on the day of admission by both a doctor and a speech and language therapist using a standardized proforma. A videofluoroscopy examination was conducted within 3 days of admission.\n94 patients underwent videofluoroscopy; 20 (21%) were seen to be aspirating, although this was not detected at the bedside in 10. In 18 (22%) of the patients the speech and language therapist considered the swallow to be unsafe. In the medical assessment, 39 patients (41%) had an unsafe swallow. Bedside assessment by a speech and language therapist gave a sensitivity of 47%, a specificity of 86%, positive predictive value (PPV) of 50% and a negative predictive value (NPV) of 85% for the presence of aspiration. Multiple logistic regression was used to identify the optimum elements of the bedside assessments for predicting the presence of aspiration. A weak voluntary cough and any alteration in conscious level gave a sensitivity of 75%, specificity of 72%, PPV of 41% and NPV of 91% for aspiration.\"\nQuestion:\n\"Can bedside assessment reliably exclude aspiration following acute stroke?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21084567": {
                "source": [
                    "\"Home blood pressure (BP) monitoring is gaining increasing popularity among patients and may be useful in hypertension management. Little is known about the reliability of stroke patients' records of home BP monitoring.\nTo assess the reliability of home BP recording in hypertensive patients who had suffered a recent stroke or transient ischaemic attack.\nThirty-nine stroke patients (mean age 73 years) randomized to the intervention arm of a trial of home BP monitoring were included. Following instruction by a research nurse, patients recorded their BPs at home and documented them in a booklet over the next year. The booklet readings over a month were compared with the actual readings downloaded from the BP monitor and were checked for errors or selective bias in recording.\nA total of 1027 monitor and 716 booklet readings were recorded. Ninety per cent of booklet recordings were exactly the same as the BP monitor readings. Average booklet readings were 0.6 mmHg systolic [95% confidence interval (95% CI) -0.6 to 1.8] and 0.3 mmHg diastolic (95% CI -0.3 to 0.8) lower than those on the monitor.\"\nQuestion:\n\"Are stroke patients' reports of home blood pressure readings reliable?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21558951": {
                "source": [
                    "\"To ascertain whether level of intrauterine cocaine exposure (IUCE) is associated with early adolescent delinquent behavior, after accounting for prenatal exposures to other psychoactive substances and relevant psychosocial factors.\nNinety-three early adolescents (12.5-14.5 years old) participating since birth in a longitudinal study of IUCE reported delinquent acts via an audio computer-assisted self-interview. Level of IUCE and exposure to cigarettes, alcohol, and marijuana were determined by maternal report, maternal and infant urine assays, and infant meconium assays at birth. Participants reported their exposure to violence on the Violence Exposure Scale for Children-Revised at ages 8.5, 9.5, and 11 years and during early adolescence, and the strictness of supervision by their caregivers during early adolescence.\nOf the 93 participants, 24 (26%) reported \u2265 3 delinquent behaviors during early adolescence. In the final multivariate model (including level of IUCE and cigarette exposure, childhood exposure to violence, and caregiver strictness/supervision) \u2265 3 delinquent behaviors were not significantly associated with level of IUCE but were significantly associated with intrauterine exposure to half a pack or more of cigarettes per day and higher levels of childhood exposure to violence, effects substantially unchanged after control for early adolescent violence exposure.\"\nQuestion:\n\"Are there effects of intrauterine cocaine exposure on delinquency during early adolescence?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25034219": {
                "source": [
                    "\"Obese children and adolescents referred to the pediatric endocrinology department were enrolled consecutively. Height and weight of all children and their mothers were measured. Maternal feeding practices were measured using an adapted version of the Child Feeding Questionnaire (CFQ). Answers were compared between obese (Body Mass Index [BMI] \u2265 30 kg/m2) and non-obese mothers.\nA total of 491 obese subjects (292 girls, mean age 12.0 \u00b1 2.8 years) and their mothers participated in this study. A direct correlation between children's BMI and their mothers' BMI was found (P<0.001) both in girls (r = 0.372) and boys (r = 0.337). While 64.4% of mothers were found obese in the study, only half of them consider themselves as obese. No difference were found in the scores of the subscales \"perceived responsibility\", \"restriction\", \"concern for child's weight\" and \"monitoring\" between obese and non-obese mothers. Child's BMI-SDS positively correlated with mothers' personal weight perception, concern for child's weight and restriction after adjustment for child's age (P<0.001, P = 0.012 and P = 0.002, respectively).\"\nQuestion:\n\"Does maternal obesity have an influence on feeding behavior of obese children?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26701174": {
                "source": [
                    "\"To ascertain whether hospital type is associated with differences in total cost and outcomes for inpatient tonsillectomy.\nCross-sectional analysis of the 2006, 2009, and 2012 Kids' Inpatient Database (KID).\nChildren \u226418 years of age undergoing tonsillectomy with/without adenoidectomy were included. Risk-adjusted generalized linear models assessed for differences in hospital cost and length of stay (LOS) among children managed by (1) non-children's teaching hospitals (NCTHs), (2) children's teaching hospitals (CTHs), and (3) nonteaching hospitals (NTHs). Risk-adjusted logistic regression compared the odds of major perioperative complications (hemorrhage, respiratory failure, death). Models accounted for clustering of patients within hospitals, were weighted to provide national estimates, and controlled for comorbidities.\nThe 25,685 tonsillectomies recorded in the KID yielded a national estimate of 40,591 inpatient tonsillectomies performed in 2006, 2009, and 2012. The CTHs had significantly higher risk-adjusted total cost and LOS per tonsillectomy compared with NCTHs and NTHs ($9423.34/2.8 days, $6250.78/2.11 days, and $5905.10/2.08 days, respectively; P<.001). The CTHs had higher odds of complications compared with NCTHs (odds ratio [OR], 1.48; 95% CI, 1.15-1.91; P = .002) but not when compared with NTHs (OR, 1.19; 95% CI, 0.89-1.59; P = .23). The CTHs were significantly more likely to care for patients with comorbidities (P<.001).\"\nQuestion:\n\"Inpatient Pediatric Tonsillectomy: Does Hospital Type Affect Cost and Outcomes of Care?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10135926": {
                "source": [
                    "\"Patients transported by helicopter often require advanced airway management. The purpose of this study was to determine whether or not the in-flight environment of air medical transport in a BO-105 helicopter impairs the ability of flight nurses to perform oral endotracheal intubation.\nThe study was conducted in an MBB BO-105 helicopter.\nFlight nurses performed three manikin intubations in each of the two study environments: on an emergency department stretcher and in-flight in the BO-105 helicopter.\nThe mean time required for in-flight intubation (25.9 +/- 10.9 seconds) was significantly longer than the corresponding time (13.2 +/- 2.8 seconds) required for intubation in the control setting (ANOVA, F = 38.7, p<.001). All intubations performed in the control setting were placed correctly in the trachea; there were two (6.7%) esophageal intubations in the in-flight setting. The difference in appropriate endotracheal intubation between the two settings was not significant (chi 2 = 0.3; p>0.05).\"\nQuestion:\n\"Is oral endotracheal intubation efficacy impaired in the helicopter environment?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21880023": {
                "source": [
                    "\"To study whether exercise during pregnancy reduces the risk of postnatal depression.\nRandomized controlled trial.\nTrondheim and Stavanger University Hospitals, Norway.\nEight hundred and fifty-five pregnant women were randomized to intervention or control groups.\nThe intervention was a 12 week exercise program, including aerobic and strengthening exercises, conducted between week 20 and 36 of pregnancy. One weekly group session was led by physiotherapists, and home exercises were encouraged twice a week. Control women received regular antenatal care.\nEdinburgh Postnatal Depression Scale (EPDS) completed three months after birth. Scores of 10 or more and 13 or more suggested probable minor and major depression, respectively.\nFourteen of 379 (3.7%) women in the intervention group and 17 of 340 (5.0%) in the control group had an EPDS score of \u226510 (p=0.46), and four of 379 (1.2%) women in the intervention group and eight of 340 (2.4%) in the control group had an EPDS score of \u226513 (p=0.25). Among women who did not exercise prior to pregnancy, two of 100 (2.0%) women in the intervention group and nine of 95 (9.5%) in the control group had an EPDS score of \u226510 (p=0.03).\"\nQuestion:\n\"Does exercise during pregnancy prevent postnatal depression?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26133538": {
                "source": [
                    "\"Abdominal bloating is reported by a majority of irritable bowel syndrome (IBS) patients. Excess colonic fermentation may cause gaseous symptoms. Several foodstuffs contain oligosaccharides with an \u03b1-galactosidic linkage that is resistant to mammalian hydrolases. Assisted hydrolysis by exogenous \u03b1-galactosidase enzyme (AG) could offer a way of controlling IBS symptoms by reducing colonic fermentation and gas production. The aim of this study was to assess the effect of AG on symptom severity and quality of life in IBS patients with abdominal bloating or flatulence.\nA total of 125 subjects with IBS received AG or placebo at meals for 12 weeks. IBS-Symptom Severity Score (IBS-SSS) and quality of life (QoL) were assessed at baseline, during the treatment and at 4-week follow-up.\nAG showed a trend toward a more prominent decrease in IBS-SSS. The responder rate at week 16 was higher for the AG group. No difference was detected in QoL between AG and placebo groups. A total of 25 patients (18 in AG group and 7 in placebo group, p = 0.016) withdrew from the study. Abdominal pain and diarrhea were more often reported as reason for withdrawal in AG group.\"\nQuestion:\n\"Does oral \u03b1-galactosidase relieve irritable bowel symptoms?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25699562": {
                "source": [
                    "\"Our previous work demonstrated that the Transmissible Liability Index (TLI), an instrument designed as an index of liability for substance use disorder (SUD), is associated with risk of substance use disorder. This longitudinal study assessed whether TLI measured in 10-12-year-olds (late childhood) predicts suicidal behavior from age 12-14 (preadolescence) to age 25 (young adulthood). We hypothesized that TLI would predict number and severity of suicide attempts.\nSubjects were sons of men who had lifetime history of SUD (n\u2009=\u2009250), called the High Average Risk (HAR) group, and sons of men with no lifetime history of a SUD (n\u2009=\u2009250), called the Low Average Risk (LAR) group. The TLI was delineated at baseline (age 10-12), and age-specific versions were administered at 12-14, 16, 19, 22, and 25 years of age.\nTLI was significantly associated with number and severity of lifetime suicide attempts.\"\nQuestion:\n\"Does the Transmissible Liability Index (TLI) assessed in late childhood predict suicidal symptoms at young adulthood?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18714572": {
                "source": [
                    "\"Vaginal intraepithelial neoplasia is a little known disease which could be related to risk factors different from simple HPV infections.\nTo ascertain wheter vaginal lesions have a natural history similar to cervical lesions.MATERIALS &\nA retrospective study to identify patients with vaginal lesions and synchronous cervical lesions through biopsy. The rate of mild cervical lesions (koilocytosis, warts, CIN I with and without koilocytosis) was compared with the rate of severe cervical lesions (CIN II and III, cervical carcinoma) in patients with mild vaginal lesions (warts and koilocytosis, and low-grade VAIN) and in patients with severe vaginal lesions (high-grade VAIN). Using koilocytosis as a marker, the rate of \"active\" cervical lesions was compared with the rate of \"non active\" cervical lesions in patients with \"active\" versus \"non active\" vaginal lesions. Finally, the rates of mild and severe cervical lesions were compared among each group of VAIN (low-grade, high-grade, with or without koilocytosis).\nIn patients with mild vaginal lesions, mild cervical lesions were significantly more frequent than severe cervical lesions. In patients with \"active\" vaginal lesions the rate of \"active\" cervical lesions was significantly higher than \"non active\" cervical lesions. The differences in rates of mild cervical lesions and severe cervical lesions among patients with high-grade VAIN and low-grade VAIN (with and without koilocytosis) were not significant.\"\nQuestion:\n\"Does vaginal intraepithelial neoplasia have the same evolution as cervical intraepithelial neoplasia?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22513023": {
                "source": [
                    "\"To assess whether Indigenous Australians age prematurely compared with other Australians, as implied by Australian Government aged care policy, which uses age 50 years and over for population-based planning for Indigenous people compared with 70 years for non-indigenous people.\nCross-sectional analysis of aged care assessment, hospital and health survey data comparing Indigenous and non-indigenous age-specific prevalence of health conditions. Analysis of life tables for Indigenous and non-indigenous populations comparing life expectancy at different ages.\nAt age 63 for women and age 65 for men, Indigenous people had the same life expectancy as non-indigenous people at age 70. There is no consistent pattern of a 20-year lead in age-specific prevalence of age-associated conditions for Indigenous compared with other Australians. There is high prevalence from middle-age onwards of some conditions, particularly diabetes (type unspecified), but there is little or no lead for others.\"\nQuestion:\n\"Do Indigenous Australians age prematurely?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21459725": {
                "source": [
                    "\"Xanthogranulomatous cholecystitis (XGC) is an uncommon variant of chronic cholecystitis, characterized by marked thickening of the gallbladder wall and dense local adhesions. It often mimics a gallbladder carcinoma (GBC), and may coexist with GBC, leading to a diagnostic dilemma. Furthermore, the premalignant nature of this entity is not known. This study was undertaken to assess the p53, PCNA and beta-catenin expression in XGC in comparison to GBC and chronic inflammation.\nSections from paraffin-embedded blocks of surgically resected specimens of GBC (69 cases), XGC (65), chronic cholecystitis (18) and control gallbladder (10) were stained with the monoclonal antibodies to p53 and PCNA, and a polyclonal antibody to beta-catenin. p53 expression was scored as the percentage of nuclei stained. PCNA expression was scored as the product of the percentage of nuclei stained and the intensity of the staining (1-3). A cut-off value of 80 for this score was taken as a positive result. Beta-catenin expression was scored as type of expression-membranous, cytoplasmic or nuclear staining.\np53 mutation was positive in 52% of GBC cases and 3% of XGC, but was not expressed in chronic cholecystitis and control gallbladders. p53 expression was lower in XGC than in GBC (P<0.0001). PCNA expression was seen in 65% of GBC cases and 11% of XGC, but not in chronic cholecystitis and control gallbladders. PCNA expression was higher in GBC than XGC (P=0.0001), but there was no significant difference between the XGC, chronic cholecystitis and control gallbladder groups. Beta-catenin expression was positive in the GBC, XGC, chronic cholecystitis and control gallbladder groups. But the expression pattern in XGC, chronic cholecystitis and control gallbladders was homogenously membranous, whereas in GBC the membranous expression pattern was altered to cytoplasmic and nuclear.\"\nQuestion:\n\"Xanthogranulomatous cholecystitis: a premalignant condition?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19302863": {
                "source": [
                    "\"The present study aims to compare strength, healing, and operation time of experimental intestinal anastomoses performed by polyglactin 910 (Vicryl; Ethicon, Edinburgh, United Kingdom) sutures with ethyl-2-cyanoacrylate glue (Pattex; Henkel, Dusseldorf, Germany).\nNinety-six Sprague-Dawley rats were divided into 2 (groups E and L). Each group was further subdivided into 6 subgroups (EA1, EA2, EA3, EB1, EB2, EB3, LA1, LA2, LA3, LB1, LB2, LB3), each containing 8 rats. Intestinal anastomosis was performed by polyglactin 910 sutures in A subgroups and with ethyl-2-cyanoacrylate in B subgroups. The anastomosis was end to end in A1 and B1, side to side in A2 and B2, and end to side in A3 and B3. Time for anastomosis performance (AT) was recorded. In group E, bursting pressures and hydroxyproline levels were determined on the second postoperative day, whereas in group L, the same measurements were made on the sixth postoperative day. One-way analysis of variance was used for analyses of variance in the groups. Quantitative data were analyzed with Student's t test. P value was considered significant at less than .05.\nThere was no significant difference between bursting pressures of subgroup pairs on both postoperative days 2 and 6. Hydroxyproline levels and AT were significantly better in B subgroups.\"\nQuestion:\n\"Is the use of cyanoacrylate in intestinal anastomosis a good and reliable alternative?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24434052": {
                "source": [
                    "\"The last 20 years has seen a marked improvement in skin cancer awareness campaigns. We sought to establish whether this has affected the presenting Breslow thickness of malignant melanoma in the South West.\nThis is a retrospective study looking at the first presentation of melanomas from 2003 to 2011. Data was accessed using the local online melanoma database.\nA total of 2001 new melanomas presented from 2003 to 2012 (Male:Female = 1:1.062). The average yearly number of melanomas was 200.1 (range = 138-312). The mean age was 62.5 years (range 12-99). Data was analysed using a Chi\u00b2 test. For 0-1 mm melanomas, there is a significant difference in the observed versus expected values over the 10 years (p = 0.0018). There is an increasing proportion of 0-1 mm (thin) melanomas presenting year on year, with a positive linear trend. This is very statistically significant (p<0.0001). The 1-2 mm melanomas are decreasing in proportion with a negative linear trend (p = 0.0013). The 2-4 mm are also decreasing in proportion (p = 0.0253). There is no significant change in the thick>4 mm melanomas (p = 0.1456).\"\nQuestion:\n\"Are we seeing the effects of public awareness campaigns?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19103915": {
                "source": [
                    "\"There is an urgent need to increase opportunistic screening for sexually transmitted infections (STIs) in community settings, particularly for those who are at increased risk including men who have sex with men (MSM). The aim of this qualitative study was to explore whether home sampling kits (HSK) for multiple bacterial STIs are potentially acceptable among MSM and to identify any concerns regarding their use. This study was developed as part of a formative evaluation of HSKs.\nFocus groups and one-to-one semi-structured interviews with MSM were conducted. Focus group participants (n = 20) were shown a variety of self-sampling materials and asked to discuss them. Individual interviewees (n = 24) had experience of the self-sampling techniques as part of a pilot clinical study. All data were digitally recorded and transcribed verbatim. Data were analysed using a framework analysis approach.\nThe concept of a HSK was generally viewed as positive, with many benefits identified relating to increased access to testing, enhanced personal comfort and empowerment. Concerns about the accuracy of the test, delays in receiving the results, the possible lack of support and potential negative impact on 'others' were raised.\"\nQuestion:\n\"Are home sampling kits for sexually transmitted infections acceptable among men who have sex with men?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22211919": {
                "source": [
                    "\"Urine samples were examined by wet smear microscopy, incubated in 5% CO(2) for 1-2 days, and species-specific real-time polymerase chain reaction (PCR) for A. schaalii was performed.\nIn 5 of the 29 screened urines, A. schaalii was found only by real-time PCR in quantities equivalent to \u2265 10(4) -10(5) CFU/mL. In addition, A. schaalii was found in quantities equivalent to \u2265 10(6) CFU/mL by both culture and PCR in two children with a urinary tract infection and large numbers of leucocytes in the urine.\"\nQuestion:\n\"Actinobaculum schaalii, a cause of urinary tract infections in children?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10757151": {
                "source": [
                    "\"Ischemic preconditioning (IP) is initiated through one or several short bouts of ischemia and reperfusion which precede a prolonged ischemia. To test whether a reperfusion must precede the prolonged index ischemia, a series without reperfusion (intraischemic preconditioning: IIP) and a series with gradual onset of ischemia, i.e. ramp ischemia (RI), which is possibly related to the development of hibernation, was compared to conventional IP (CIP).\nExperiments were performed an 27 blood-perfused rabbit hearts (Langendorff apparatus) that were randomized into one of four series: (1) control (n = 7): 60 min normal flow - 60 min low flow (10%) ischemia - 60 min reperfusion. (2) CIP (n = 7): 4 times 5 min zero flow with 10 min reperfusion each - 60 min low flow (10%) - ischemia 60 min reperfusion. (3) IIP (n = 7): 50 min normal flow - 10 min no flow - 60min low flow (10%) ischemia -4 60min reperfusion. (4) RI (n=6): gradual reduction to 10% flow during 60min - 60min low flow (10%) ischemia - 60min reperfusion. At the end of each protocol, the infarcted area was assessed.\nThe infarct area in control hearts was 6.7+/-1.4% (means+/-SEM) of LV total area, in CIP hearts 2.6+/-0.8%, in IIP hearts 3.1+/-0.5%, and in RI hearts 3.0+/-0.3% (all p<0.05 vs. control). The differences between the three protection protocols were statistically not significant, and no protective protocol reduced post-ischemic myocardial dysfunction.\"\nQuestion:\n\"Does ischemic preconditioning require reperfusion before index ischemia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25957366": {
                "source": [
                    "\"Electronic health records have the potential to facilitate family history use by primary care physicians (PCPs) to provide personalized care. The objective of this study was to determine whether automated, at-the-visit tailored prompts about family history risk change PCP behavior.\nAutomated, tailored prompts highlighting familial risk for heart disease, stroke, diabetes, and breast, colorectal, or ovarian cancer were implemented during 2011 to 2012. Medical records of a cohort of community-based primary care patients, aged 35 to 65 years, who previously participated in our Family Healthware study and had a moderate or strong familial risk for any of the 6 diseases were subsequently reviewed. The main outcome measures were PCP response to the prompts, adding family history risk to problem summary lists, and patient screening status for each disease.\nThe 492 eligible patients had 847 visits during the study period; 152 visits had no documentation of response to a family history prompt. Of the remaining 695 visits, physician responses were reviewed family history (n = 372, 53.5%), discussed family history (n = 159, 22.9%), not addressed (n = 155, 22.3%), and reviewed family history and ordered tests/referrals (n = 5, 0.7%). There was no significant change in problem summary list documentation of risk status or screening interventions for any of the 6 diseases.\"\nQuestion:\n\"Prompting Primary Care Providers about Increased Patient Risk As a Result of Family History: Does It Work?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22694248": {
                "source": [
                    "\"Although the retroperitoneal approach has been the preferred choice for open urological procedures, retroperitoneoscopy is not the preferred approach for laparoscopy. This study aims to develop a training model for retroperitoneoscopy and to establish an experimental learning curve.\nFifteen piglets were operated on to develop a standard retroperitoneoscopic nephrectomy (RPN) training model. All procedures were performed with three ports. Intraoperative data (side, operative time, blood loss, peritoneal opening) were recorded. Animals were divided into groups A, the first eight, and B, the last seven cases. Data were statistically analyzed.\nWe performed fifteen RPNs. The operative time varied from 15 to 50 minutes (median 30 minutes). Blood loss varied from 5 to 100 mL (median 20 mL). We experienced five peritoneal openings; we had two surgical vascular complications managed laparoscopically. There was statistical difference between groups A and B for peritoneal opening (p = 0.025), operative time (p = 0.0037), and blood loss (p = 0.026).\nRPN in a porcine model could simulate the whole procedure, from creating the space to nephrectomy completion. Experimental learning curve was eight cases, after statistical data analysis.\"\nQuestion:\n\"Is there a model to teach and practice retroperitoneoscopic nephrectomy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26348845": {
                "source": [
                    "\"Rapid prescreening (RPS) is one of the quality assurance (QA) methods used in gynecologic cytology. The efficacy of RPS has been previously studied but mostly with respect to squamous lesions; in fact, there has been no study so far specifically looking at the sensitivity of RPS for detecting glandular cell abnormalities.\nA total of 80,565 Papanicolaou (Pap) smears underwent RPS during a 25-month period. A sample was designated as \"review for abnormality\" (R) if any abnormal cells (at the threshold of atypical squamous cells of undetermined significance/atypical glandular cells [AGC]) were thought to be present or was designated as negative (N) if none were detected. Each sample then underwent full screening (FS) and was designated as either R or N and also given a cytologic interpretation.\nThe final cytologic interpretation was a glandular cell abnormality (\u2265AGC) in 107 samples (0.13%); 39 of these (36.4%) were flagged as R on RPS. Twenty-four patients (33.8%) out of 71 who had histologic follow-up were found to harbor a high-grade squamous intraepithelial lesion or carcinoma; 13 of those 24 Pap smears (54.2%) had been flagged as R on RPS. Notably, 11 AGC cases were picked up by RPS only and not by FS and represented false-negative cases; 2 of these showed endometrial adenocarcinoma on histologic follow-up.\"\nQuestion:\n\"Pap smears with glandular cell abnormalities: Are they detected by rapid prescreening?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9100537": {
                "source": [
                    "\"Cytologic criteria reported to be helpful in the distinction of proliferative breast disease without atypia (PBD) from nonproliferative breast disease (NPBD) have not been rigorously tested.\nFifty-one air-dried, Diff-Quik-stained fine-needle aspirates (FNA) of palpable breast lesions with biopsy-proven diagnoses of NPBD (34 cases) or PBD (17 cases) were reviewed. The smears were evaluated for the cellularity, size, and architectural arrangement of the epithelial groups; the presence of single epithelial cells and myoepithelial cells; and nuclear characteristics.\nThe only cytologic feature found to be significantly different between PBD and NPBD was a swirling pattern of epithelial cells. A swirling pattern was noted in 13 of 17 PBD cases (76%) and 12 of 34 NPBD cases (35%) (P = 0.008).\"\nQuestion:\n\"Can nonproliferative breast disease and proliferative breast disease without atypia be distinguished by fine-needle aspiration cytology?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26418441": {
                "source": [
                    "\"Polyps identified at colonoscopy are predominantly diminutive (<5\u2009mm) with a small risk (>1%) of high-grade dysplasia or carcinoma; however, the cost of histological assessment is substantial.AIM: The aim of this study was to determine whether prediction of colonoscopy surveillance intervals based on real-time endoscopic assessment of polyp histology is accurate and cost effective.\nA prospective cohort study was conducted across a tertiary care and private community hospital. Ninety-four patients underwent colonoscopy and polypectomy of diminutive (\u22645\u2009mm) polyps from October 2012 to July 2013, yielding a total of 159 polyps. Polyps were examined and classified according to the Sano-Emura classification system. The endoscopic assessment (optical diagnosis) of polyp histology was used to predict appropriate colonoscopy surveillance intervals. The main outcome measure was the accuracy of optical diagnosis of diminutive colonic polyps against the gold standard of histological assessment.\nOptical diagnosis was correct in 105/108 (97.2%) adenomas. This yielded a sensitivity, specificity and positive and negative predictive values (with 95%CI) of 97.2% (92.1-99.4%), 78.4% (64.7-88.7%), 90.5% (83.7-95.2%) and 93% (80.9-98.5%) respectively. Ninety-two (98%) patients were correctly triaged to their repeat surveillance colonoscopy. Based on these findings, a cut and discard approach would have resulted in a saving of $319.77 per patient.\"\nQuestion:\n\"Can we ease the financial burden of colonoscopy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15381614": {
                "source": [
                    "\"Cutaneous melanoma in nonwhite persons has a manifestation and a prognosis that are different than those of cutaneous melanoma in white persons.\nCase series.\nTertiary care university-affiliated community medical center located in a multiethnic state in which white persons are a minority of the population.\nConsecutive series of 357 patients with melanoma seen between January 1994 and August 2003.\nEthnicity, age, sex, primary site, tumor thickness, nodal status, stage at diagnosis, and survival.\nThere were 208 men and 149 women who ranged in age from 15 to 93 years (mean, 58 years). Twenty-two patients initially had unknown primary sites. Of these 357 patients, 67 (18.7%) were nonwhite. There was no statistically significant difference in the age (P =.10) or sex (P =.57) distribution of these 2 populations. Nonwhite patients at initial diagnosis had thicker tumors (P =.002), more frequently had ulcerated primary tumors (P<.001), more frequently had positive nodes (P =.004), and were at a more advanced stage (P =.002) than their white counterparts. The anatomic distribution between the 2 populations was significantly different (P<.001), with a high incidence of melanoma on the sole and subungual locations and a substantially less frequent occurrence on the head and neck, trunk, and extremities in the nonwhite population when compared with the white population. The overall survival rate of the nonwhite patients was significantly worse than that of the white patients, but when stratified by stage at initial diagnosis, there was no difference in outcome.\"\nQuestion:\n\"Cutaneous melanoma in a multiethnic population: is this a different disease?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9363244": {
                "source": [
                    "\"To determine the effect of occupational exposure in a nuclear power plant in Griefswald, Germany on male and female fecundity.\nThe frequency of men and women exposed to ionizing radiation through work in a nuclear power plant among 270 infertile couples was retrospectively compared to a control fertile population using a pair-matched analysis. The total cumulative equivalent radiation dose was determined. In addition, the spermiograms of the male partners in both groups were compared and correlated to the degree of exposure.\nNo differences were noted in the frequency of nuclear power plant exposure between sterile and fertile groups. There was a higher rate of anomalous spermiograms in nuclear power plant workers. However, abnormalities were temporary. No correlation was found between the cumulative equivalent radiation dose and abnormal spermiograms.\"\nQuestion:\n\"Does occupational nuclear power plant radiation affect conception and pregnancy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23677366": {
                "source": [
                    "\"Anteroposterior, lateral, and right and left oblique lumbar spine radiographs are often a standard part of the evaluation of children who are clinically suspected of having spondylolysis. Recent concerns regarding radiation exposure and costs have brought the value of oblique radiographs into question. The purpose of the present study was to determine the diagnostic value of oblique views in the diagnosis of spondylolysis.\nRadiographs of fifty adolescents with L5 spondylolysis without spondylolisthesis and fifty controls were retrospectively reviewed. All controls were confirmed not to have spondylolysis on the basis of computed tomographic scanning, magnetic resonance imaging, or bone scanning. Anteroposterior, lateral, and right and left oblique radiographs of the lumbar spine were arranged into two sets of slides: one showing four views (anteroposterior, lateral, right oblique, and left oblique) and one showing two views (anteroposterior and lateral only). The slides were randomly presented to four pediatric spine surgeons for diagnosis, with four-view slides being presented first, followed by two-view slides. The slides for twenty random patients were later reanalyzed in order to calculate of intra-rater agreement. A power analysis demonstrated that this study was adequately powered. Inter-rater and intra-rater agreement were assessed on the basis of the percentage of overall agreement and intraclass correlation coefficients (ICCs). PCXMC software was used to generate effective radiation doses. Study charges were determined from radiology billing data.\nThere was no significant difference in sensitivity and specificity between four-view and two-view radiographs in the diagnosis of spondylolysis. The sensitivity was 0.59 for two-view studies and 0.53 for four-view studies (p = 0.33). The specificity was 0.96 for two-view studies and 0.94 for four-view studies (p = 0.60). Inter-rater agreement, intra-rater agreement, and agreement with gold-standard ICC values were in the moderate range and also demonstrated no significant differences. Percent overall agreement was 78% for four-view studies and 82% for two-view studies. The radiation effective dose was 1.26 mSv for four-view studies and 0.72 mSv for two-view studies (difference, 0.54 mSv). The charge for four-view studies was $145 more than that for two-view studies.\"\nQuestion:\n\"Do oblique views add value in the diagnosis of spondylolysis in adolescents?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15703931": {
                "source": [
                    "\"Compared with computed tomography (CT) and magnetic resonance imaging (MRI), positron emission tomography (PET) may have additional value in the assessment of primary and recurrent cervical cancer. However, the degree of tumour uptake of (18)F-2-fluoro-2-deoxy-D: -glucose (FDG) uptake is sometimes influenced by diabetes mellitus (DM). Therefore, we conducted this prospective study to compare the diagnostic ability of FDG-PET in patients with cervical cancer complicated by DM and those without DM.\nPatients with untreated locally advanced primary or clinically curable recurrent cervical carcinoma were enrolled. Both FDG-PET and MRI/CT scans were performed within 2 weeks. Patients were categorised into the following groups: hyperglycaemic DM (fasting blood sugar>126 mg/dl), euglycaemic DM and non-DM. The lesions were confirmed histologically or by clinical follow-up. The receiver operating characteristic curve method, with calculation of the area under the curve (AUC), was used to evaluate the discriminative power.\nFrom February 2001 to January 2003, 219 patients (75 with primary and 144 with recurrent cervical cancer) were eligible for analysis. Sixteen had hyperglycaemic DM, 12 had euglycaemic DM and 191 were in the non-DM group. The diagnostic power of PET in the hyperglycaemic DM, euglycaemic DM and non-DM groups did not differ significantly with regard to the identification of either metastatic lesions (AUC, 0.967/0.947/0.925, P>0.05) or primary tumours/local recurrence (AUC, 0.950/0.938/0.979, P>0.05). Considering all DM patients, PET showed a significantly higher detection power than MRI/CT scans in respect of metastatic lesions (AUC=0.956 vs 0.824, P=0.012).\"\nQuestion:\n\"Does diabetes mellitus influence the efficacy of FDG-PET in the diagnosis of cervical cancer?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23831910": {
                "source": [
                    "\"From March 2007 to January 2011, 88 DBE procedures were performed on 66 patients. Indications included evaluation anemia/gastrointestinal bleed, small bowel IBD and dilation of strictures. Video-capsule endoscopy (VCE) was used prior to DBE in 43 of the 66 patients prior to DBE evaluation.\nThe mean age was 62 years. Thirty-two patients were female, 15 were African-American; 44 antegrade and 44 retrograde DBEs were performed. The mean time per antegrade DBE was 107.4\u00b130.0 minutes with a distance of 318.4\u00b1152.9 cm reached past the pylorus. The mean time per lower DBE was 100.7\u00b127.3 minutes with 168.9\u00b1109.1 cm meters past the ileocecal valve reached. Endoscopic therapy in the form of electrocautery to ablate bleeding sources was performed in 20 patients (30.3%), biopsy in 17 patients (25.8%) and dilation of Crohn's-related small bowel strictures in 4 (6.1%). 43 VCEs with pathology noted were performed prior to DBE, with findings endoscopically confirmed in 32 cases (74.4%). In 3 cases the DBE showed findings not noted on VCE.\"\nQuestion:\n\"Double balloon enteroscopy: is it efficacious and safe in a community setting?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20530150": {
                "source": [
                    "\"Children referred with symptomatic gallstones complicating HS between April 1999 and April 2009 were prospectively identified and reviewed retrospectively. During this period, the policy was to undertake concomitant splenectomy only if indicated for haematological reasons and not simply because of planned cholecystectomy.\nA total of 16 patients (mean age 10.4, range 3.7 to 16 years, 11 women) with HS and symptomatic gallstones underwent cholecystectomy. Three patients subsequently required a splenectomy for haematological reasons 0.8-2.5 years after cholecystectomy; all three splenectomies were performed laparoscopically. There were no postoperative complications in the 16 patients; postoperative hospital stay was 1-3 days after either cholecystectomy or splenectomy. The 13 children with a retained spleen remain under regular review by a haematologist (median follow-up 4.6, range 0.5 to 10.6 years) and are well and transfusion independent.\"\nQuestion:\n\"Is cholecystectomy really an indication for concomitant splenectomy in mild hereditary spherocytosis?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12442934": {
                "source": [
                    "\"To determine whether prior exposure of non-steroidal anti-inflammatory drugs increases perioperative blood loss associated with major orthopaedic surgery.\nFifty patients scheduled for total hip replacement were allocated to two groups (double blind, randomized manner). All patients were pretreated for 2 weeks before surgery: Group 1 with placebo drug, Group 2 with ibuprofen. All patients were injected intrathecally with bupivacaine 20mg plus morphine 0.1 mg, in a total volume of 4 mL, to provide surgical anaesthesia.\nThe presence of severe adverse effects caused eight patients in the ibuprofen group and six in the placebo group to terminate their participation in the trial. The perioperative blood loss increased by 45% in the ibuprofen group compared with placebo. The total (+/-SD) blood loss in the ibuprofen group was 1161 (+/-472) mL versus 796 (+/-337) mL in the placebo group.\"\nQuestion:\n\"Does ibuprofen increase perioperative blood loss during hip arthroplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27184293": {
                "source": [
                    "\"Microbial contamination can be a marker for faulty process and is assumed to play an important role in the collection of hematopoietic progenitor cell (HPC) and infusion procedure. We aimed to determine the microbial contamination rates and evaluate the success of hematopoietic cell transplantation (HCT) in patients who received contaminated products.PATIENTS-\nWe analyzed microbial contamination records of HPC grafts between 2012 and 2015, retrospectively. Contamination rates of autologous donors were evaluated for at three steps: at the end of mobilization, following processing with dimethyl sulfoxide, and just before stem cell infusion. Grafts of allogeneic donors were assessed only before HCT.\nA total of 445 mobilization procedures were carried out on 333 (167 autologous and 166 allogeneic) donors. The microbiological contamination of peripheral blood (323/333 donations) and bone marrow (10/333 donations) products were analyzed. Bacterial contamination was detected in 18 of 1552 (1.15 %) culture bottles of 333 donors. During the study period 248 patients underwent HCT and among these patients microbial contamination rate on sample basis was 1.3 % (16/1212). Microbial contamination detected in nine patients (7 autologous; 2 allogeneic). In 8 of 9 patients, a febrile neutropenic attack was observed. The median day for the neutropenic fever was 4 days (0-9). None of the patients died within the post-transplant 30 days who received contaminated products.\"\nQuestion:\n\"Does microbial contamination influence the success of the hematopoietic cell transplantation outcomes?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23506394": {
                "source": [
                    "\"Arterial calcification is a significant cardiovascular risk factor in hemodialysis patients. A series of factors are involved in the process of arterial calcification; however, the relationship between malnutrition and arterial calcification is still unclear.\n68 hemodialysis patients were enrolled in this study. Nutrition status was evaluated using modified quantitative subjective global assessment (MQSGA). Related serum biochemical parameters were measured. And the radial artery samples were collected during the arteriovenous fistula surgeries. Hematoxylin/eosin stain was used to observe the arterial structures while Alizarin red stain to observe calcified depositions and classify calcified degree. The expressions of bone morphogenetic protein 2 (BMP2) and matrix Gla protein (MGP) were detected by immunohistochemistry and western blot methods.\n66.18% hemodialysis patients were malnutrition. In hemodialysis patients, the calcified depositions were mainly located in the medial layer of the radial arteries and the expressions of BMP2 and MGP were both increased in the calcified areas. The levels of serum albumin were negatively associated with calcification score and the expressions of BMP2 and MGP. While MQSGA score, serum phosphorus and calcium\u2009\u00d7\u2009phosphorus product showed positive relationships with calcification score and the expressions of BMP2 and MGP.\"\nQuestion:\n\"Malnutrition, a new inducer for arterial calcification in hemodialysis patients?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17916877": {
                "source": [
                    "\"To determine the therapeutic effect (alleviation of vascular type headache) and side effects of a slow intravenous metoclopramide infusion over 15 min compared with those effects of a bolus intravenous metoclopramide infusion over 2 min in the treatment of patients with recent onset vascular type headache.\nAll adults treated with metoclopramide for vascular type headache were eligible for entry into this clinical randomised double blinded trial. This study compared the effects of two different rates of intravenous infusion of metoclopramide over a period of 13 months at a university hospital emergency department. During the trial, side effects and headache scores were recorded at baseline (0 min), and then at 5, 15, 30 and 60 min. Repeated measures analysis of variance was used to compare the medication's efficacy and side effects.\nA total of 120 patients presenting to the emergency department met the inclusion criteria. Of these, 62 patients (51.7%) were given 10 mg metoclopramide as a slow intravenous infusion over 15 min (SIG group) and 58 patients (48.3%) were given 10 mg metoclopramide intravenous bolus infusion over 2 min (BIG group). 17 of the 58 patients in the BIG group (29.3%) and 4 of the 62 patients (6.5%) in the SIG group had akathisia (p = 0.001). There were no significant differences between the BIG and SIG groups in terms of mean headache scores (p = 0.34) and no adverse reactions in the study period. Metoclopramide successfully relieved the headache symptom(s) of patients in both the BIG and SIG groups.\"\nQuestion:\n\"Intravenous administration of metoclopramide by 2 min bolus vs 15 min infusion: does it affect the improvement of headache while reducing the side effects?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12630042": {
                "source": [
                    "\"The long-term survival of patients with gastric cancer is governed by various factors, such as the clinical stage of the cancer, the patient's nutritional state, and the treatment and may be governed by the volume of intraperitoneal adipose tissue. The aim of this study is to clarify the relationship between the degree of the patients' body mass index and their long-term survival.\nGastric cancer patients who had undergone a gastrectomy with D2-lymphadenectomy and with resection A and B according to the criteria of the Japanese Research Society for Gastric Cancer Rules were subgrouped into those patients with a body mass index<0.185 (the lower body mass index group) and those patients with a body mass index>0.210 (the higher body mass index group). The patient's morbidity and long-term survival rate was retrospectively compared between the 2 groups.\nA significantly longer mean survival rate was observed for the lower body mass index group in stage 2 (1667 vs. 1322 days, P = 0.0240). Also, a significantly longer mean survival rate was observed for the higher BMI group in stage 3a (1431 vs. 943, P = 0.0071).\"\nQuestion:\n\"Does body mass index (BMI) influence morbidity and long-term survival in gastric cancer patients after gastrectomy?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10759659": {
                "source": [
                    "\"To compare the accuracy achieved by a trained urology nurse practitioner (UNP) and consultant urologist in detecting bladder tumours during flexible cystoscopy.\nEighty-three patients underwent flexible cystoscopy by both the UNP and consultant urologist, each unaware of the other's findings. Before comparing the findings, each declared whether there was tumour or any suspicious lesion requiring biopsy.\nOf 83 patients examined by flexible cystoscopy, 26 were found to have a tumour or a suspicious lesion. One tumour was missed by the UNP and one by the urologist; each tumour was minute. Analysis using the chance-corrected proportional agreement (Kappa) was 0.94, indicating very close agreement.\"\nQuestion:\n\"The nurse cystoscopist: a feasible option?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9569972": {
                "source": [
                    "\"To investigate whether the S + G2/M fraction (proliferative index) is a prognostic determinant in breast cancers classified as Auer IV.\nPrognostic evaluation of Auer IV DNA histograms with respect to the high versus low S + G2/M fraction, obtained by image cytometry on consecutive breast cancer imprint preparations.\nWhen studying recurrence-free survival (n = 136), the prognostic value of S + G2/M was found to vary with time: it was negligible before the median time to relapse (1.5 years) but thereafter statistically significant, in both univariate and multivariate analysis. The same pattern was found when overall survival was used as the end point; the effect was delayed to about the median time until death (three years). Tumors with a low S + G2/M fraction were smaller and more often estrogen receptor- and progesterone receptor-positive than those with a high S + G2/M fraction.\"\nQuestion:\n\"Proliferative index obtained by DNA image cytometry. Does it add prognostic information in Auer IV breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25891436": {
                "source": [
                    "\"Previous studies have reported that the total bilirubin (TB) level is associated with coronary artery disease, heart failure and atrial fibrillation. These heart diseases can produce cardiogenic cerebral embolism and cause cardioembolic stroke. However, whether the serum TB could be a biomarker to differentiate cardioembolic stroke from other stroke subtypes is unclear.\nOur study consisted of 628 consecutive patients with ischaemic stroke. Various clinical and laboratory variables of the patients were analysed according to serum TB quartiles and stroke subtypes.\nThe higher TB quartile group was associated with atrial fibrillation, larger left atrium diameter, lower left ventricular fractional shortening and cardioembolic stroke (P<0.001, P = 0.001, P = 0.033, P<0.001, respectively). Furthermore, serum TB was a statistically significant independent predictor of cardioembolic stroke in a multivariable setting (Continuous, per unit increase OR = 1.091, 95%CI: 1.023-1.164, P = 0.008).\"\nQuestion:\n\"Is serum total bilirubin useful to differentiate cardioembolic stroke from other stroke subtypes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10411439": {
                "source": [
                    "\"Lower limb compartment syndrome has been reported to occur after colorectal, urological, and gynecological procedures during which the patient's lower limbs are elevated for prolonged periods of time.\nWe investigated lower limb perfusion in a group of patients undergoing prolonged pelvic surgery both during and immediately after surgery, using intra-arterial blood pressure monitoring, laser doppler flowmetry, and pulse oximetry.\nUse of the modified lithotomy position was not associated with any demonstrable decrease in lower limb perfusion. The addition of 15 degrees head-down tilt, however, during pelvic dissection, led to an immediate and significant drop in lower limb perfusion (P<0.05; Mann-Whitney U test). The subgroup of patients analyzed postoperatively showed a ten-fold increase (P<0.01) in perfusion that was confined to the muscle compartment with no demonstrable increase in skin perfusion or intra-arterial pedal blood pressure.\"\nQuestion:\n\"Lloyd-Davies position with Trendelenburg--a disaster waiting to happen?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21228436": {
                "source": [
                    "\"The purpose of this study was to evaluate the impact of a patient-safety curriculum administered during a paediatric clerkship on medical students' attitudes towards patient safety.\nMedical students viewed an online video introducing them to systems-based analyses of medical errors. Faculty presented an example of a medication administration error and demonstrated use of the Learning From Defects tool to investigate the defect. Student groups identified and then analysed medication errors during their clinical rotation using the Learning From Defects framework to organise and present their findings. Outcomes included patient safety attitudinal changes, as measured by questions derived from the Safety Attitudes Questionnaire.\n108 students completed the curriculum between July 2008 and July 2009. All student groups (25 total) identified, analysed and presented patient safety concerns. Curriculum effectiveness was demonstrated by significant changes on questionnaire items related to patient safety attitudes. The majority of students felt that the curriculum was relevant to their clinical rotation and should remain part of the clerkship.\"\nQuestion:\n\"Can teaching medical students to investigate medication errors change their attitudes towards patient safety?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22349635": {
                "source": [
                    "\"Treatment of elderly cancer patients has gained importance. One question regarding the treatment of metastatic spinal cord compression (MSCC) is whether elderly patients benefit from surgery in addition to radiotherapy? In attempting to answer this question, we performed a matched-pair analysis comparing surgery followed by radiotherapy to radiotherapy alone.\nData from 42 elderly (age>\u200965 years) patients receiving surgery plus radiotherapy (S\u2009+\u2009RT) were matched to 84 patients (1:2) receiving radiotherapy alone (RT). Groups were matched for ten potential prognostic factors and compared regarding motor function, local control, and survival. Additional matched-pair analyses were performed for the subgroups of patients receiving direct decompressive surgery plus stabilization of involved vertebrae (DDSS, n\u2009=\u200981) and receiving laminectomy (LE, n\u2009=\u200945).\nImprovement of motor function occurred in 21% after S\u2009+\u2009RT and 24% after RT (p\u2009=\u20090.39). The 1-year local control rates were 81% and 91% (p\u2009=\u20090.44), while the 1-year survival rates were 46% and 39% (p\u2009=\u20090.71). In the matched-pair analysis of patients receiving DDSS, improvement of motor function occurred in 22% after DDSS\u2009+\u2009RT and 24% after RT alone (p\u2009=\u20090.92). The 1-year local control rates were 95% and 89% (p\u2009=\u20090.62), and the 1-year survival rates were 54% and 43% (p\u2009=\u20090.30). In the matched-pair analysis of patients receiving LE, improvement of motor function occurred in 20% after LE\u2009+\u2009RT and 23% after RT alone (p\u2009=\u20090.06). The 1-year local control rates were 50% and 92% (p\u2009=\u20090.33). The 1-year survival rates were 32% and 32% (p\u2009=\u20090.55).\"\nQuestion:\n\"Do elderly patients benefit from surgery in addition to radiotherapy for treatment of metastatic spinal cord compression?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22251324": {
                "source": [
                    "\"This study investigated associations between the performance of dental students in each of the three components of the selection procedure [academic average, Undergraduate Medicine and Health Sciences Admission Test (UMAT) and structured interview], socio-demographic characteristics and their academic success in an undergraduate dental surgery programme.\nLongitudinal review of admissions data relating to students entering dental education at the University of Otago, New Zealand, between 2004 and 2009 was compared with academic performance throughout the dental programme.\nAfter controlling for variables, pre-admission academic average, UMAT scores and interview performance did not predict performance as a dental student. Class place in second year, however, was a strong predictor of class place in final year. Multivariate analysis demonstrated that the best predictors of higher class placement in the final year were New Zealand European ethnicity and domestic (rather than international) student status. Other socio-demographic characteristics were not associated with performance. These interim findings provide a sound base for the ongoing study.\"\nQuestion:\n\"Does performance in selection processes predict performance as a dental student?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "29112560": {
                "source": [
                    "\"It is unclear whether traveling long distances to high-volume centers would compensate for travel burden among patients undergoing rectal cancer resection.\nThe purpose of this study was to determine whether operative volume outweighs the advantages of being treated locally by comparing the outcomes of patients with rectal cancer treated at local, low-volume centers versus far, high-volume centers.\nThis was a population-based study.\nThe National Cancer Database was queried for patients with rectal cancer.\nPatients with stage II or III rectal cancer who underwent surgical resection between 2006 and 2012 were included.\nThe outcomes of interest were margins, lymph node yield, receipt of neoadjuvant chemoradiation, adjuvant chemotherapy, readmission within 30 days, 30-day and 90-day mortality, and 5-year overall survival.\nA total of 18,605 patients met inclusion criteria; 2067 patients were in the long-distance/high-volume group and 1362 in the short-distance/low-volume group. The median travel distance was 62.6 miles for the long-distance/high-volume group and 2.3 miles for the short-distance/low-volume group. Patients who were younger, white, privately insured, and stage III were more likely to have traveled to a high-volume center. When controlled for patient factors, stage, and hospital factors, patients in the short-distance/low-volume group had lower odds of a lymph node yield \u226512 (OR = 0.51) and neoadjuvant chemoradiation (OR = 0.67) and higher 30-day (OR = 3.38) and 90-day mortality (OR = 2.07) compared with those in the long-distance/high-volume group. The short-distance/low-volume group had a 34% high risk of overall mortality at 5 years compared with the long-distance/high-volume group.\nWe lacked data regarding patient and physician decision making and surgeon-specific factors.\"\nQuestion:\n\"Is the Distance Worth It?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26134053": {
                "source": [
                    "\"Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\nWe distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\nWe received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\"\nQuestion:\n\"Outcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27217036": {
                "source": [
                    "\"Longer duration of neoadjuvant (NA) imatinib\u00a0(IM) used for locally advanced (LA) gastrointestinal stromal tumours (GIST) is not based on biology of the tumour reflected by kit mutation analysis.\nLA or locally recurrent (LR) GIST treated with NA IM from May 2008 to March 2015 from a prospective database were included in\u00a0the analysis. Archived formalin-fixed paraffin-embedded tissues (FFPE) were used for testing KIT exons 9, 11, 13 and 17 by PCR.\nOne hundred twenty-five patients with LA or LR GIST were treated with NA IM. Forty-five patients (36\u00a0%) had undergone c-kit mutation testing. Exon 11 was seen in 25 patients (55.5\u00a0%), 3 with exon 9 (6.7\u00a0%) and 2 with exon 13 (4.4\u00a0%). Twelve were wild type (26.6\u00a0%) and \u00a03 (6.7 %) were declared uninterpretable. Response rate (RR) for the exon 11 mutants was higher than the non-exon 11 mutant group (84 vs. 40\u00a0%, p\u2009=\u20090.01). Disease stabilization rate (DSR) rates were also higher in the exon 11 subgroup than non-exon 11 group (92 vs. 75\u00a0%). Eighty-four per cent exon 11 and 75\u00a0% non-exon 11 mutants were surgical candidates. Patients undergoing surgery had significantly improved event free survival (EFS) (p\u2009<\u20090.001) compared to patients not undergoing surgery, with the same trend seen in OS (p\u2009=\u20090.021). Patients with a SD on response to NA IM had a lower EFS (p\u2009=\u20090.076) and OS compared to patients achieving CR/PR. There were no differences between the various exon variants in terms of outcomes and responses\"\nQuestion:\n\"Neoadjuvant Imatinib in Locally Advanced Gastrointestinal stromal Tumours, Will Kit Mutation Analysis Be a Pathfinder?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23794696": {
                "source": [
                    "\"To investigate the effect of bracket-ligature combination on the amount of orthodontic space closure over three months.\nRandomized clinical trial with three parallel groups.\nA hospital orthodontic department (Chesterfield Royal Hospital, UK).\nForty-five patients requiring upper first premolar extractions.\nInformed consent was obtained and participants were randomly allocated into one of three groups: (1) conventional pre-adjusted edgewise brackets and elastomeric ligatures; (2) conventional pre-adjusted edgewise brackets and Super Slick(\u00ae) low friction elastomeric ligatures; (3) Damon 3MX(\u00ae) passive self-ligating brackets. Space closure was undertaken on 0\u00b7019\u00d70\u00b7025-inch stainless steel archwires with nickel-titanium coil springs. Participants were recalled at four weekly intervals. Upper alginate impressions were taken at each visit (maximum three). The primary outcome measure was the mean amount of space closure in a 3-month period.\nA one-way ANOVA was undertaken [dependent variable: mean space closure (mm); independent variable: group allocation]. The amount of space closure was very similar between the three groups (1 mm per 28 days); however, there was a wide variation in the rate of space closure between individuals. The differences in the amount of space closure over three months between the three groups was very small and non-significant (P\u200a=\u200a0\u00b7718).\"\nQuestion:\n\"Does the bracket-ligature combination affect the amount of orthodontic space closure over three months?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27642458": {
                "source": [
                    "\"Polio eradication is now feasible after removal of Nigeria from the list of endemic countries and global reduction of cases of wild polio virus in 2015 by more than 80%. However, all countries must remain focused to achieve eradication. In August 2015, the Catholic bishops in Kenya called for boycott of a polio vaccination campaign citing safety concerns with the polio vaccine. We conducted a survey to establish if the coverage was affected by the boycott.\nA cross sectional survey was conducted in all the 32 counties that participated in the campaign. A total of 90,157 children and 37,732 parents/guardians were sampled to determine the vaccination coverage and reasons for missed vaccination.\nThe national vaccination coverage was 93% compared to 94% in the November 2014 campaign. The proportion of parents/guardians that belonged to Catholic Church was 31% compared to 7% of the children who were missed. Reasons for missed vaccination included house not being visited (44%), children not being at home at time of visit (38%), refusal by parents (12%), children being as leep (1%), and various other reasons (5%). Compared to the November 2014 campaign, the proportion of children who were not vaccinated due to parent's refusal significantly increased from 6% to 12% in August 2015.\"\nQuestion:\n\"Did the call for boycott by the Catholic bishops affect the polio vaccination coverage in Kenya in 2015?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18182265": {
                "source": [
                    "\"In this study, the authors discussed the feasibility and value of diffusion-weighted (DW) MR imaging in the detection of uterine endometrial cancer in addition to conventional nonenhanced MR images.\nDW images of endometrial cancer in 23 patients were examined by using a 1.5-T MR scanner. This study investigated whether or not DW images offer additional incremental value to conventional nonenhanced MR imaging in comparison with histopathological results. Moreover, the apparent diffusion coefficient (ADC) values were measured in the regions of interest within the endometrial cancer and compared with those of normal endometrium and myometrium in 31 volunteers, leiomyoma in 14 patients and adenomyosis in 10 patients. The Wilcoxon rank sum test was used, with a p<0.05 considered statistically significant.\nIn 19 of 23 patients, endometrial cancers were detected only on T2-weighted images. In the remaining 4 patients, of whom two had coexisting leiomyoma, no cancer was detected on T2-weighted images. This corresponds to an 83% detection sensitivity for the carcinomas. When DW images and fused DW images/T2-weighted images were used in addition to the T2-weighted images, cancers were identified in 3 of the remaining 4 patients in addition to the 19 patients (overall detection sensitivity of 96%). The mean ADC value of endometrial cancer (n=22) was (0.97+/-0.19)x10(-3)mm(2)/s, which was significantly lower than those of the normal endometrium, myometrium, leiomyoma and adenomyosis (p<0.05).\"\nQuestion:\n\"Body diffusion-weighted MR imaging of uterine endometrial cancer: is it helpful in the detection of cancer in nonenhanced MR imaging?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19401574": {
                "source": [
                    "\"To evaluate the diagnostic accuracy of gadofosveset-enhanced magnetic resonance (MR) angiography in the assessment of carotid artery stenosis, with digital subtraction angiography (DSA) as the reference standard, and to determine the value of reading first-pass, steady-state, and \"combined\" (first-pass plus steady-state) MR angiograms.\nThis study was approved by the local ethics committee, and all subjects gave written informed consent. MR angiography and DSA were performed in 84 patients (56 men, 28 women; age range, 61-76 years) with carotid artery stenosis at Doppler ultrasonography. Three readers reviewed the first-pass, steady-state, and combined MR data sets, and one independent observer evaluated the DSA images to assess stenosis degree, plaque morphology and ulceration, stenosis length, and tandem lesions. Interobserver agreement regarding MR angiographic findings was analyzed by using intraclass correlation and Cohen kappa coefficients. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated by using the McNemar test to determine possible significant differences (P<.05).\nInterobserver agreement regarding all MR angiogram readings was substantial. For grading stenosis, sensitivity, specificity, PPV, and NPV were, respectively, 90%, 92%, 91%, and 91% for first-pass imaging; 95% each for steady-state imaging; and 96%, 99%, 99%, and 97% for combined imaging. For evaluation of plaque morphology, respective values were 84%, 86%, 88%, and 82% for first-pass imaging; 98%, 97%, 98%, and 97% for steady-state imaging; and 98%, 100%, 100%, and 97% for combined imaging. Differences between the first-pass, steady-state, and combined image readings for assessment of stenosis degree and plaque morphology were significant (P<.001).\"\nQuestion:\n\"Gadofosveset-enhanced MR angiography of carotid arteries: does steady-state imaging improve accuracy of first-pass imaging?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25079920": {
                "source": [
                    "\"As parents of young children are often unaware their child is overweight, screening provides the opportunity to inform parents and provide the impetus for behaviour change. We aimed to determine if parents could recall and understand the information they received about their overweight child after weight screening.\nRandomised controlled trial of different methods of feedback.\nParticipants were recruited through primary and secondary care but appointments took place at a University research clinic.\n1093 children aged 4-8\u2005years were screened. Only overweight children (n=271, 24.7%) are included in this study. Parents of overweight children were randomised to receive feedback regarding their child's weight using best practice care (BPC) or motivational interviewing (MI) at face-to-face interviews typically lasting 20-40\u2005min. 244 (90%) parents participated in a follow-up interview 2\u2005weeks later to assess recall and understanding of information from the feedback session.\nInterviews were audio-taped and transcribed verbatim before coding for amount and accuracy of recall. Scores were calculated for total recall and sub-categories of interest.\nOverall, 39% of the information was recalled (mean score 6.3 from possible score of 16). Parents given feedback via BPC recalled more than those in the MI group (difference in total score 0.48; 95% CI 0.05 to 0.92). Although 94% of parents were able to correctly recall their child's weight status, fewer than 10 parents could accurately describe what the measurements meant. Maternal education (0.81; 0.25 to 1.37) and parental ratings of how useful they found the information (0.19; 0.04 to 0.35) were significant predictors of recall score in multivariate analyses.\"\nQuestion:\n\"Do parents recall and understand children's weight status information after BMI screening?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19106867": {
                "source": [
                    "\"Recent suicide bombings pose the novel problem for Trauma Centers of the massive simultaneous arrival of many gravely wounded patients.\nWe report the experience of the French-German Military Trauma Group, a Level 2 Trauma Center, in Afghanistan during the wave of suicide bombings in February 2007.\nFourteen casualties were received. A first triage was carried out by the U S Army Level I group prior to evacuation. A second surgical triage was carried out with systematic ultrasound exam. Four cases (ISS>25) were re-categorized and underwent emergency surgical procedures.\"\nQuestion:\n\"The Main Gate Syndrome: a new format in mass-casualty victim \"surge\" management?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25168472": {
                "source": [
                    "\"The intent of this study was to determine if the use of a single or combination of static foot posture measurements can be used to predict rearfoot, midfoot, and forefoot plantar surface area in individuals with pronated or normal foot types.\nTwelve foot measurements were collected on 52 individuals (mean age 25.8 years) with the change in midfoot width used to place subjects in a pronated or normal foot mobility group. Dynamic plantar contact area was collected during walking with a pressure sensor platform. The 12 measures were entered into a stepwise regression analysis to determine the optimal set of measures associated with regional plantar surface area.\nA two variable model was found to describe the relationship between the foot measurements and forefoot plantar contact area (r(2)=0.79, p<0.0001). A four variable model was found to describe the relationship between the foot measurements and midfoot plantar contact area (r(2)=0.85, p<0.0001) in those individuals with a 1.26cm or greater change in midfoot width.\"\nQuestion:\n\"Can static foot posture measurements predict regional plantar surface area?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9191526": {
                "source": [
                    "\"In an attempt to improve the care they provide for their patients with breast cancer, the authors' institution developed a multidisciplinary breast cancer clinic (MDBCC) to offer \"one-stop shopping\" consultation and support for newly diagnosed breast cancer patients.\nOne hundred sixty-two patients, the control group for this study, were evaluated at Henry Ford Hospital during the year prior to the opening of the MDBCC. These patients, who were referred in the traditional sequential consultation manner, were compared with the first 177 patients seen during the first year of the clinic's operation. Retrospective chart reviews were conducted to assess treatment timeliness, and anonymous questionnaires were used to assess patient satisfaction.\nThe authors found that the MDBCC increased patient satisfaction by encouraging involvement of patients' families and friends and by helping patients make treatment decisions (P<0.001). The time between diagnosis and the initiation of treatment was also significantly decreased (42.2 days vs. 29.6 days; P<0.0008).\"\nQuestion:\n\"Multidisciplinary breast cancer clinics. Do they work?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23761381": {
                "source": [
                    "\"Testosterone measurement by liquid chromatography tandem mass spectrometry (LC-MS/MS) is well accepted as the preferred technique for the analysis of testosterone. Variation is seen between assays and this may be due to differences in calibration as commercial calibrators for this assay are not readily available. We investigated the effects calibration in routine clinical LC-MS/MS assays.\nAll LC-MS/MS users that were registered with the UKNEQAS external quality assurance scheme for testosterone were invited to take part in the study. A set of seven serum samples and serum-based calibrators were sent to all laboratories that expressed an interest. The laboratories were instructed to analyse all samples using there own calibrators and return the results and a method questionnaire for analysis.\nFifteen laboratories took part in the study. There was no consensus on supplier of testosterone or matrix for the preparation of calibrators and all were prepared in-house. Also, a wide variety of mass spectrometers, internal standards, chromatography conditions and sample extractions were used. The variation in results did not improve when the results were corrected with a common calibrator.\"\nQuestion:\n\"Is calibration the cause of variation in liquid chromatography tandem mass spectrometry testosterone measurement?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22954812": {
                "source": [
                    "\"Recent reports indicate that the prevalence of bipolar disorder (BD) in patients with an acute major depressive episode might be higher than previously thought. We aimed to study systematically all patients who sought therapy for major depressive episode (MDE) within the BRIDGE study in Germany, reporting on an increased number (increased from 2 in the international BRIDGE report to 5) of different diagnostic algorithms.\nA total of 252 patients with acute MDE (DSM-IV confirmed) were examined for the existence of BD (a) according to DSM-IV criteria, (b) according to modified DSM-IV criteria (without the exclusion criterion of 'mania not induced by substances/antidepressants'), (c) according to a Bipolarity Specifier Algorithm which expands the DSM-IV criteria, (d) according to HCL-32R (Hypomania-Checklist-32R), and (e) according to a criteria-free physician's diagnosis.\nThe five different diagnostic approaches yielded immensely variable prevalences for BD: (a) 11.6; (b) 24.8%; (c) 40.6%; (d) 58.7; e) 18.4% with only partial overlap between diagnoses according to the physician's diagnosis or HCL-32R with diagnoses according to the three DSM-based algorithms.\"\nQuestion:\n\"Are bipolar disorders underdiagnosed in patients with depressive episodes?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23810330": {
                "source": [
                    "\"Intraoperative neuromonitoring (IONM) aims to control nerve-sparing total mesorectal excision (TME) for rectal cancer in order to improve patients' functional outcome. This study was designed to compare the urogenital and anorectal functional outcome of TME with and without IONM of innervation to the bladder and the internal anal sphincter.\nA consecutive series of 150 patients with primary rectal cancer were analysed. Fifteen match pairs with open TME and combined urogenital and anorectal functional assessment at follow up were established identical regarding gender, tumour site, tumour stage, neoadjuvant radiotherapy and type of surgery. Urogenital and anorectal function was evaluated prospectively on the basis of self-administered standardized questionnaires, measurement of residual urine volume and longterm-catheterization rate.\nNewly developed urinary dysfunction after surgery was reported by 1 of 15 patients in the IONM group and by 6 of 15 in the control group (p\u00a0=\u00a00.031). Postoperative residual urine volume was significantly higher in the control group. At follow up impaired anorectal function was present in 1 of 15 patients undergoing TME with IONM and in 6 of 15 without IONM (p\u00a0=\u00a00.031). The IONM group showed a trend towards a lower rate of sexual dysfunction after surgery.\"\nQuestion:\n\"Is intraoperative neuromonitoring associated with better functional outcome in patients undergoing open TME?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22680064": {
                "source": [
                    "\"To determine the ability of early sonogram to predict the presentation of twin A at birth.\nA retrospective cohort study was conducted on all twin pregnancies evaluated at our Fetal Evaluation Unit from 2007 to 2009. Sonogram records were reviewed for the presentation of twin A at seven gestational age intervals and inpatient medical records were reviewed for the presentation of twin A at delivery. The positive predictive value, sensitivity, and specificity of presentation as determined by ultrasound, at each gestational age interval, for the same presentation at delivery were calculated.\nTwo hundred and thirty-eight twin pregnancies met inclusion criteria. A total of 896 ultrasounds were reviewed. The positive predictive value of cephalic presentation of twin A as determined by ultrasound for the persistence of cephalic presentation at delivery reached 95% after 28 weeks gestation. The positive predictive value for noncephalic presentation as established by sonogram for noncephalic at delivery was>90% after 32 weeks gestation.\"\nQuestion:\n\"Can third trimester ultrasound predict the presentation of the first twin at delivery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26126304": {
                "source": [
                    "\"To compare in vitro fertilization (IVF) outcomes in low responders stimulated with microdose leuprolide protocol (ML) following pretreatment with either oral contraceptive pill (OCP) or luteal estradiol (E2) + GnRH antagonist (E2 + antag) for follicular synchronization prior to controlled ovarian hyperstimulation (COH).\nThis was a retrospective study of 130 women, who were poor responders, undergoing IVF with either OCP/ML or E2+ antag/ML protocols. The main outcome measures were ongoing pregnancy rates, number of oocytes retrieved, and cancellation rate.\nBoth groups were similar in baseline characteristics. There were no significant differences in gonadotropin requirement, cancellation rate, and number of embryos transferred. Ongoing pregnancy rates (40% vs. 15%) were significantly higher in the OCP/ML group. Trends toward greater number of oocytes retrieved (7.7 \u00b1 3.4 vs. 5.9 \u00b1 4.2) and improved implantation rates (20% vs. 12%) were also noted, but these did not reach statistical significance.\"\nQuestion:\n\"Estradiol and Antagonist Pretreatment Prior to Microdose Leuprolide in in Vitro Fertilization. Does It Improve IVF Outcomes in Poor Responders as Compared to Oral Contraceptive Pill?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27288618": {
                "source": [
                    "\"To determine whether prophylactic inhaled heparin is effective for the prevention and treatment of pneumonia patients receiving mechanical ventilation (MV) in the intensive care unit.\nA phase 2, double blind randomized controlled trial stratified for study center and patient type (non-operative, post-operative) was conducted in three university-affiliated intensive care units. Patients aged \u226518years and requiring invasive MV for more than 48hours were randomized to usual care, nebulization of unfractionated sodium heparin (5000 units in 2mL) or placebo nebulization with 0.9% sodium chloride (2mL) four times daily with the main outcome measures of the development of ventilator associated pneumonia (VAP), ventilator associated complication (VAC) and sequential organ failure assessment scores in patients with pneumonia on admission or who developed VAP.\nAustralian and New Zealand Clinical Trials Registry ACTRN12612000038897.\nTwo hundred and fourteen patients were enrolled (72 usual care, 71 inhaled sodium heparin, 71 inhaled sodium chloride). There were no differences between treatment groups in terms of the development of VAP, using either Klompas criteria (6-7%, P=1.00) or clinical diagnosis (24-26%, P=0.85). There was no difference in the clinical consistency (P=0.70), number (P=0.28) or the total volume of secretions per day (P=.54). The presence of blood in secretions was significantly less in the usual care group (P=0.005).\"\nQuestion:\n\"Is inhaled prophylactic heparin useful for prevention and Management of Pneumonia in ventilated ICU patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17940352": {
                "source": [
                    "\"To evaluate the impact of HER2 immunoreactivity on clinical outcome in locally advanced urothelial carcinoma patients who received surgery alone, or methotrexate, vinblastine, epirubicin, and cisplatin (M-VEC) as adjuvant chemotherapy.\nWe studied 114 formalin-fixed paraffin-embedded specimens obtained from locally advanced urothelial carcinoma patients receiving surgery alone or adjuvant M-VEC. The authors evaluated HER2 immunoreactivity using immunohistochemical staining and explored the influence of pathological parameters and HER2 immunoreactivity on progression-free survival (PFS) and disease-specific overall survival (OS) using univariate and multivariate Cox's analyses.\nUrothelial carcinoma of the bladder had a significantly higher frequency of HER2 immunoreactivity than that of the upper urinary tract (60.7 vs. 20.7%, p<0.0001). Overall, nodal status was a strong and independent prognostic indicator for clinical outcome. The HER2 immunoreactivity was significantly associated with PFS (p = 0.02) and disease-specific OS (p = 0.005) in advanced urothelial carcinoma patients. As for patients with adjuvant M-VEC, HER2 immunoreactivity was a significant prognostic factor for PFS (p = 0.03) and disease-specific OS (p = 0.02) using univariate analysis, but not multivariate analysis, and not for patients receiving watchful waiting.\"\nQuestion:\n\"Does HER2 immunoreactivity provide prognostic information in locally advanced urothelial carcinoma patients receiving adjuvant M-VEC chemotherapy?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10375486": {
                "source": [
                    "\"to describe variation in utilisation of carotid endarterectomy (CEA) within two English health regions and explore relationships between use, need and proximity to services.\nconsecutive case series of operations. Comparison at a population level with district stroke mortality, hospital admissions and material deprivation.\nstandardised utilisation rates for CEA and measures of inter-district variability. Spearman's rank correlation coefficients for associations between variables.\nvariation in utilisation rates was considerable (14-fold difference across district populations). More individuals had bilateral surgery in the Yorkshire region than in the Northern (11.7% vs. 5.5%, p=0.002). There was no association between utilisation rates for CEA and district stroke mortality (r=-0.06, 95% CI -0.41 to 0.30) or admission rates for stroke (r=0.17, 95% CI -0.2 to 0.49). There was a strong relationship between residence in districts where services were located and higher utilisation. Rates of CEA were lowest in the regions' most affluent wards.\"\nQuestion:\n\"Are variations in the use of carotid endarterectomy explained by population Need?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15466981": {
                "source": [
                    "\"The combined use of free and total prostate-specific antigen (PSA) in early detection of prostate cancer has been controversial. This article systematically evaluates the discriminating capacity of a large number of combination tests.\nFree and total PSA were analyzed in stored serum samples taken prior to diagnosis in 429 cases and 1,640 controls from the Physicians' Health Study. We used a classification algorithm called logic regression to search for clinically useful tests combining total and percent free PSA and receiver operating characteristic analysis and compared these tests with those based on total and complexed PSA. Data were divided into training and test subsets. For robustness, we considered 35 test-train splits of the original data and computed receiver operating characteristic curves for each test data set.\nThe average area under the receiver operating characteristic curve across test data sets was 0.74 for total PSA and 0.76 for the combination tests. Combination tests with higher sensitivity and specificity than PSA>4.0 ng/mL were identified 29 out of 35 times. All these tests extended the PSA reflex range to below 4.0 ng/mL. Receiver operating characteristic curve analysis indicated that the overall diagnostic performance as expressed by the area under the curve did not differ significantly for the different tests.\"\nQuestion:\n\"Prostate-specific antigen and free prostate-specific antigen in the early detection of prostate cancer: do combination tests improve detection?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19309468": {
                "source": [
                    "\"A variable effect of inflammation on alloimmunization to transfused red blood cells (RBCs) in mice has been recently reported. We investigated whether RBC alloimmunization in humans was affected by transfusion of blood products in temporal proximity to experiencing a febrile transfusion reaction (FTR) to platelets (PLTs), an event predominantly mediated by inflammatory cytokines.\nBlood bank databases were used to identify patients who experienced an FTR or possible FTR to PLTs from August 2000 to March 2008 (FTR group). The control group of patients received a PLT transfusion on randomly selected dates without experiencing an FTR. The \"event\" was defined as the PLT transfusion that caused the FTR in the FTR group or the index PLT transfusion in the control group. The number of transfused blood products and their proximity to the event were recorded along with other recipient data. The primary endpoint was the rate of RBC alloimmunization between the two groups.\nThere were 190 recipients in the FTR group and 245 in the control group. Overall, the recipients in the control group were younger and received more blood products on the day of their event and over the subsequent 10 days. The alloimmunization rate among recipients in the FTR group was higher than in the control group (8% vs. 3%, respectively; p = 0.026).\"\nQuestion:\n\"Does a febrile reaction to platelets predispose recipients to red blood cell alloimmunization?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18565233": {
                "source": [
                    "\"Epidemiologic studies have suggested that hypertriglyceridemia and insulin resistance are related to the development of colon cancer. Nuclear peroxisome proliferator-activated receptors (PPAR), which play a central role in lipid and glucose metabolism, had been hypothesized as being involved in colon cancerogenesis. In animal studies the lipid-lowering PPAR ligand bezafibrate suppressed colonic tumors. However, the effect of bezafibrate on colon cancer development in humans is unknown. Therefore, we proposed to investigate a possible preventive effect of bezafibrate on the development of colon cancer in patients with coronary artery disease during a 6-year follow-up.\nOur population included 3011 patients without any cancer diagnosis who were enrolled in the randomized, double blind Bezafibrate Infarction Prevention (BIP) Study. The patients received either 400 mg of bezafibrate retard (1506 patients) or placebo (1505 patients) once a day. Cancer incidence data were obtained by matching a subject's identification numbers with the National Cancer Registry. Each matched record was checked for correct identification.\nDevelopment of new cancer (all types) was recorded in 177 patients: in 79 (5.25%) patients from the bezafibrate group vs. 98 (6.51%) from the placebo group. Development of colon cancer was recorded in 25 patients: in 8 (0.53%) patients from the bezafibrate group vs. 17 (1.13%) from the placebo group, (Fisher's exact test: one side p = 0.05; two side p = 0.07). A difference in the incidence of cancer was only detectable after a 4 year lag and progressively increased with continued follow-up. On multivariable analysis the colon cancer risk in patients who received bezafibrate tended to be lower with a hazard ratio of 0.47 and 95% confidence interval 0.2-1.1.\"\nQuestion:\n\"Does the lipid-lowering peroxisome proliferator-activated receptors ligand bezafibrate prevent colon cancer in patients with coronary artery disease?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9107172": {
                "source": [
                    "\"If long-term use of left ventricular assist devices (LVADs) as bridges to transplantation is successful, the issue of permanent device implantation in lieu of transplantation could be addressed through the creation of appropriately designed trials. Our medium-term experience with both pneumatically and electrically powered ThermoCardiosystems LVADs is presented to outline the benefits and limitations of device support in lieu of transplantation.\nDetailed records were kept prospectively for all patients undergoing LVAD insertion. Fifty-eight LVADs were inserted over 5 years, with a survival rate of 74%. Mean patient age was 50 years, and duration of support averaged 98 days. Although common, both preexisting infection and infection during LVAD support were not associated with increased mortality or decreased rate of successful transplantation. Thromboembolic complications were rare, occurring in only three patients (5%) despite the absence of anticoagulation. Ventricular arrhythmias were well tolerated in all patients except in cases of early perioperative right ventricular failure, with no deaths. Right ventricular failure occurred in one third of patients and was managed in a small percentage by right ventricular assist device (RVAD) support and/or inhaled nitric oxide therapy. There were no serious device malfunctions, but five graft-related hemorrhages resulted in two deaths. Finally, a variety of noncardiac surgical procedures were performed in LVAD recipients, with no major morbidity and mortality.\"\nQuestion:\n\"Bridge experience with long-term implantable left ventricular assist devices. Are they an alternative to transplantation?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10575390": {
                "source": [
                    "\"To compare adherence to follow-up recommendations for colposcopy or repeated Papanicolaou (Pap) smears for women with previously abnormal Pap smear results.\nRetrospective cohort study.\nThree northern California family planning clinics.\nAll women with abnormal Pap smear results referred for initial colposcopy and a random sample of those referred for repeated Pap smear. Medical records were located and reviewed for 90 of 107 women referred for colposcopy and 153 of 225 women referred for repeated Pap smears.\nRoutine clinic protocols for follow-up--telephone call, letter, or certified letter--were applied without regard to the type of abnormality seen on a Pap smear or recommended examination.\nDocumented adherence to follow-up within 8 months of an abnormal result. Attempts to contact the patients for follow-up, adherence to follow-up recommendations, and patient characteristics were abstracted from medical records. The probability of adherence to follow-up vs the number of follow-up attempts was modeled with survival analysis. Cox proportional hazards models were used to examine multivariate relationships related to adherence.\nThe rate of overall adherence to follow-up recommendations was 56.0% (136/243). Adherence to a second colposcopy was not significantly different from that to a repeated Pap smear (odds ratio, 1.40; 95% confidence interval, 0.80-2.46). The use of as many as 3 patient reminders substantially improved adherence to follow-up. Women without insurance and women attending 1 of the 3 clinics were less likely to adhere to any follow-up recommendation (hazard ratio for no insurance, 0.43 [95% confidence interval, 0.20-0.93], and for clinic, 0.35 [95% confidence interval, 0.15-0.73]).\"\nQuestion:\n\"Do follow-up recommendations for abnormal Papanicolaou smears influence patient adherence?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28407529": {
                "source": [
                    "\"Patient outcome after resection of colorectal liver metastases (CLM) following second-line preoperative chemotherapy (PCT) performed for insufficient response or toxicity of the first-line, is little known and has here been compared to the outcome following first-line.\nFrom January 2005 to June 2013, 5624 and 791 consecutive patients of a prospective international cohort received 1 and 2 PCT lines before CLM resection (group 1 and 2, respectively). Survival and prognostic factors were analysed.\nAfter a mean follow-up of 30.1 months, there was no difference in survival from CLM diagnosis (median, 3-, and 5-year overall survival [OS]: 58.6 months, 76% and 49% in group 2 versus 58.9 months, 71% and 49% in group 1, respectively, P\u00a0=\u00a00.32). After hepatectomy, disease-free survival (DFS) was however shorter in group 2: 17.2 months, 27% and 15% versus 19.4 months, 32% and 23%, respectively (P\u00a0=\u00a00.001). Among the initially unresectable patients of group 1 and 2, no statistical difference in OS or DFS was observed. Independent predictors of worse OS in group 2 were positive primary lymph nodes, extrahepatic disease, tumour progression on second line, R2 resection\u00a0and number of hepatectomies/year<50. Positive primary nodes, synchronous and bilateral metastases were predictors of shorter DFS. Initial unresectability did not impact OS or DFS in group 2.\"\nQuestion:\n\"Resection of colorectal liver metastases after second-line chemotherapy: is it worthwhile?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23359100": {
                "source": [
                    "\"Heterotopic ossification is a common complication after total hip arthroplasty. Non-steroidal anti-inflammatory drugs (NSAIDs) are known to prevent heterotopic ossifications effectively, however gastrointestinal complaints are reported frequently. In this study, we investigated whether etoricoxib, a selective cyclo-oxygenase-2 (COX-2) inhibitor that produces fewer gastrointestinal side effects, is an effective alternative for the prevention of heterotopic ossification.\nWe investigated the effectiveness of oral etoricoxib 90 mg for seven days in a prospective two-stage study design for phase-2 clinical trials in a small sample of patients (n\u2009=\u200942). A cemented primary total hip arthroplasty was implanted for osteoarthritis. Six months after surgery, heterotopic ossification was determined on anteroposterior pelvic radiographs using the Brooker classification.\nNo heterotopic ossification was found in 62 % of the patients that took etoricoxib; 31 % of the patients had Brooker grade 1 and 7 % Brooker grade 2 ossification.\"\nQuestion:\n\"Is etoricoxib effective in preventing heterotopic ossification after primary total hip arthroplasty?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26449554": {
                "source": [
                    "\"The pressures delivered by autotitrating continuous positive airways pressure (CPAP) devices not only treat obstructive sleep apnoea (OSA) effectively but also give potentially interesting physiological information about the forces impinging on the pharynx. In earlier work from this unit, we used correlations between autoCPAP pressure and both OSA severity and obesity, to construct an algorithm to estimate the fixed CPAP pressure a patient required for subsequent clinical use. We wished to discover if these relationships could be reliably extended to a much more obese group.\nWe performed a prospective cohort study in an obese population. Measurements of obesity were made, OSA severity was recorded, and the 95th centile autoCPAP pressure was recorded during 1\u00a0week of autoCPAP. Spearman's rank correlation was performed between measurements of obesity and autoCPAP pressure, and between OSA severity and autoCPAP pressure.\nFifty-four obese individuals (median body mass index (BMI) 43.0\u00a0kg/m(2)), 52\u00a0% of whom had OSA (apnoea-hypopnoea index (AHI)\u2009\u2265\u200915), had a median 95th centile autoCPAP pressure of 11.8\u2009cmH2O. We found no significant correlation between autoCPAP pressure and neck circumference, waist circumference or BMI. There was a moderate correlation between autoCPAP pressure and OSA severity (AHI r\u2009=\u20090.34, p\u2009=\u20090.02; oxygen desaturation index (ODI) r\u2009=\u20090.48, p\u2009<\u20090.001).\"\nQuestion:\n\"Does either obesity or OSA severity influence the response of autotitrating CPAP machines in very obese subjects?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12769830": {
                "source": [
                    "\"Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\"\nQuestion:\n\"Should tumor depth be included in prognostication of soft tissue sarcoma?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24073931": {
                "source": [
                    "\"In recent years, many advances in pancreatic surgery have been achieved. Nevertheless, the rate of pancreatic fistula following pancreatic tail resection does not differ between various techniques, still reaching up to 30% in prospective multicentric studies. Taking into account contradictory results concerning the usefulness of covering resection margins after distal pancreatectomy, we sought to perform a systematic, retrospective analysis of patients that underwent distal pancreatectomy at our center.\nWe retrospectively analysed the data of 74 patients that underwent distal pancreatectomy between 2001 and 2011 at the community hospital in Neuss. Demographic factors, indications, postoperative complications, surgical or interventional revisions, and length of hospital stay were registered to compare the outcome of patients undergoing distal pancreatectomy with coverage of the resection margins vs. patients undergoing distal pancreatectomy without coverage of the resection margins. Differences between groups were calculated using Fisher's exact and Mann-Whitney U test.\nMain indications for pancreatic surgery were insulinoma (n=18, 24%), ductal adenocarcinoma (n=9, 12%), non-single-insulinoma-pancreatogenic-hypoglycemia-syndrome (NSIPHS) (n=8, 11%), and pancreatic cysts with pancreatitis (n=8, 11%). In 39 of 74 (53%) patients no postoperative complications were noted. In detail we found that 23/42 (55%) patients with coverage vs. 16/32 (50%) without coverage of the resection margins had no postoperative complications. The most common complications were pancreatic fistulas in eleven patients (15%), and postoperative bleeding in nine patients (12%). Pancreatic fistulas occurred in patients without coverage of the resection margins in 7/32 (22%) vs. 4/42 (1011%) with coverage are of the resection margins, yet without reaching statistical significance. Postoperative bleeding ensued with equal frequency in both groups (12% with coverage versus 13% without coverage of the resection margins). The reoperation rate was 8%. The hospital stay for patients without coverage was 13 days (5-60) vs. 17 days (8-60) for patients with coverage.\"\nQuestion:\n\"Is the covering of the resection margin after distal pancreatectomy advantageous?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18158048": {
                "source": [
                    "\"There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant.\nAn institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis.\nFifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.\"\nQuestion:\n\"Histologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10798511": {
                "source": [
                    "\"Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations.\nWe reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination.\nOf 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\"\nQuestion:\n\"Blunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9645785": {
                "source": [
                    "\"Changes in the spectrum of general surgery and the delivery of surgical care have placed the requirement for a mandatory general surgery rotation in the surgical clerkship in question.\nWe tested the hypothesis that equal mastery of surgical clerkship objectives can be obtained in a clerkship with and without general surgery. Students chose any two surgical rotations and were assessed by written examination, objective structured clinical examination (OSCE), ward evaluations, self-assessment objectives questionnaire, and satisfaction survey.\nData for 54 students showed no differences in scores between groups on any parameter. No specific concerns related to the absence of general surgery were identified.\"\nQuestion:\n\"Is a mandatory general surgery rotation necessary in the surgical clerkship?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26175531": {
                "source": [
                    "\"It is unclear whether intravenous glycoprotein IIb/IIIa inhibitors or ischemic time might modify any clinical benefits observed with aspiration thrombectomy before primary percutaneous coronary intervention (PCI) in patients with ST-segment-elevation myocardial infarction.\nElectronic databases were searched for trials that randomized ST-segment-elevation myocardial infarction patients to aspiration thrombectomy before PCI versus conventional PCI. Summary estimates were constructed using a DerSimonian-Laird model. Seventeen trials with 20\u2009960 patients were available for analysis. When compared with conventional PCI, aspiration thrombectomy was not associated with a significant reduction in the risk of mortality 2.8% versus 3.2% (risk ratio [RR], 0.89; 95% confidence interval [CI], 0.76-1.04; P=0.13), reinfarction 1.3% versus 1.4% (RR, 0.93; 95% CI, 0.73-1.17; P=0.52), the combined outcome of mortality or reinfarction 4.1% versus 4.6% (RR, 0.90; 95% CI, 0.79-1.02; P=0.11), or stent thrombosis 0.9% versus 1.2% (RR, 0.82; 95% CI, 0.62-1.08; P=0.15). Aspiration thrombectomy was associated with a nonsignificant increase in the risk of stroke 0.6% versus 0.4% (RR, 1.45; 95% CI, 0.96-2.21; P=0.08). Meta-regression analysis did not identify a difference for the log RR of mortality, reinfarction, and the combined outcome of mortality or reinfarction with intravenous glycoprotein IIb/IIIa inhibitors (P=0.17, 0.70, and 0.50, respectively) or with ischemic time (P=0.29, 0.66, and 0.58, respectively).\"\nQuestion:\n\"Is Aspiration Thrombectomy Beneficial in Patients Undergoing Primary Percutaneous Coronary Intervention?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11340218": {
                "source": [
                    "\"In primary and secondary prevention trials, statins have been shown to reduce the risk of stroke. In addition to lipid lowering, statins have a number of antiatherothrombotic and neuroprotective properties. In a preliminary observational study, we explored whether clinical outcome is improved in patients who are on treatment with statins when stroke occurs.\nWe conducted a population-based case-referent study of 25- to 74-year-old stroke patients with, for each case of a patient who was on statin treatment at the onset of stroke (n=125), 2 referent patients who were not treated with statins but were matched for age, gender, year of onset, and stroke subtype (n=250).\nThe unadjusted odds ratio for early discharge to home (versus late discharge or death) was 1.41 (95% CI 0.91 to 2.17) when patients on statin treatment were compared with referent stroke patients not on statins. Prognostic factors were, in general, more unfavorable among patients on statins. When this was adjusted for in a logistic regression model, the use of statins was a moderately strong but statistically nonsignificant predictor of discharge to home (multiple-adjusted odds ratio 1.42, 95% CI 0.90 to 2.22).\"\nQuestion:\n\"Does pretreatment with statins improve clinical outcome after stroke?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24487044": {
                "source": [
                    "\"We sought to determine the target populations and drug efficacy, toxicity, cost, and initiation age thresholds under which a pharmacologic regimen for knee osteoarthritis (OA) prevention could be cost-effective.\nWe used the Osteoarthritis Policy (OAPol) Model, a validated state-transition simulation model of knee OA, to evaluate the cost-effectiveness of using disease-modifying OA drugs (DMOADs) as prophylaxis for the disease. We assessed four cohorts at varying risk for developing OA: (1) no risk factors, (2) obese, (3) history of knee injury, and (4) high-risk (obese with history of knee injury). The base case DMOAD was initiated at age 50 with 40% efficacy in the first year, 5% failure per subsequent year, 0.22% major toxicity, and annual cost of $1,000. Outcomes included costs, quality-adjusted life expectancy (QALE), and incremental cost-effectiveness ratios (ICERs). Key parameters were varied in sensitivity analyses.\nFor the high-risk cohort, base case prophylaxis increased quality-adjusted life-years (QALYs) by 0.04 and lifetime costs by $4,600, and produced an ICER of $118,000 per QALY gained. ICERs>$150,000/QALY were observed when comparing the base case DMOAD to the standard of care in the knee injury only cohort; for the obese only and no risk factors cohorts, the base case DMOAD was less cost-effective than the standard of care. Regimens priced at $3,000 per year and higher demonstrated ICERs above cost-effectiveness thresholds consistent with current US standards.\"\nQuestion:\n\"Pharmacologic regimens for knee osteoarthritis prevention: can they be cost-effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21881325": {
                "source": [
                    "\"Recent studies have demonstrated that statins have pleiotropic effects, including anti-inflammatory effects and atrial fibrillation (AF) preventive effects. The objective of this study was to assess the efficacy of preoperative statin therapy in preventing AF after coronary artery bypass grafting (CABG).\n221 patients underwent CABG in our hospital from 2004 to 2007. 14 patients with preoperative AF and 4 patients with concomitant valve surgery were excluded from this study. Patients were divided into two groups to examine the influence of statins: those with preoperative statin therapy (Statin group, n = 77) and those without it (Non-statin group, n = 126). In addition, patients were divided into two groups to determine the independent predictors for postoperative AF: those with postoperative AF (AF group, n = 54) and those without it (Non-AF group, n = 149). Patient data were collected and analyzed retrospectively.\nThe overall incidence of postoperative AF was 26%. Postoperative AF was significantly lower in the Statin group compared with the Non-statin group (16% versus 33%, p = 0.005). Multivariate analysis demonstrated that independent predictors of AF development after CABG were preoperative statin therapy (odds ratio [OR]0.327, 95% confidence interval [CI] 0.107 to 0.998, p = 0.05) and age (OR 1.058, 95% CI 1.004 to 1.116, p = 0.035).\"\nQuestion:\n\"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26237424": {
                "source": [
                    "\"To evaluate the impact of patient-prosthesis mismatch (PPM) on survival, functional status, and quality of life (QoL) after aortic valve replacement (AVR) with small prosthesis size in elderly patients.\nBetween January 2005 and December 2013, 152 patients with pure aortic stenosis, aged at least 75 years, underwent AVR, with a 19 or 21\u200amm prosthetic heart valve. PPM was defined as an indexed effective orifice area less than 0.85\u200acm/m. Median age was 82 years (range 75-93 years). Mean follow-up was 56 months (range 1-82 months) and was 98% complete. Late survival rate, New York Heart Association functional class, and QoL (RAND SF-36) were assessed.\nOverall, PPM was found in 78 patients (53.8%). Among them, 42 patients (29%) had an indexed effective orifice area less than 0.75\u200acm/m and 17 less than 0.65\u200acm/m (11.7%). Overall survival at 5 years was 78\u200a\u00b1\u200a4.5% and was not influenced by PPM (P\u200a=\u200aNS). The mean New York Heart Association class for long-term survivors with PPM improved from 3.0 to 1.7 (P\u200a<\u200a0.001). QoL (physical functioning 45.18\u200a\u00b1\u200a11.35, energy/fatigue 49.36\u200a\u00b1\u200a8.64, emotional well being 58.84\u200a\u00b1\u200a15.44, social functioning 61.29\u200a\u00b1\u200a6.15) was similar to that of no-PPM patients (P\u200a=\u200aNS).\"\nQuestion:\n\"Does patient-prosthesis mismatch after aortic valve replacement affect survival and quality of life in elderly patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24684514": {
                "source": [
                    "\"Optimization of the preoperative hemoglobin (Hb) level is an effective way to reduce allogeneic transfusion in total knee arthroplasty (TKA) though the procedure is expensive, requires close monitoring and is often inconvenient for patients with reduced mobility. Our aim was to investigate the value of preoperative Hb levels to predict transfusion and thereby tailoring Hb optimization to patient characteristics.\nAll consecutive patients who undergone primary TKA in our center over 2\u00a0years, and received tranexamic acid intraoperatively, were reviewed. The adjusted association between preoperative Hb levels and transfusion was assessed by multivariate logistic regression, and the estimated probability of transfusion for individual patients was derived from the logistic model.\nOut of the 784 patients who meet the inclusion criteria, risk of transfusion was associated with poorer performance status, as measured by the America Association of Anestesiology (ASA) score III/IV (OR: 3\u00b73, P\u00a0<\u00a00\u00b7001) and lower preoperative Hb level (OR 3\u00b78 for each g/dl below 13\u00a0g/dl; P\u00a0<\u00a00\u00b7001). According to the Hb level, the estimated probability of transfusion was 0\u00b703 (range: 0\u00b703-0\u00b764) for ASA I/II patients and 0\u00b710 (range: 0\u00b710-0\u00b784) for ASA III/IV.\"\nQuestion:\n\"Should all patients be optimized to the same preoperative hemoglobin level to avoid transfusion in primary knee arthroplasty?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "7860319": {
                "source": [
                    "\"We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/\nHealth Care Financing Administration (HCFA) hospital mortality data for FY 1989.\nUsing hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality.\nFor the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes.\"\nQuestion:\n\"Measuring hospital mortality rates: are 30-day data enough?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12684740": {
                "source": [
                    "\"Alcohol-associated cues elicit craving in human addicts but little is known about craving mechanisms. Current animal models focus on relapse and this may confound the effect of environmental cues. OBJECTIVES. To develop a model to study the effects of environmental cues on alcohol consumption in animals not experiencing withdrawal or relapse.\nRats were trained to orally self-administer an alcohol (5% w/v)/saccharin (0.2%) solution 30 min a day for 20 days. After stable responding on a free choice between alcohol/saccharin and water, rats were exposed to 5, 10 or 15 min of alcohol-associated cues or 5 min of non-alcohol associated cues. The effect of a 5-min cue was measured after a 10-day break from training or pre-treatment with 0.03, 0.1 or 1 mg/kg naltrexone.\nRats given 5 min of alcohol-associated cues responded significantly more on the active lever (26% increase) and consumed more alcohol as verified by increased blood alcohol levels (8.9 mM versus control 7.5 mM). Ten or 15 min of cues did not change alcohol consumption and 5 min in a novel environment decreased response by 66%. After a 10-day break in training, 5 min of alcohol-associated cues still increased alcohol consumption (29% increase) and the cue effect could be dose-dependently blocked by naltrexone (143% decrease at 0.03 mg/kg).\"\nQuestion:\n\"Cue-induced behavioural activation: a novel model of alcohol craving?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20538207": {
                "source": [
                    "\"It is generally considered that kidney grafts should be preserved at 4 degrees C during cold storage. However, actual temperature conditions are not known. We decided to study the temperature levels during preservation with the Biotainer storage can and Vitalpack transport pack.\nTemperature was monitored using the Thermobouton probe during preservation of pig kidneys, in the same conditions used with human grafts. The probe recorded the temperature level every 10 minutes during four days. We compared the results found with the new storage can with results obtained in the same conditions with the storage can formerly used by our team. We also studied the best position of the probe for temperature monitoring and the influence of the amount of ice within the transport pack on the temperature level. We then monitored the temperature during the conservation of actual human kidney grafts harvested at our institution from August 2007 to May 2008.\nThe temperature levels were the same regardless of the position of the probe within the transport pack. The lowest temperature was maintained during 15 hours, and the temperature level stayed below 5 degrees C for 57 hours with the new storage can. The former storage can maintained the lowest temperature level for 80 minutes, and temperature reached 5 degrees C after 10 hours 40 minutes. Temperature levels were similar when 2 or 4 kg of crushed ice were used. We observed similar results when monitoring the conservation of human grafts.\"\nQuestion:\n\"Should temperature be monitorized during kidney allograft preservation?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18322741": {
                "source": [
                    "\"Atrial fibrillation, which occurs in 12% of all major foregut surgeries, can prolong hospital stay and increase morbidity. Minimally invasive techniques in foregut surgery have been suggested to cause less tissue trauma. We examined the factors associated with new-onset atrial fibrillation after foregut surgery at our institution.\nWe retrospectively examined the records of 154 adult patients who underwent major foregut surgery which included esophagectomy, partial or total gastrectomy, redo Heller myotomy, redo or transthoracic fundoplications. Univariate and multivariate logistic regression analysis with standard modeling techniques were performed to determine risk factors for new-onset atrial fibrillation.\nOf the 154 patients, 14 patients developed new-onset atrial fibrillation with a higher mean age of 67.1 years (+/-8.8 years) versus 56.4 years (+/-14.1 years) (p = 0.006). Laparoscopic (p = 0.004) and nonthoracic surgeries (p = 0.01) were associated with lower risk of atrial fibrillation. Patients with atrial fibrillation had received more fluid (6.5 +/- 2.8 liters versus 5.3 +/- 2.0 liters) and had longer operations (370 +/- 103 min versus 362 +/- 142 min), none of which were statistically significant. The average intensive care length of stay of patients was longer: 7.5 +/- 6.8 days versus 4.0 +/- 7.1 days (p = 0.004). Multivariate analysis revealed an association of atrial fibrillation with age (OR 1.08, 95% CI 1.02-1.14, p = 0.01), and laparoscopic surgery (OR 0.09, 95% CI 0.01-0.95, p = 0.04) after adjusting for surgery type.\"\nQuestion:\n\"Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "16418930": {
                "source": [
                    "\"Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\n100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\nDifferences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\"\nQuestion:\n\"Landolt C and snellen e acuity: differences in strabismus amblyopia?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24267613": {
                "source": [
                    "\"In this prospective non randomized observational cohort study we have evaluated the influence of age on outcome of laparoscopic total fundoplication for GERD.\nSix hundred and twenty consecutive patients underwent total laparoscopic fundoplication for GERD. Five hundred and twenty-four patients were younger than 65 years (YG), and 96 patients were 65 years or older (EG). The following parameters were considered in the preoperative and postoperative evaluation: presence, duration, and severity of GERD symptoms, presence of a hiatal hernia, manometric and 24 hour pH-monitoring data, duration of operation, incidence of complications and length of hospital stay.\nElderly patients more often had atypical symptoms of GERD and at manometric evaluation had a higher rate of impaired esophageal peristalsis in comparison with younger patients. The duration of the operation was similar between the two groups. The incidence of intraoperative and postoperative complications was low and the difference was not statistically significant between the two groups. An excellent outcome was observed in 93.0% of young patients and in 88.9% of elderly patients (p = NS).\"\nQuestion:\n\"Is the advanced age a contraindication to GERD laparoscopic surgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18568239": {
                "source": [
                    "\"To evaluate the influence of the urologist's experience on the surgical results and complications of transurethral resection of the prostate (TURP).\nSixty-seven patients undergoing transurethral resection of the prostate without the use of a video camera were randomly allocated into three groups according to the urologist's experience: a urologist having done 25 transurethral resections of the prostate (Group I - 24 patients); a urologist having done 50 transurethral resections of the prostate (Group II - 24 patients); a senior urologist with vast transurethral resection of the prostate experience (Group III - 19 patients). The following were recorded: the weight of resected tissue, the duration of the resection procedure, the volume of irrigation used, the amount of irrigation absorbed and the hemoglobin and sodium levels in the serum during the procedure.\nThere were no differences between the groups in the amount of irrigation fluid used per operation, the amount of irrigation fluid absorbed or hematocrit and hemoglobin variation during the procedure. The weight of resected tissue per minute was approximately four times higher in group III than in groups I and II. The mean absorbed irrigation fluid was similar between the groups, with no statistical difference between them (p=0.24). Four patients (6%) presented with TUR syndrome, without a significant difference between the groups.\"\nQuestion:\n\"Is the ability to perform transurethral resection of the prostate influenced by the surgeon's previous experience?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18496363": {
                "source": [
                    "\"To characterize the gender dimorphism after injury with specific reference to the reproductive age of the women (young,<48 yrs of age, vs. old,>52 yrs of age) in a cohort of severely injured trauma patients for which significant variation in postinjury care is minimized.\nSecondary data analysis of an ongoing prospective multicenter cohort study.\nAcademic, level I trauma and intensive care unit centers.\nBlunt-injured adults with hemorrhagic shock.\nNone.\nSeparate Cox proportional hazard regression models were formulated based on all patients to evaluate the effects of gender on mortality, multiple organ failure, and nosocomial infection, after controlling for all important confounders. These models were then used to characterize the effect of gender in young and old age groups. Overall mortality, multiple organ failure, and nosocomial infection rates for the entire cohort (n = 1,036) were 20%, 40%, and 45%, respectively. Mean Injury Severity Score was 32 +/- 14 (mean +/- SD). Men (n = 680) and women (n = 356) were clinically similar except that men required higher crystalloid volumes, more often had a history of alcoholism and liver disease, and had greater ventilatory and intensive care unit requirements. Female gender was independently associated with a 43% and 23% lower risk of multiple organ failure and nosocomial infection, respectively. Gender remained an independent risk factor in young and old subgroup analysis, with the protection afforded by female gender remaining unchanged.\"\nQuestion:\n\"Characterization of the gender dimorphism after injury and hemorrhagic shock: are hormonal differences responsible?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26104852": {
                "source": [
                    "\"Hereditary transthyretin (ATTR) amyloidosis with increased left ventricular wall thickness could easily be misdiagnosed by echocardiography as hypertrophic cardiomyopathy (HCM). Our aim was to create a diagnostic tool based on echocardiography and ECG that could optimise identification of ATTR amyloidosis.\nData were analysed from 33 patients with biopsy proven ATTR amyloidosis and 30 patients with diagnosed HCM. Conventional features from ECG were acquired as well as two dimensional and Doppler echocardiography, speckle tracking derived strain and tissue characterisation analysis. Classification trees were used to select the most important variables for differentiation between ATTR amyloidosis and HCM.\nThe best classification was obtained using both ECG and echocardiographic features, where a QRS voltage>30\u2009mm was diagnostic for HCM, whereas in patients with QRS voltage<30\u2009mm, an interventricular septal/posterior wall thickness ratio (IVSt/PWt)>1.6 was consistent with HCM and a ratio<1.6 supported the diagnosis of ATTR amyloidosis. This classification presented both high sensitivity (0.939) and specificity (0.833).\"\nQuestion:\n\"Can echocardiography and ECG discriminate hereditary transthyretin V30M amyloidosis from hypertrophic cardiomyopathy?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17565137": {
                "source": [
                    "\"To evaluate the effect of an antismoking advertisement on young people's perceptions of smoking in movies and their intention to smoke.SUBJECTS/\n3091 cinema patrons aged 12-24 years in three Australian states; 18.6% of the sample (n = 575) were current smokers.DESIGN/\nQuasi-experimental study of patrons, surveyed after having viewed a movie. The control group was surveyed in week 1, and the intervention group in weeks 2 and 3. Before seeing the movie in weeks 2 and 3, a 30 s antismoking advertisement was shown, shot in the style of a movie trailer that warned patrons not to be sucked in by the smoking in the movie they were about to see.\nAttitude of current smokers and non-smokers to smoking in the movies; intention of current smokers and non-smokers to smoke in 12 months.\nAmong non-smokers, 47.8% of the intervention subjects thought that the smoking in the viewed movie was not OK compared with 43.8% of the control subjects (p = 0.04). However, there was no significant difference among smokers in the intervention (16.5%) and control (14.5%) groups (p = 0.4). A higher percentage of smokers in the intervention group indicated that they were likely to be smoking in 12 months time (38.6%) than smokers in the control group (25.6%; p<0.001). For non-smokers, there was no significant difference in smoking intentions between groups, with 1.2% of intervention subjects and 1.6% of controls saying that they would probably be smoking in 12 months time (p = 0.54).\"\nQuestion:\n\"Out of the smokescreen II: will an advertisement targeting the tobacco industry affect young people's perception of smoking in movies and their intention to smoke?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26548832": {
                "source": [
                    "\"Longitudinally following patients requires a full-time employee (FTE)-dependent data inflow infrastructure. There are efforts to capture patient-reported outcomes (PROs) by the use of non-FTE-dependent methodologies. In this study, we set out to assess the reliability of PRO data captured via FTE-dependent compared with non-FTE-dependent methodologies.\nA total of 119 adult patients (65 men) who underwent 1-and 2-level lumbar fusions at Duke University Medical Center were enrolled in this prospective study. Enrollment criteria included available demographic, clinical, and PRO data. All patients completed 2 sets of questionnaires--the first a phone interviews and the second a self-survey. There was at least a 2-week period between the phone interviews and self-survey. Questionnaires included the Oswestry Disability Index (ODI), the visual analog scale for back pain (VAS-BP), and the visual analog scale for leg pain (VAS-LP). Repeated-measures analysis of variance was used to compare the reliability of baseline PRO data captured.\nA total of 39.49% of patients were smokers, 21.00% had diabetes, and 11.76% had coronary artery disease; 26.89% reported history of anxiety disorder, and 28.57% reported history of depression. A total of 97.47% of patients had a high-school diploma or General Education Development, and 49.57% attained a 4-year college degree or postgraduate degree. We observed a high correlation between baseline PRO data captured between FTE-dependent versus non-FTE dependent methodologies (ODI: r = -0.89, VAS-BP: r = 0.74, VAS-LP: r = 0.70). There was no difference in PROs of baseline pain and functional disability between FTE-dependent and non-FTE-dependent methodologies: baseline ODI (FTE-dependent: 47.73 \u00b1 16.77 [mean \u00b1 SD] vs. non-FTE-dependent: 45.81 \u00b1 12.11, P = 0.39), VAS-LP (FTE-dependent: 6.13 \u00b1 2.78 vs. non-FTE-dependent: 6.46 \u00b1 2.79, P = 0.36) and VAS-BP (FTE-dependent: 6.33 \u00b1 2.90 vs. non-FTE-dependent: 6.53 \u00b1 2.48, P = 0.57).\"\nQuestion:\n\"Assessing Patient Reported Outcomes Measures via Phone Interviews Versus Patient Self-Survey in the Clinic: Are We Measuring the Same Thing?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19100463": {
                "source": [
                    "\"Tacrolimus is a potent immunosuppressive drug used in organ transplantation. Because of its substantial toxic effects, narrow therapeutic index, and interindividual pharmacokinetic variability, therapeutic drug monitoring of whole-blood tacrolimus concentrations has been recommended. We investigated the comparability of the results of 2 immunoassay systems, affinity column-mediated immunoassay (ACMIA) and microparticle enzyme immunoassay (MEIA), comparing differences in the tacrolimus concentrations measured by the 2 methods in relation to the hematologic and biochemical values of hepatic and renal functions.\nA total of 154 samples from kidney or liver transplant recipients were subjected to Dimension RxL HM with a tacrolimus Flex reagent cartilage for the ACMIA method and IMx tacrolimus II for the MEIA method.\nTacrolimus concentrations measured by the ACMIA method (n = 154) closely correlated with those measured by the MEIA method (r = 0.84). The Bland-Altman plot using concentration differences between the 2 methods and the average of the 2 methods showed no specific trends. The tacrolimus levels determined by both the MEIA method and the ACMIA method were not influenced by hematocrit levels, but the difference between the 2 methods (ACMIA - MEIA) tended to be larger in low hematocrit samples (P<.001).\"\nQuestion:\n\"Is the affinity column-mediated immunoassay method suitable as an alternative to the microparticle enzyme immunoassay method as a blood tacrolimus assay?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24481006": {
                "source": [
                    "\"79 adjacent proximal surfaces without restorations in permanent teeth were examined. Patients suspected to have carious lesions after a visual clinical and a bitewing examination participated in a CBCT examination (Kodak 9000 3D, 5 \u00d7 3.7 cm field of view, voxel size 0.07 mm). Ethical approval and informed consent were obtained according to the Helsinki Declaration. Radiographic assessment recording lesions with or without cavitation was performed by two observers in bitewings and CBCT sections. Orthodontic separators were placed interdentally between two lesion-suspected surfaces. The separator was removed after 3 days and the surfaces recorded as cavitated (yes/no), i.e. validated clinically. Differences between the two radiographic modalities (sensitivity, specificity and overall accuracy) were estimated by analyzing the binary data in a generalized linear model.\nFor both observers, sensitivity was significantly higher for CBCT than for bitewings (average difference 33%, p<0.001) while specificity was not significantly different between the methods (p = 0.19). The overall accuracy was also significantly higher for CBCT (p<0.001).\"\nQuestion:\n\"Should cavitation in proximal surfaces be reported in cone beam computed tomography examination?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17598882": {
                "source": [
                    "\"A genetic component is well established in the etiology of breast cancer. It is not well known, however, whether genetic traits also influence prognostic features of the malignant phenotype.\nWe carried out a population-based cohort study in Sweden based on the nationwide Multi-Generation Register. Among all women with breast cancer diagnosed from 1961 to 2001, 2,787 mother-daughter pairs and 831 sister pairs with breast cancer were identified; we achieved complete follow-up and classified 5-year breast cancer-specific prognosis among proband (mother or oldest sister) into tertiles as poor, intermediary, or good. We used Kaplan-Meier estimates of survival proportions and Cox models to calculate relative risks of dying from breast cancer within 5 years depending on the proband's outcome.\nThe 5-year survival proportion among daughters whose mothers died within 5 years was 87% compared to 91% if the mother was alive (p = 0.03). Among sisters, the corresponding proportions were 70% and 88%, respectively (p = 0.001). After adjustment for potential confounders, daughters and sisters of a proband with poor prognosis had a 60% higher 5-year breast cancer mortality compared to those of a proband with good prognosis (hazard ratio [HR], 1.6; 95% confidence interval [CI], 1.2 to 2.2; p for trend 0.002). This association was slightly stronger among sisters (HR, 1.8; 95% CI, 1.0 to 3.4) than among daughters (HR, 1.6; 95% CI, 1.1 to 2.3).\"\nQuestion:\n\"Is breast cancer prognosis inherited?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23414523": {
                "source": [
                    "\"The potential effects of binge drinking during pregnancy on child motor function have only been assessed in a few, small studies. We aimed to examine the effects of binge alcohol consumption during early pregnancy, including number of binge episodes and timing of binge drinking, on child motor function at age 5.\nWe performed a prospective follow-up study of 678 women and their children sampled from the Danish National Birth Cohort based on maternal alcohol consumption during pregnancy. At 5 years of age, the children were tested with the Movement Assessment Battery for Children. Parental education, maternal IQ, prenatal maternal smoking, the child's age at testing, sex of child, and tester were considered core confounders, while the full model also controlled for prenatal maternal average alcohol intake, maternal age and prepregnancy body mass index, parity, home environment, postnatal parental smoking, health status, participation in organized sport, and indicators for hearing and vision impairment.\nThere were no systematic or significant differences in motor function between children of mothers reporting isolated episodes of binge drinking and children of mothers with no binge episodes. No association was observed with respect to the number of binge episodes (maximum of 12) and timing of binge drinking.\"\nQuestion:\n\"Does binge drinking during early pregnancy increase the risk of psychomotor deficits?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22303473": {
                "source": [
                    "\"Family caregivers of dementia patients are at increased risk of developing depression or anxiety. A multi-component program designed to mobilize support of family networks demonstrated effectiveness in decreasing depressive symptoms in caregivers. However, the impact of an intervention consisting solely of family meetings on depression and anxiety has not yet been evaluated. This study examines the preventive effects of family meetings for primary caregivers of community-dwelling dementia patients.\nA randomized multicenter trial was conducted among 192 primary caregivers of community dwelling dementia patients. Caregivers did not meet the diagnostic criteria for depressive or anxiety disorder at baseline. Participants were randomized to the family meetings intervention (n\u200a=\u200a96) or usual care (n\u200a=\u200a96) condition. The intervention consisted of two individual sessions and four family meetings which occurred once every 2 to 3 months for a year. Outcome measures after 12 months were the incidence of a clinical depressive or anxiety disorder and change in depressive and anxiety symptoms (primary outcomes), caregiver burden and quality of life (secondary outcomes). Intention-to-treat as well as per protocol analyses were performed.\nA substantial number of caregivers (72/192) developed a depressive or anxiety disorder within 12 months. The intervention was not superior to usual care either in reducing the risk of disorder onset (adjusted IRR 0.98; 95% CI 0.69 to 1.38) or in reducing depressive (randomization-by-time interaction coefficient\u200a=\u200a-1.40; 95% CI -3.91 to 1.10) or anxiety symptoms (randomization-by-time interaction coefficient\u200a=\u200a-0.55; 95% CI -1.59 to 0.49). The intervention did not reduce caregiver burden or their health related quality of life.\"\nQuestion:\n\"Does a family meetings intervention prevent depression and anxiety in family caregivers of dementia patients?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12040336": {
                "source": [
                    "\"The role of early revascularization among patients with acute myocardial infarction complicated by cardiogenic shock remains controversial. Angioplasty registries, while suggesting a benefit, are subject to selection bias, and clinical trials have been underpowered to detect early benefits. If an invasive strategy is beneficial in this population, patients admitted to hospitals with onsite coronary revascularization might be expected to have a better prognosis. We sought to determine whether access to cardiovascular resources at the admitting hospital influenced the prognosis of patients with acute myocardial infarction complicated by cardiogenic shock.\nBy use of the Cooperative Cardiovascular Project database (a retrospective medical record review of Medicare patients discharged with acute myocardial infarction), we identified patients aged>or =65 years whose myocardial infarction was complicated by cardiogenic shock.\nOf the 601 patients with cardiogenic shock, 287 (47.8%) were admitted to hospitals without revascularization services and 314 (52.2%) were admitted to hospitals with coronary angioplasty and coronary artery bypass surgery facilities. Clinical characteristics were similar across the subgroups. Patients admitted to hospitals with revascularization services were more likely to undergo coronary revascularization during the index hospitalization and during the first month after acute myocardial infarction. After adjustment for demographic, clinical, hospital, and treatment strategies, the presence of onsite revascularization services was not associated with a significantly lower 30-day (odds ratio 0.83, 95% CI 0.47, 1.45) or 1-year mortality (odds ratio 0.91, 95% CI 0.49, 1.72).\"\nQuestion:\n\"Cardiogenic shock complicating acute myocardial infarction in elderly patients: does admission to a tertiary center improve survival?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27615402": {
                "source": [
                    "\"Parental drinking has been shown to be associated with offspring drinking. However, the relationship appears to be more complex than often assumed and few studies have tracked it over longer time periods.\nTo explore the long-term (10-year) transmission of familial drinking during adolescence to offspring drinking patterns in young adulthood.\nSwedish longitudinal study, assessing the relationship between familial drinking in 2000 and offspring drinking in 2010 using simultaneous quantile regression analysis (n=744).DATA: Data on familial drinking was gathered from the Swedish level-of-living surveys (LNU) and from partner LNU in 2000 while data on offspring drinking in young adulthood was gathered from LNU 2010. Drinking among offspring, parents and potential stepparents was measured through identical quantity-frequency indices referring to the past 12 months in 2010 and 2000 respectively.\nYoung adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of non-abstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. Supplementary analyses confirmed these patterns.\"\nQuestion:\n\"Does the familial transmission of drinking patterns persist into young adulthood?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21164063": {
                "source": [
                    "\"A possible role for fondaparinux as a bridging agent in the perioperative setting is explored.\nAnticoagulation guidelines provide minimal direction on the perioperative use of fondaparinux. Fondaparinux's extended half-life of 17-21 hours complicates its use as a perioperative bridging therapy. The ideal time for discontinuation before surgery is an issue, particularly in surgeries with a high bleeding risk or in which neuraxial anesthesia is used. Guidance for perioperative bridging with fondaparinux must be derived from pharmacokinetic data, surgical prophylaxis trials, case reports, and anesthesia guidelines. Published trials used fondaparinux sodium 2.5 mg daily for venous thromboembolism prophylaxis in surgical patients, and the majority avoided its use before surgery in patients receiving neuraxial anesthesia. Three case reports cited the use of fondaparinux sodium as perioperative bridge therapy; one used a 2.5-mg dose, and the other two used a full treatment dose of 7.5 mg. Furthermore, professional anesthesia guidelines conflict in their recommendations regarding the timing of drug administration with neuraxial catheter use. For these reasons, it may be optimal to avoid fondaparinux use before surgery. In some instances, the use of low-molecular-weight heparin or inpatient use of i.v. unfractionated heparin is not possible, is contraindicated, or has limited efficacy, such as a patient with history of heparin-induced thrombocytopenia or antithrombin III deficiency. Fondaparinux may have a role in bridge therapy for these patients.\"\nQuestion:\n\"Is there a role for fondaparinux in perioperative bridging?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20577124": {
                "source": [
                    "\"Hyperleptinemia and oxidative stress play a major role in the development of cardiovascular diseases in obesity. This study aimed to investigate whether there is a relationship between plasma levels of leptin and phagocytic nicotinamide adenine dinucleotide phosphate (NADPH) oxidase activity, and its potential relevance in the vascular remodeling in obese patients.\nThe study was performed in 164 obese and 94 normal-weight individuals (controls). NADPH oxidase activity was evaluated by luminescence in phagocytic cells. Levels of leptin were quantified by ELISA in plasma samples. Carotid intima-media thickness (cIMT) was measured by ultrasonography. In addition, we performed in-vitro experiments in human peripheral blood mononuclear cells and murine macrophages.\nPhagocytic NADPH oxidase activity and leptin levels were enhanced (P<0.05) in obese patients compared with controls. NADPH oxidase activity positively correlated with leptin in obese patients. This association remained significant in a multivariate analysis. cIMT was higher (P<0.05) in obese patients compared with controls. In addition, cIMT also correlated positively with leptin and NADPH oxidase activity in obese patients. In-vitro studies showed that leptin induced NADPH oxidase activation. Inhibition of the leptin-induced NADPH oxidase activity by wortmannin and bisindolyl maleimide suggested a direct involvement of the phosphatidylinositol 3-kinase and protein kinase C pathways, respectively. Finally, leptin-induced NADPH oxidase activation promoted macrophage proliferation.\"\nQuestion:\n\"Is leptin involved in phagocytic NADPH oxidase overactivity in obesity?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15539888": {
                "source": [
                    "\"The atopy patch test (APT), namely the patch test with aeroallergens, is regarded as specific for patients with atopic dermatitis (AD), but small numbers of positive APT were reported in the past also in atopic subjects without dermatitis and in healthy persons.\nThe aim of this study was to evaluate the response to the APT with house dust mites (HDM) in subjects nonaffected by AD and to compare the outcomes observed in these cases with those pointed out in AD patients, evaluating also the differences between two allergen extracts manufactured at different purifications and concentrations.\nForty-seven atopic subjects without eczema (AWE), 33 nonatopic (NA) subjects and 77 adult AD patients were patch tested with an extract of purified bodies of HDM at 20% and with another extract of whole bodies of HDM at 30%, the latter corresponding to 300 microg/g of Der p 1. The reproducibility of APT was also tested in 8 AD patients, in 37 AWE subjects and in 19 NA subjects.\nPositive responses with extract at 20% were observed in 29 (37.7%) AD, in 5 (10.6%) AWE and in 4 (12.1%) NA subjects. The APT with HDM at 30% was positive in 32 (41.6%) AD, 9 (19.1%) AWE and 4 (12.1%) NA persons. The rates of positivity and the intensity scores of responses were significantly different between AD and non-AD subjects (p<0.01). The reproducibility of the APT in the three groups was satisfactory.\"\nQuestion:\n\"Is the atopy patch test with house dust mites specific for atopic dermatitis?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "25304452": {
                "source": [
                    "\"The gluten-free diet has traditionally been accepted as a healthy diet, but there are articles advocating that it may have some nutritional deficiencies. The current study assesses whether there was any change in the contributions of calories, essential elements, proportion of fatty acids, vitamins, minerals and fiber in children who were diagnosed with celiac diseases, comparing the diet with gluten prior one year after diagnosis with the diet without gluten to the year of diagnosis. The level of clinical or analytical impact that nutritional deficits could have was also assessed.\nA prospective,descriptive, observational study in which information was collected from a dietary survey, anthropometric and analytical data at pre-diagnosis of celiac disease and following a gluten diet and one year after celiac disease diagnosis, under gluten-free diet.\nA total of 37 patients meet the study criteria. A decrease in the intake of saturated fatty acids was found, with an increase of monounsaturated fatty acids and an increase in the intake of phosphorus in the diet without gluten. A deficient intake of vitamin D was found in both diets. Clinically, at year of gluten-free diet there was an improvement in weight and size. Analytically, there was an improvement in hemoglobin, ferritin, vitamin D, and parathyroid hormone in plasma.\"\nQuestion:\n\"Nutritional assessment of gluten-free diet. Is gluten-free diet deficient in some nutrient?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23806388": {
                "source": [
                    "\"To examine the ability of various postoperative nomograms to predict prostate cancer-specific mortality (PCSM) and to validate that they could predict aggressive biochemical recurrence (BCR). Prostate-specific antigen (PSA), grade, and stage are the classic triad used to predict BCR after radical prostatectomy (RP). Multiple nomograms use these to predict risk of BCR. A previous study showed that several nomograms could predict aggressive BCR (prostate-specific antigen doubling time [PSADT]\u00a0<9 months) more accurately than BCR. However, it remains unknown if they can predict more definitive endpoints, such as PCSM.\nWe performed Cox analyses to examine the ability of 4 postoperative nomograms, the Duke Prostate Center (DPC) nomogram, the Kattan postoperative nomogram, the Johns Hopkins Hospital (JHH) nomogram, and the joint Center for Prostate Disease Research(CPDR)/Cancer of the Prostate Strategic Urologic Research Endeavor (CaPSURE) nomogram to predict BCR and PCSM among 1778 men in the Shared Equal Access Regional Cancer Hospital (SEARCH) database who underwent RP between 1990 and 2009. We also compared their ability to predict BCR and aggressive BCR in a subset of men. We calculated the c-index for each nomogram to determine its predictive accuracy for estimating actual outcomes.\nWe found that each nomogram could predict aggressive BCR and PCSM in a statistically significant manner and that they all predicted PCSM more accurately than they predicted BCR (ie, with higher c-index values).\"\nQuestion:\n\"Do nomograms designed to predict biochemical recurrence (BCR) do a better job of predicting more clinically relevant prostate cancer outcomes than BCR?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "18802997": {
                "source": [
                    "\"Assessing the clinical course of inflammatory bowel disease (IBD) patients consists of periodical clinical evaluations and laboratory tests. We aimed to assess the role of calprotectin tests in predicting clinical relapse in IBD patients.\nNinety-seven patients with ulcerative colitis (UC) and 65 with Crohn's disease (CD) in clinical remission were prospectively included in the study. A 10-g stool sample was collected for calprotectin assay. The cutoff level was set at 130 mg/kg of feces. Patients were followed up for 1 yr after the test or until relapse. The cumulative proportion of relapses was estimated by the Kaplan-Meier analysis. Statistics for equality of survival distribution were tested using the log-rank test.\nThe calprotectin test was positive in 44 UC patients and 26 of them relapsed within a year, while 11 of 53 UC patients with a negative calprotectin test relapsed within the same time frame. Thirty CD patients had a positive calprotectin test and 13 of them relapsed within a year, as did 7 of the 35 with a negative test result. A significant correlation emerged between a positive calprotectin test and the probability of relapse in UC patients (P= 0.000). In CD patients, only cases of colonic CD showed a significant correlation between a positive calprotectin test and the probability of relapse, i.e., 6 colonic CD patients were positive for the calprotectin test and 4 relapsed (P= 0.02).\"\nQuestion:\n\"Can calprotectin predict relapse risk in inflammatory bowel disease?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22491528": {
                "source": [
                    "\"To determine if composite measures based on process indicators are consistent with short-term outcome indicators in surgical colorectal cancer care.\nLongitudinal analysis of consistency between composite measures based on process indicators and outcome indicators for 85 Dutch hospitals.\nThe Dutch Surgical Colorectal Audit database, the Netherlands.\n4732 elective patients with colon carcinoma and 2239 with rectum carcinoma treated in 85 hospitals were included in the analyses.\nAll available process indicators were aggregated into five different composite measures. The association of the different composite measures with risk-adjusted postoperative mortality and morbidity was analysed at the patient and hospital level.\nAt the patient level, only one of the composite measures was negatively associated with morbidity for rectum carcinoma. At the hospital level, a strong negative association was found between composite measures and hospital mortality and morbidity rates for rectum carcinoma (p<0.05), and hospital morbidity rates for colon carcinoma.\"\nQuestion:\n\"Combining process indicators to evaluate quality of care for surgical patients with colorectal cancer: are scores consistent with short-term outcome?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17462393": {
                "source": [
                    "\"Beating-heart valve surgery appears to be a promising technique for protection of hypertrophied hearts. Normothermic normokalemic simultaneous antegrade/retrograde perfusion (NNSP) may improve myocardial perfusion. However, its effects on myocardial oxygenation and energy metabolism remain unclear. The present study was to determine whether NNSP improved myocardial oxygenation and energy metabolism of hypertrophied hearts relative to normothermic normokalemic antegrade perfusion (NNAP).\nTwelve hypertrophied pig hearts underwent a protocol consisting of three 20-minute perfusion episodes (10 minutes NNAP and 10 minutes NNSP in a random order) with each conducted at a different blood flow in the left anterior descending coronary artery (LAD [100%, 50%, and 20% of its initial control]). Myocardial oxygenation was assessed using near-infrared spectroscopic imaging. Myocardial energy metabolism was monitored using localized phosphorus-31 magnetic resonance spectroscopy.\nWith 100% LAD flow, both NNAP and NNSP maintained myocardial oxygenation, adenosine triphosphate, phosphocreatine, and inorganic phosphate at normal levels. When LAD flow was reduced to 50% of its control level, NNSP resulted in a small but significant decrease in myocardial oxygenation and phosphocreatine, whereas those measurements did not change significantly during NNAP. With LAD flow further reduced to 20% of its control level, both NNAP and NNSP caused a substantial decrease in myocardial oxygenation, adenosine triphosphate, and phosphocreatine with an increase in inorganic phosphate. However, the changes were significantly greater during NNSP than during NNAP.\"\nQuestion:\n\"Does normothermic normokalemic simultaneous antegrade/retrograde perfusion improve myocardial oxygenation and energy metabolism for hypertrophied hearts?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23149821": {
                "source": [
                    "\"A higher prevalence of cardiovascular risk factors (CRFs) in HIV-infected patients, together with chronic infection and treatments, has resulted in an increased risk of silent myocardial ischaemia (SMI). The objective of this study was to evaluate whether myocardial SPECT should be used for screening HIV-infected patients with no clinical symptoms of coronary artery disease.\nThe prevalence of SMI detected by myocardial SPECT was determined in 94 HIV-infected patients with a normal clinical cardiovascular examination in relation to anthropomorphic parameters, CRFs, inflammatory and HIV infection status, and treatment.\nCoronary artery disease was detected in nine patients (eight with ischaemia, one with myocardial infarction), corresponding to 9.6 % positivity. All but two of the scintigraphic diagnoses of ischaemia were confirmed by coronarography. Univariate analysis revealed that the overall number of CRFs and the combination of gender and age were associated with a diagnosis of SMI (p<0.05). According to multivariate analysis, the only independent parameter significantly associated with the scintigraphic diagnosis of SMI was the combination of gender and age (p = 0.01). All the positive myocardial SPECT scans were in men older than 52 years with at least two other CRFs. In this subpopulation of 47 patients, the prevalence of SMI detected by myocardial SPECT reached 19.2 %.\"\nQuestion:\n\"Should HIV-infected patients be screened for silent myocardial ischaemia using gated myocardial perfusion SPECT?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "28056802": {
                "source": [
                    "\"It has recently been shown that non-high density lipoprotein cholesterol (non-HDL-C) may be a better predictor of cardiovascular risk than low density lipoprotein cholesterol (LDL-C). Based on known ethic differences in lipid parameters and cardiovascular risk prediction, we sought to study the predictability of attaining non-HDL-C target and long-term major adverse cardiovascular event (MACE) in Thai patients after acute myocardial infarction (AMI) compared to attaining LDL-C target.\nWe retrospectively obtained the data of all patients who were admitted at Maharaj Nakorn Chiang Mai hospital due to AMI during 2006-2013. The mean non-HDL-C and LDL-C during long-term follow-up were used to predict MACE at each time point. The patients were classified as target attainment if non-HDL-C\u2009<100\u00a0mg/dl and/or LDL-C\u2009<70\u00a0mg/dl. The MACE was defined as combination of all-cause death, nonfatal coronary event and nonfatal stroke.\nDuring mean follow-up of 2.6\u2009\u00b1\u20091.6\u00a0years among 868 patients after AMI, 34.4% achieved non-HDL-C target, 23.7% achieved LDL-C target and 21.2% experienced MACEs. LDL-C and non-HDL-C were directly compared in Cox regression model. Compared with non-HDL-C\u2009<100\u00a0mg/dl, patients with non-HDL-C of>130\u00a0mg/dl had higher incidence of MACEs (HR 3.15, 95% CI 1.46-6.80, P\u2009=\u20090.003). Surprisingly, LDL-C\u2009>100\u00a0mg/dl was associated with reduced risk of MACE as compared to LDL\u2009<70\u00a0mg/dl (HR 0.42, 95% CI 0.18-0.98, p\u2009=\u20090.046) after direct pairwise comparison with non-HDL-C level.\"\nQuestion:\n\"Is non-HDL-cholesterol a better predictor of long-term outcome in patients after acute myocardial infarction compared to LDL-cholesterol?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21658267": {
                "source": [
                    "\"There are three main service delivery channels: clinical services, outreach, and family and community. To determine which delivery channels are associated with the greatest reductions in under-5 mortality rates (U5MR), we used data from sequential population-based surveys to examine the correlation between changes in coverage of clinical, outreach, and family and community services and in U5MR for 27 high-burden countries.\nHousehold survey data were abstracted from serial surveys in 27 countries. Average annual changes (AAC) between the most recent and penultimate survey were calculated for under-five mortality rates and for 22 variables in the domains of clinical, outreach, and family- and community-based services. For all 27 countries and a subset of 19 African countries, we conducted principal component analysis to reduce the variables into a few components in each domain and applied linear regression to assess the correlation between changes in the principal components and changes in under-five mortality rates after controlling for multiple potential confounding factors.\nAAC in under 5-mortality varied from 6.6% in Nepal to -0.9% in Kenya, with six of the 19 African countries all experiencing less than a 1% decline in mortality. The strongest correlation with reductions in U5MR was observed for access to clinical services (all countries: p = 0.02, r\u00b2 = 0.58; 19 African countries p<0.001, r\u00b2 = 0.67). For outreach activities, AAC U5MR was significantly correlated with antenatal care and family planning services, while AAC in immunization services showed no association. In the family- and community services domain, improvements in breastfeeding were associated with significant changes in mortality in the 30 countries but not in the African subset; while in the African countries, nutritional status improvements were associated with a significant decline in mortality.\"\nQuestion:\n\"Do improvements in outreach, clinical, and family and community-based services predict improvements in child survival?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20608141": {
                "source": [
                    "\"Prostate-specific antigen (PSA) levels can show wide fluctuations when repeatedly measured. Here we investigatewd if: (a) biopsy timing influences the prostate cancer (PC) detection rate in patients with fluctuating PSA (flu-PSA) in comparison with patients with steadily increasing PSA (si-PSA); (b) PSA slope estimated in patients with flu-PSA predicts a different risk of cancer detection; (c) flu-PSA and si-PSA patients develop PC in topographically different sites; (d) the behaviour of pre-operative PSA is an expression of a disease with defferent characteristics to the following radical prostatectomy.\nThe study involved 211 patients who underwent at least a second biopsy after a first negative prostate biopsy. PSA Slope, PSA velocity (PSAV) and PSA doubling time (PSADT) were estimated. Flu-PSA level was defined as a PSA series with at least one PSA value lower than the one immediately preceding it.\n82 patients had flu-PSA levels and 129 si-PSA levels. There were no significant differences between the two groups in terms of cancer detection, clinical or pathological stage, but the si-PSA group with cancer had a higher Gleason score. No difference was found for PSA Slope between flu-PSA patients with cancer and those without.\"\nQuestion:\n\"PSA repeatedly fluctuating levels are reassuring enough to avoid biopsy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19351635": {
                "source": [
                    "\"National guidelines and government directives have adopted policies for urgent assessment of patients with a transient ischaemic attack or minor stroke not admitted to hospital. The risk of recurrent stroke increases substantially with age, as does the potential benefit of secondary prevention. In order to develop effective strategies for older patients, it is important to identify how stroke care is currently provided for this patient group.\nBetween 2004 and 2006, older patients (>75 years) referred to a neurovascular clinic were compared with younger patients (<or =75 years). Sociodemographic details, clinical features, resource use and secondary prevention in a neurovascular clinic were collected.\nOf 379 patients referred to the clinic, 129 (34%) were given a non-stroke diagnosis. Of the remaining 250 patients, 149 (60%) were<or =75 years. Median time from symptom onset to clinic appointment was similar for the two groups (24 (IQR 15-42) vs 24 (IQR 14-43) days; p = 0.58). Older patients were more likely to be in atrial fibrillation (10.1% vs 22.8%, p<0.001) and have lacunar stroke (34.7% vs 22.1%; p = 0.04). CT rates were similar in the two groups (27.8% vs 80.0%, p = 0.75). Scans were performed more quickly in younger patients (p<0.01). MRI scan rates were higher in younger patients (26% vs 4%, p<0.01), as was carotid Doppler imaging (92% vs 77%, p<0.01). There were no differences in prescribed secondary preventive treatments. Older patients experienced less delay for carotid endarterectomy (49 vs 90 days, p<0.01). Younger patients were more likely to be given advice on weight reduction (30.2% vs 12.9%, p<0.01) and diet (46.3% vs 31.7%, p = 0.02) than older patients.\"\nQuestion:\n\"Do older patients receive adequate stroke care?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "15050326": {
                "source": [
                    "\"To determine whether the risk of secondary breast cancer after radiotherapy (RT) for Hodgkin's disease is greater among women who underwent RT around time of pregnancy.\nThe records of 382 women treated with RT for Hodgkin's disease were reviewed and divided into those who received RT around the time of pregnancy and those who were not pregnant. Comparisons of the overall incidence, actuarial rates, and latency to breast cancer between the two groups were made. Multivariate Cox regression modeling was performed to determine possible contributing factors.\nOf the 382 women, 14 developed breast cancer (3.7%). The increase in the overall incidence (16.0% vs. 2.3%, p = 0.0001) and the actuarial rate of breast cancer among the women in the pregnant group (p = 0.011) was statistically significant. The women treated around the time of pregnancy had a 10- and 15-year actuarial rate of breast cancer of 6.7% and 32.6%, respectively. The 10-year and 15-year actuarial rate for the nonpregnant women was 0.4% and 1.7%, respectively. The median latency from RT to the diagnosis of breast cancer was 13.1 and 18.9 years for women in the pregnant and nonpregnant groups, respectively. In the multivariate analysis, pregnancy around the time of RT was the only variable associated with an increased risk of breast cancer. The risk was dependent on the length of time from pregnancy to RT, with women receiving RT during pregnancy and within 1 month of pregnancy having an increased risk of breast cancer compared with nonpregnant women and women irradiated later than 1 month after pregnancy (hazard ratio, 22.49; 95% confidence interval, 5.56-90.88; p<0.001).\"\nQuestion:\n\"Does radiotherapy around the time of pregnancy for Hodgkin's disease modify the risk of breast cancer?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17578985": {
                "source": [
                    "\"In this prospective, randomized, double-blind study, we compared the tibial and the peroneal evoked motor response with regard to efficacy of sciatic nerve block using the parasacral approach.\nTwenty-six ASA I-III patients scheduled for elective lower limb surgery were randomized to receive a parasacral sciatic block, using a nerve stimulator technique seeking either a tibial (n = 14) or peroneal (n = 12) motor response. After the evoked motor response was obtained, a solution of 10 mL 2% lidocaine with epinephrine and 10 mL 0.75% ropivacaine (actual final concentration of epinephrine, 1/160,000) was slowly injected through the needle. Sensory and motor blocks were assessed every 5 min for 30 min by an anesthesiologist blinded to the elicited motor response. If the block was not complete 30 min after injection of the local anesthetics, it was considered as failed, and general anesthesia was supplemented.\nTime to perform the block and level of minimal and maximal stimulation were not different between groups. The success rate of complete block was significantly higher in the tibial compared to the peroneal group (11 of 14 vs 2 of 12; P = 0.002).\"\nQuestion:\n\"Parasacral sciatic nerve block: does the elicited motor response predict the success rate?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21276532": {
                "source": [
                    "\"Complications associated with blood transfusions have resulted in widespread acceptance of low hematocrit levels in surgical patients. However, preoperative anemia seems to be a risk factor for adverse postoperative outcomes in certain surgical patients. This study investigated the National Surgical Quality Improvement Program (NSQIP) database to determine if preoperative anemia in patients undergoing open and laparoscopic colectomies is an independent predictor for an adverse composite outcome (CO) consisting of myocardial infarction, stroke, progressive renal insufficiency or death within 30 days of operation, or for an increased hospital length of stay (LOS).\nHematocrit levels were categorized into 4 classes: severe, moderate, mild, and no anemia. From 2005 to 2008, the NSQIP database recorded 23,348 elective open and laparoscopic colectomies that met inclusion criteria. Analyses using multivariable models, controlling for potential confounders and stratifying on propensity score, were performed.\nCompared with nonanemic patients, those with severe, moderate, and mild anemia were more likely to have the adverse CO with odds ratios of 1.83 (95% CI 1.05 to 3.19), 2.19 (95 % CI 1.63 to 2.94), and 1.49 (95% CI 1.20 to 1.86), respectively. Patients with a normal hematocrit had a reduced hospital LOS, compared with those with severe, moderate, and mild anemia (p<0.01). A history of cardiovascular disease did not significantly influence these findings.\"\nQuestion:\n\"Does preoperative anemia adversely affect colon and rectal surgery outcomes?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "8111516": {
                "source": [
                    "\"To determine whether volunteer family physician reports of the frequency of influenza-like illness (ILI) usefully supplement information from other influenza surveillance systems conducted by the Centers for Disease Control and Prevention.\nEvaluation of physician reports from five influenza surveillance seasons (1987-88 through 1991-92).\nFamily physician office practices in all regions of the United States.\nAn average of 140 physicians during each of five influenza seasons.\nNone.\nAn office visit or hospitalization of a patient for ILI, defined as presence of fever (temperature>or = 37.8 degrees C) and cough, sore throat, or myalgia, along with the physician's clinical judgment of influenza. A subset of physicians collected specimens for confirmation of influenza virus by culture.\nPhysicians attributed 81,408 (5%) of 1,672,542 office visits to ILI; 2754 (3%) patients with ILI were hospitalized. Persons 65 years of age and older accounted for 11% of visits for ILI and 43% of hospitalizations for ILI. In three of five seasons, physicians obtained influenza virus isolates from a greater proportion of specimens compared with those processed by World Health Organization laboratories (36% vs 12%). Influenza virus isolates from sentinel physicians peaked from 1 to 4 weeks earlier than those reported by World Health Organization laboratories. Physicians reported peak morbidity 1 to 4 weeks earlier than state and territorial health departments in four of five seasons and 2 to 5 weeks earlier than peak mortality reported by 121 cities during seasons with excess mortality associated with pneumonia and influenza.\"\nQuestion:\n\"Do family physicians make good sentinels for influenza?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26859535": {
                "source": [
                    "\"Updated guidelines for the screening and management of cervical cancer in the United States recommend starting Papanicolaou (Pap) testing at age 21 and screening less frequently with less aggressive management for abnormalities. We sought to examine updated Pap test screening guidelines and how they may affect the detection of invasive cervical cancer, especially among women<30 years of age.\nPatients diagnosed at Brigham and Women's Hospital with invasive cervical cancer between 2002 and 2012 were retrospectively identified. Prior screening history was obtained and patients were divided into two groups based on age<30 years or age \u226530 years. The two groups were then compared with respect to demographics, pathological findings, and time to diagnosis.\nA total of 288 patients with invasive cervical carcinoma were identified. Among these patients, 109 had adequate information on prior screening history. Invasive adenocarcinoma (IAC) was diagnosed in 37 (33.94%) patients, whereas 64 (58.72%) patients were diagnosed with invasive squamous cell carcinoma (ISCC). The remaining eight patients were diagnosed with other types of cancers of the cervix. A total of 13 patients were younger than 30 while 96 patients were 30 or older. The mean time from normal Pap to diagnosis of IAC was 15 months in patients younger than 30 years of age compared to 56 months in patients aged 30 and older (p\u2009<\u20090.001). The mean time from normal Pap to diagnosis of ISCC was 38 months in patients younger than 30 years of age and 82 months in patients aged 30 and older (p\u2009=\u20090.018).\"\nQuestion:\n\"Screening History Among Women with Invasive Cervical Cancer in an Academic Medical Center: Will We Miss Cancers Following Updated Guidelines?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17279467": {
                "source": [
                    "\"Cyclical pamidronate therapy in a 2-year-old child with skeletal fragility resulted in remodelling of vertebral fractures and improvement in bone mineral density (BMD) at distal radial and spinal sites. The BMD at both sites decreased precipitously within 24 months of stopping treatment, raising the question as to whether bisphosphonates can be stopped in a growing child with skeletal fragility.\nAt age 23 months, a male toddler sustained a low trauma fracture of his right femur. Skeletal radiographs revealed generalised osteopenia with multiple vertebral body fractures. He was diagnosed with type IV osteogenesis imperfecta; however, no mutations were found in COL1A1 or COL1A2 genes.\nThis case report presents bone densitometry data before, during and after bisphosphonate treatment. Axial QCT was main outcome from 2 years of age; DXA and pQCT were taken after age 5.\nQCT confirmed that he had low spinal trabecular volumetric BMD (Z-score -2.4). After 4 years of treatment his vertebral fractures had been remodelled and all bone densitometry values (QCT, DXA and pQCT) were within normal range and therefore treatment was discontinued. Shortly after this he suffered stress fractures of his left mid tibia and at the sclerotic metaphyseal line corresponding to his first APD treatment. He had marked reduction in spinal trabecular and distal radial vBMD; change in BMAD was less marked.\"\nQuestion:\n\"Can bisphosphonate treatment be stopped in a growing child with skeletal fragility?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26113007": {
                "source": [
                    "\"Orthodontic patients show high prevalence of tooth-size discrepancy. This study investigates the possible association between arch form, clinically significant tooth-size discrepancy, and sagittal molar relationship.\nPretreatment orthodontic casts of 230 Saudi patients were classified into one of three arch form types (tapered, ovoid, and square) using digitally scanned images of the mandibular arches. Bolton ratio was calculated, sagittal molar relationship was defined according to Angle classification, and correlations were analyzed using ANOVA, chi-square, and t-tests.\nNo single arch form was significantly more common than the others. Furthermore, no association was observed between the presence of significant Bolton discrepancy and the sagittal molar relationship or arch form. Overall Bolton discrepancy is significantly more prevalent in males.\"\nQuestion:\n\"Is arch form influenced by sagittal molar relationship or Bolton tooth-size discrepancy?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24507422": {
                "source": [
                    "\"Patients presenting with transient ischemic attack or stroke may have symptom-related lesions on acute computed tomography angiography (CTA) such as free-floating intraluminal thrombus (FFT). It is difficult to distinguish FFT from carotid plaque, but the distinction is critical as management differs. By contouring the shape of these vascular lesions (\"virtual endarterectomy\"), advanced morphometric analysis can be performed. The objective of our study is to determine whether quantitative shape analysis can accurately differentiate FFT from atherosclerotic plaque.\nWe collected 23 consecutive cases of suspected carotid FFT seen on CTA (13 men, 65 \u00b1 10 years; 10 women, 65.5 \u00b1 8.8 years). True-positive FFT cases (FFT+) were defined as filling defects resolving with anticoagulant therapy versus false-positives (FFT-), which remained unchanged. Lesion volumes were extracted from CTA images and quantitative shape descriptors were computed. The five most discriminative features were used to construct receiver operator characteristic (ROC) curves and to generate three machine-learning classifiers. Average classification accuracy was determined by cross-validation.\nFollow-up imaging confirmed sixteen FFT+ and seven FFT- cases. Five shape descriptors delineated FFT+ from FFT- cases. The logistic regression model produced from combining all five shape features demonstrated a sensitivity of 87.5% and a specificity of 71.4% with an area under the ROC curve = 0.85 \u00b1 0.09. Average accuracy for each classifier ranged from 65.2%-76.4%.\"\nQuestion:\n\"Can shape analysis differentiate free-floating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke or transient ischemic attack?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19482903": {
                "source": [
                    "\"Earlier studies have demonstrated low peak oxygen uptake ((.)Vo(2)peak) in children with spina bifida. Low peak heart rate and low peak respiratory exchange ratio in these studies raised questions regarding the true maximal character of (.)Vo(2)peak values obtained with treadmill testing.\nThe aim of this study was to determine whether the Vo(2)peak measured during an incremental treadmill test is a true reflection of the maximum oxygen uptake ((.)Vo(2)max) in children who have spina bifida and are ambulatory.\nA cross-sectional design was used for this study.\nTwenty children who had spina bifida and were ambulatory participated. The (.)Vo(2)peak was measured during a graded treadmill exercise test. The validity of (.)Vo(2)peak measurements was evaluated by use of previously described guidelines for maximum exercise testing in children who are healthy, as well as differences between Vo(2)peak and (.)Vo(2) during a supramaximal protocol ((.)Vo(2)supramaximal).\nThe average values for (.)Vo(2)peak and normalized (.)Vo(2)peak were, respectively, 1.23 L/min (SD=0.6) and 34.1 mL/kg/min (SD=8.3). Fifteen children met at least 2 of the 3 previously described criteria; one child failed to meet any criteria. Although there were no significant differences between (.)Vo(2)peak and Vo(2)supramaximal, 5 children did show improvement during supramaximal testing.\nThese results apply to children who have spina bifida and are at least community ambulatory.\"\nQuestion:\n\"Treadmill testing of children who have spina bifida and are ambulatory: does peak oxygen uptake reflect maximum oxygen uptake?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21904069": {
                "source": [
                    "\"Knowing the collaterals is essential for a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nTo ascertain the sources of the blood supply to the spleen after a spleen-preserving distal pancreatectomy with resection of the splenic vessels.\nPerfusion of the cadaveric left gastric and right gastroepiploic arteries with methylene blue after occlusion of all the arteries except the short gastric arteries (n=10). Intraoperative color Doppler ultrasound was used for the evaluation of the hilar arterial blood flow at distal pancreatectomy (n=23) after 1) clamping of the splenic artery alone, 2) clamping of the splenic and left gastroepiploic arteries and 3) clamping of the splenic and short gastric arteries. CT angiography of the gastric and splenic vessels before and after a spleen-preserving distal pancreatectomy (n=10).\nPerfusion of the cadaveric arteries revealed no effective direct or indirect (through the submucous gastric arterial network) communication between the left gastric and the branches of the short gastric arteries. In no case did intraoperative color Doppler ultrasound detect any hilar arterial blood flow after the clamping of the splenic and left gastroepiploic arteries. The clamping of the short gastric arteries did not change the flow parameters. In none of the cases did a post-spleen-preserving distal pancreatectomy with resection of the splenic vessels CT angiography delineate the short gastric vessels supplying the spleen. In all cases, the gastroepiploic arcade was the main arterial pathway feeding the spleen.\"\nQuestion:\n\"Spleen-preserving distal pancreatectomy with resection of the splenic vessels. Should one rely on the short gastric arteries?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11334578": {
                "source": [
                    "\"To evaluate the effectiveness of feeding information on pharmacy back to primary care doctors in order to create awareness (knowledge) of pharmaceutical expenditure (PE).\nRetrospective cross-sectional study, through personal interview.\nReformed PC, Sabadell, Barcelona.\nThe 80 PC doctors working with primary care teams.\nAs the personal feed-back on PE, each doctor was asked for the PE generated during 1997 and the mean cost of prescriptions to active and pensioner patients. The statistical test used was the t test to compare means for paired data, with p<0.05 the required level of significance.\nOut of the total doctors interviewed (80), 71 replies were obtained for the annual PE and 76 for the mean cost of prescriptions, for both active and pensioner patients. Significant differences were found between the annual PE in reality and doctors' estimates: around twelve million pesetas. The differences between the real mean costs of prescription and the estimates were also significant.\"\nQuestion:\n\"Is there awareness of pharmaceutical expenditure in the reformed primary care system?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10593212": {
                "source": [
                    "\"To investigate the importance of loss of consciousness (LOC) in predicting neuropsychological test performance in a large sample of patients with head injury.\nRetrospective comparison of neuropsychological test results for patients who suffered traumatic LOC, no LOC, or uncertain LOC.\nAllegheny General Hospital, Pittsburgh, Pennsylvania.\nThe total number of patients included in this study was 383.\nNeuropsychological test measures, including the visual reproduction, digit span, and logical memory subtests of the Wechsler memory scale (revised), the Trail Making test, Wisconsin Card Sorting test, Hopkins Verbal Learning test, Controlled Oral Word Association, and the Galveston Orientation and Amnesia test (GOAT).\nNo significant differences were found between the LOC, no LOC, or uncertain LOC groups for any of the neuropsychological measures used. Patients who had experienced traumatic LOC did not perform more poorly on neuropsychological testing than those with no LOC or uncertain LOC. All three groups demonstrated mildly decreased performance on formal tests of speed of information processing, attentional process, and memory.\"\nQuestion:\n\"Does loss of consciousness predict neuropsychological decrements after concussion?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "27338535": {
                "source": [
                    "\"Current risk assessment models for surgical site occurrence (SSO) and surgical site infection (SSI) after open ventral hernia repair (VHR) have limited external validation. Our aim was to determine (1) whether existing models stratify patients into groups by risk and (2) which model best predicts the rate of SSO and SSI.\nPatients who underwent open VHR and were followed for at least 1\u00a0mo were included. Using two data sets-a retrospective multicenter database (Ventral Hernia Outcomes Collaborative) and a single-center prospective database (Prospective)-each patient was assigned a predicted risk with each of the following models: Ventral Hernia Risk Score (VHRS), Ventral Hernia Working Group (VHWG), Centers for Disease Control and Prevention Wound Class, and Hernia Wound Risk Assessment Tool (HW-RAT). Patients in the Prospective database were also assigned a predicted risk from the American College of Surgeons National Surgical Quality Improvement Program (ACS-NSQIP). Areas under the receiver operating characteristic curve (area under the curve [AUC]) were compared to assess the predictive accuracy of the models for SSO and SSI. Pearson's chi-square was used to determine which models were able to risk-stratify patients into groups with significantly differing rates of actual SSO and SSI.\nThe Ventral Hernia Outcomes Collaborative database (n\u00a0=\u00a0795) had an overall SSO and SSI rate of 23% and 17%, respectively. The AUCs were low for SSO (0.56, 0.54, 0.52, and 0.60) and SSI (0.55, 0.53, 0.50, and 0.58). The VHRS (P\u00a0=\u00a00.01) and HW-RAT (P\u00a0<\u00a00.01) significantly stratified patients into tiers for SSO, whereas the VHWG (P\u00a0<\u00a00.05) and HW-RAT (P\u00a0<\u00a00.05) stratified for SSI. In the Prospective database (n\u00a0=\u00a088), 14% and 8% developed an SSO and SSI, respectively. The AUCs were low for SSO (0.63, 0.54, 0.50, 0.57, and 0.69) and modest for SSI (0.81, 0.64, 0.55, 0.62, and 0.73). The ACS-NSQIP (P\u00a0<\u00a00.01) stratified for SSO, whereas the VHRS (P\u00a0<\u00a00.01) and ACS-NSQIP (P\u00a0<\u00a00.05) stratified for SSI. In both databases, VHRS, VHWG, and Centers for Disease Control and Prevention overestimated risk of SSO and SSI, whereas HW-RAT and ACS-NSQIP underestimated risk for all groups.\"\nQuestion:\n\"Do risk calculators accurately predict surgical site\u00a0occurrences?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "17691856": {
                "source": [
                    "\"Rising health care costs and the need to consolidate expertise in tertiary services have led to the centralisation of services. In the UK, the result has been that many rural maternity units have become midwife-led. A key consideration is that midwives have the skills to competently and confidently provide maternity services in rural areas, which may be geographically isolated and where the midwife may only see a small number of pregnant women each year. Our objective was to compare the views of midwives in rural and urban settings, regarding their competence and confidence with respect to 'competencies' identified as being those which all professionals should have in order to provide effective and safe care for low-risk women.\nThis was a comparative questionnaire survey involving a stratified sample of remote and rural maternity units and an ad hoc comparison group of three urban maternity units in Scotland. Questionnaires were sent to 82 midwives working in remote and rural areas and 107 midwives working in urban hospitals with midwife-led units.\nThe response rate from midwives in rural settings was considerably higher (85%) than from midwives in the urban areas (60%). Although the proportion of midwives who reported that they were competent was broadly similar in the two groups, there were some significant differences regarding specific competencies. Midwives in the rural group were more likely to report competence for breech delivery (p = 0.001), while more urban midwives reported competence in skills such as intravenous fluid replacement (p<0.001) and initial and discharge examination of the newborn (p<0.001). Both groups reported facing barriers to continuing professional development; however, more of the rural group had attended an educational event within the last month (p<0.001). Lack of time was a greater barrier for urban midwives (p = 0.02), whereas distance to training was greater for rural midwives (p = 0.009). Lack of motivation or interest was significantly higher in urban units (p = 0.006).\"\nQuestion:\n\"Midwives' competence: is it affected by working in a rural location?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "10401824": {
                "source": [
                    "\"Laparoscopic techniques can be used to treat patients whose antireflux surgery has failed.\nCase series.\nTwo academic medical centers.\nForty-six consecutive patients, of whom 21 were male and 25 were female (mean age, 55.6 years; range, 15-80 years). Previous antireflux procedures were laparoscopic (21 patients), laparotomy (21 patients), thoracotomy (3 patients), and thoracoscopy (1 patient).\nThe cause of failure, operative and postoperative morbidity, and the level of follow-up satisfaction were determined for all patients.\nThe causes of failure were hiatal herniation (31 patients [67%]), fundoplication breakdown (20 patients [43%]), fundoplication slippage (9 patients [20%]), tight fundoplication (5 patients [11%]), misdiagnosed achalasia (2 patients [4%]), and displaced Angelchik prosthesis (2 patients [4%]). Twenty-two patients (48%) had more than 1 cause. Laparoscopic reoperative procedures were Nissen fundoplication (n = 22), Toupet fundoplication (n = 13), paraesophageal hernia repair (n = 4), Dor procedure (n = 2), Angelchik prosthesis removal (n = 2), Heller myotomy (n = 2), and the takedown of a wrap (n = 1). In addition, 18 patients required crural repair and 13 required paraesophageal hernia repair. The mean +/- SEM duration of surgery was 3.5+/-1.1 hours. Operative complications were fundus tear (n = 8), significant bleeding (n = 4), bougie perforation (n = 1), small bowel enterotomy (n = 1), and tension pneumothorax (n = 1). The conversion rate (from laparoscopic to an open procedure) was 20% overall (9 patients) but 0% in the last 10 patients. Mortality was 0%. The mean +/- SEM hospital stay was 2.3+/-0.9 days for operations completed laparoscopically. Follow-up was possible in 35 patients (76%) at 17.2+/-11.8 months. The well-being score (1 best; 10, worst) was 8.6+/-2.1 before and 2.9+/-2.4 after surgery (P<.001). Thirty-one (89%) of 35 patients were satisfied with their decision to have reoperation.\"\nQuestion:\n\"Is laparoscopic reoperation for failed antireflux surgery feasible?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22233470": {
                "source": [
                    "\"The 58th World Health Assembly called for all health systems to move towards universal coverage where everyone has access to key promotive, preventive, curative and rehabilitative health interventions at an affordable cost. Universal coverage involves ensuring that health care benefits are distributed on the basis of need for care and not on ability to pay. The distribution of health care benefits is therefore an important policy question, which health systems should address. The aim of this study is to assess the distribution of health care benefits in the Kenyan health system, compare changes over two time periods and demonstrate the extent to which the distribution meets the principles of universal coverage.\nTwo nationally representative cross-sectional households surveys conducted in 2003 and 2007 were the main sources of data. A comprehensive analysis of the entire health system is conducted including the public sector, private-not-for-profit and private-for-profit sectors. Standard benefit incidence analysis techniques were applied and adopted to allow application to private sector services.\nThe three sectors recorded similar levels of pro-rich distribution in 2003, but in 2007, the private-not-for-profit sector was pro-poor, public sector benefits showed an equal distribution, while the private-for-profit sector remained pro-rich. Larger pro-rich disparities were recorded for inpatient compared to outpatient benefits at the hospital level, but primary health care services were pro-poor. Benefits were distributed on the basis of ability to pay and not on need for care.\"\nQuestion:\n\"Does the distribution of health care benefits in Kenya meet the principles of universal coverage?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "9542484": {
                "source": [
                    "\"To determine whether successful completion of the Perinatal Education Programme (PEP) improves obstetric practice.\nThe three midwife obstetric units (MOUs) in a health district of Mpumalanga were included in the study. Two MOUs enrolled in the PEP and the third did not. A 'before-and-after' study design was used to assess any changes in practice, and to monitor whether any changes occurred in the district during the time of the study; data were also collected at the third MOU. Data were collected by scoring of the obstetric files after the patient had delivered.\nWe ascertained whether the obstetric history, syphilis testing, blood group testing, haemoglobin measurement and uterine growth assessment were performed during antenatal care along with whether appropriate action was taken. For intrapartum care, estimation of fetal weight, the performance of pelvimetry, blood pressure monitoring, urine testing, evaluation of head above pelvis, fetal heart rate monitoring, monitoring of contractions and plotting of cervical dilatation, and whether the appropriate actions were taken, were assessed.\nEight of the 13 midwives at the two MOUs completed the PEP and all demonstrated an improvement in knowledge. Case notes of 303 patients from the various clinics were studied. There was no change in the referral patterns of any of the clinics during the study period. The obstetric history was well documented, but in no group was there a satisfactory response to a detected problem; appropriate action was taken in between 0% and 12% of cases. Syphilis testing was performed in 56-82% of cases, with no difference between the groups. The haemoglobin level was measured in only 4-15% of patients, with no difference before or after completion of the PEP. Where a problem in uterine growth was detected, an appropriate response occurred in 0-8% of patients and no difference before or after completion of the PEP was ascertained. In all groups, estimation of fetal weight and pelvimetry were seldom performed, the urine and fetal heart rate documentation were moderately well done and the blood pressure monitoring, assessment of head above pelvis, monitoring of contractions and plotting of cervical dilatation were usually performed. No differences before or after the PEP were detected. Where problems were detected, appropriate actions taken during labour improved, but not significantly.\"\nQuestion:\n\"Does successful completion of the Perinatal Education Programme result in improved obstetric practice?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "20971618": {
                "source": [
                    "\"Cutaneous infections such as impetigo contagiosum (IC), molluscum contagiosum (MC) and herpes virus infection (HI) appear to be associated with atopic dermatitis (AD), but there are no reports of concrete epidemiological evidence.\nWe evaluated the association of childhood AD with these infections by conducting a population-based cross-sectional study.\nEnrolled in this study were 1117 children aged 0-6 years old attending nursery schools in Ishigaki City, Okinawa Prefecture, Japan. Physical examination was performed by dermatologists, and a questionnaire was completed on each child's history of allergic diseases including AD, asthma, allergic rhinitis and egg allergy, and that of skin infections including IC, MC and HI, as well as familial history of AD.\nIn 913 children (AD; 132), a history of IC, MC or HI was observed in 45.1%, 19.7%, and 2.5%, respectively. Multiple logistic regression analysis revealed that the odds of having a history of IC were 1.8 times higher in AD children than in non-AD children. Meanwhile, a history of MC was significantly correlated to the male gender, but not to a personal history of AD. As for HI, we found no correlated factors in this study.\"\nQuestion:\n\"Are lifetime prevalence of impetigo, molluscum and herpes infection really increased in children having atopic dermatitis?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19712912": {
                "source": [
                    "\"To evaluate the effect of the 80-hour workweek restrictions on resident education within surgical programs in the New England area.\nWeb-based survey.\nAll Accreditation Council for Graduate Medical Education (ACGME) accredited surgical residency programs in New England (n = 20).\nProgram directors/coordinators in each surgical residency program in New England.\nFirst, American Board of Surgery In-Training Examination (ABSITE) scores and the passing rate of the ABS certifying examination were recorded for the years 2001, 2002, 2005, and 2006. Second, the changes in the curriculum of surgical education were documented as perceived by program coordinators and directors.\nIn all, 85% (17/20) of surgical programs in New England responded to the survey. The programs began to implement the 80-hour workweek from 2002 to 2004. An equal distribution of community (n = 8) and university programs (n = 9) was sampled. Prior to the initiation of the 80-hour workweek, residency programs emphasized weekly didactic sessions given by attending physicians (88%), mock orals (88%), and conventional journal club (76%). After the 80-hour workweek was implemented, the education curriculum most often consisted of didactic sessions by attending (100%), mock orals (88%), and simulation laboratories (75%). No difference was observed in ABSITE scores and first-time pass rates of the ABS examination before or after the introduction of the 80-hour workweek (20% response). Only 25% of programs felt that surgical education was improved after the implementation of the 80-hour workweek, whereas 31% felt education was worse. Overall, 44% of respondents believed that there was no difference in surgical education.\"\nQuestion:\n\"Has the 80-hour workweek improved surgical resident education in New England?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "19156007": {
                "source": [
                    "\"To investigate whether the Patient Health Questionnaire-9 (PHQ-9) possesses the essential psychometric characteristics to measure depressive symptoms in people with visual impairment.\nThe PHQ-9 scale was completed by 103 participants with low vision. These data were then assessed for fit to the Rasch model.\nThe participants' mean +/- standard deviation (SD) age was 74.7 +/- 12.2 years. Almost one half of them (n = 46; 44.7%) were considered to have severe vision impairment (presenting visual acuity<6/60 in the better eye). Disordered thresholds were evident initially. Collapsing the two middle categories produced ordered thresholds and fit to the Rasch model (chi = 10.1; degrees of freedom = 9; p = 0.34). The mean (SD) items and persons Fit Residual values were -0.31 (1.12) and -0.25 (0.78), respectively, where optimal fit of data to the Rasch model would have a mean = 0 and SD = 1. Unidimensionality was demonstrated confirming the construct validity of the PHQ-9 and there was no evidence of differential item functioning on a number of factors including visual disability. The person separation reliability value was 0.80 indicating that the PHQ-9 has satisfactory precision. There was a degree of mistargeting as expected in this largely non-clinically depressed sample.\"\nQuestion:\n\"Can clinicians use the PHQ-9 to assess depression in people with vision loss?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "24614789": {
                "source": [
                    "\"Postoperative CSF fistulas were described in 16 of 198 patients (8%) who underwent spine surgery between 2009 and 2010. The choice of the therapeutic strategy was based on the clinical condition of the patients, taking into account the possibility to maintain the prone position continuously and the risk of morbidity due to prolonged bed rest. Six patients were treated conservatively (position prone for three weeks), ten patients were treated by positioning an external CSF lumbar drainage for ten days. The mean follow-up period was ten months.\nAll patients healed their wound properly and no adverse events were recorded. Patients treated conservatively were cured in a mean period of 30 days, while patients treated with CSF drainage were cured in a mean period of 10 days.\"\nQuestion:\n\"Is lumbar drainage of postoperative cerebrospinal fluid fistula after spine surgery effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22266735": {
                "source": [
                    "\"The International Association of the Diabetes and Pregnancy Study Groups (IADPSG) recently recommended new criteria for diagnosing gestational diabetes mellitus (GDM). This study was undertaken to determine whether adopting the IADPSG criteria would be cost-effective, compared with the current standard of care.\nWe developed a decision analysis model comparing the cost-utility of three strategies to identify GDM: 1) no screening, 2) current screening practice (1-h 50-g glucose challenge test between 24 and 28 weeks followed by 3-h 100-g glucose tolerance test when indicated), or 3) screening practice proposed by the IADPSG. Assumptions included that 1) women diagnosed with GDM received additional prenatal monitoring, mitigating the risks of preeclampsia, shoulder dystocia, and birth injury; and 2) GDM women had opportunity for intensive postdelivery counseling and behavior modification to reduce future diabetes risks. The primary outcome measure was the incremental cost-effectiveness ratio (ICER).\nOur model demonstrates that the IADPSG recommendations are cost-effective only when postdelivery care reduces diabetes incidence. For every 100,000 women screened, 6,178 quality-adjusted life-years (QALYs) are gained, at a cost of $125,633,826. The ICER for the IADPSG strategy compared with the current standard was $20,336 per QALY gained. When postdelivery care was not accomplished, the IADPSG strategy was no longer cost-effective. These results were robust in sensitivity analyses.\"\nQuestion:\n\"Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "22302658": {
                "source": [
                    "\"Patients with aggressive lower extremity musculoskeletal tumors may be candidates for either above-knee amputation or limb-salvage surgery. However, the subjective and objective benefits of limb-salvage surgery compared with amputation are not fully clear.QUESTIONS/\nWe therefore compared functional status and quality of life for patients treated with above-knee amputation versus limb-salvage surgery.\nWe reviewed 20 of 51 patients aged 15 years and older treated with above-knee amputation or limb-salvage surgery for aggressive musculoskeletal tumors around the knee between 1994 and 2004 as a retrospective cohort study. At last followup we obtained the Physiological Cost Index, the Reintegration to Normal Living Index, SF-36, and the Toronto Extremity Salvage Score questionnaires. The minimum followup was 12 months (median, 56 months; range, 12-108 months).\nCompared with patients having above-knee amputation, patients undergoing limb-salvage surgery had superior Physiological Cost Index scores and Reintegration to Normal Living Index. The Toronto Extremity Salvage scores and SF-36 scores were similar in the two groups.\"\nQuestion:\n\"Does limb-salvage surgery offer patients better quality of life and functional capacity than amputation?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "14599616": {
                "source": [
                    "\"Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume.\nNinety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard.\nThirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.\"\nQuestion:\n\"Can a practicing surgeon detect early lymphedema reliably?\""
                ],
                "target": "maybe",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "11567820": {
                "source": [
                    "\"To test the hypothesis that increasing the nerve length within the treatment volume for trigeminal neuralgia radiosurgery would improve pain relief.\nEighty-seven patients with typical trigeminal neuralgia were randomized to undergo retrogasserian gamma knife radiosurgery (75 Gy maximal dose with 4-mm diameter collimators) using either one (n = 44) or two (n = 43) isocenters. The median follow-up was 26 months (range 1-36).\nPain relief was complete in 57 patients (45 without medication and 12 with low-dose medication), partial in 15, and minimal in another 15 patients. The actuarial rate of obtaining complete pain relief (with or without medication) was 67.7% +/- 5.1%. The pain relief was identical for one- and two-isocenter radiosurgery. Pain relapsed in 30 of 72 responding patients. Facial numbness and mild and severe paresthesias developed in 8, 5, and 1 two-isocenter patients vs. 3, 4, and 0 one-isocenter patients, respectively (p = 0.23). Improved pain relief correlated with younger age (p = 0.025) and fewer prior procedures (p = 0.039) and complications (numbness or paresthesias) correlated with the nerve length irradiated (p = 0.018).\"\nQuestion:\n\"Does increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26222664": {
                "source": [
                    "\"A retrospective analysis.\nThe purpose of this study was to determine whether the deformity angular ratio (DAR) can reliably assess the neurological risks of patients undergoing deformity correction.\nIdentifying high-risk patients and procedures can help ensure that appropriate measures are taken to minimize neurological complications during spinal deformity corrections. Subjectively, surgeons look at radiographs and evaluate the riskiness of the procedure. However, 2 curves of similar magnitude and location can have significantly different risks of neurological deficit during surgery. Whether the curve spans many levels or just a few can significantly influence surgical strategies. Lenke et al have proposed the DAR, which is a measure of curve magnitude per level of deformity.\nThe data from 35 pediatric spinal deformity correction procedures with thoracic 3-column osteotomies were reviewed. Measurements from preoperative radiographs were used to calculate the DAR. Binary logistic regression was used to model the relationship between DARs (independent variables) and presence or absence of an intraoperative alert (dependent variable).\nIn patients undergoing 3-column osteotomies, sagittal curve magnitude and total curve magnitude were associated with increased incidence of transcranial motor evoked potential changes. Total DAR greater than 45\u00b0 per level and sagittal DAR greater than 22\u00b0 per level were associated with a 75% incidence of a motor evoked potential alert, with the incidence increasing to 90% with sagittal DAR of 28\u00b0 per level.\"\nQuestion:\n\"The Deformity Angular Ratio: Does It Correlate With High-Risk Cases for Potential Spinal Cord Monitoring Alerts in Pediatric 3-Column Thoracic Spinal Deformity Corrective Surgery?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26578404": {
                "source": [
                    "\"Breathlessness is one of the most distressing symptoms experienced by patients with advanced cancer and noncancer diagnoses alike. Often, severity of breathlessness increases quickly, calling for rapid symptom control. Oral, buccal, and parenteral routes of provider-controlled drug administration have been described. It is unclear whether patient-controlled therapy (PCT) systems would be an additional treatment option.\nTo investigate whether intravenous opioid PCT can be an effective therapeutic method to reduce breathlessness in patients with advanced disease. Secondary aims were to study the feasibility and acceptance of opioid PCT in patients with refractory breathlessness.\nThis was a pilot observational study with 18 inpatients with advanced disease and refractory breathlessness receiving opioid PCT. Breathlessness was measured on a self-reported numeric rating scale. Richmond Agitation Sedation Scale scores, Palliative Performance Scale scores, vital signs, and a self-developed patient satisfaction questionnaire were used for measuring secondary outcomes. Descriptive and interference analyses (Friedman test) and post hoc analyses (Wilcoxon tests and Bonferroni corrections) were performed.\nEighteen of 815 patients (advanced cancer; median age\u00a0=\u00a057.5\u00a0years [range 36-81]; 77.8% female) received breathlessness symptom control with opioid PCT; daily morphine equivalent dose at Day 1 was median\u00a0=\u00a020.3\u00a0mg (5.0-49.6\u00a0mg); Day 2: 13.0\u00a0mg (1.0-78.5\u00a0mg); Day 3: 16.0\u00a0mg (8.3-47.0\u00a0mg). Numeric rating scale of current breathlessness decreased (baseline: median\u00a0=\u00a05 [range 1-10]; Day 1: median\u00a0=\u00a04 [range 0-8], P\u00a0<\u00a00.01; Day 2: median\u00a0=\u00a04 [range 0-5], P\u00a0<\u00a00.01). Physiological parameters were stable over time. On Day 3, 12/12 patients confirmed that this mode of application provided relief of breathlessness.\"\nQuestion:\n\"Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "23588461": {
                "source": [
                    "\"Ascitis and undernutrition are frequent complications of cirrhosis, however ascitis volume and anthropometric assessment are not routinely documented or considered in prognostic evaluation. In a homogeneous cohort followed during two years these variables were scrutinized, aiming to ascertain relevance for longterm outcome.\nPopulation (N = 25, all males with alcoholic cirrhosis) was recruited among patients hospitalized for uncomplicated ascitis. Exclusion criteria were refractory or tense ascitis, cancer, spontaneous bacterial peritonitis, bleeding varices and critical illness. Measurements included ultrasonographically estimated ascitis volume, dry body mass index/BMI , upper arm anthropometrics, hematologic counts and liver function tests.\nPopulation (age 48.3 \u00b1 11.3 years, BMI 21.1 \u00b1 3.5 kg/m\u00b2, serum albumin 2.5 \u00b1 0.8 g/dL) was mostly in the Child-Pugh C category (77.8%) but clinically stable. During the follow-up period of 22.6 \u00b1 3.8 months, additional hospitalizations numbered 1.7 \u00b1 1.0 and more than one quarter succumbed. Admission ascitis volume corresponded to 7.1 \u00b1 3.6 L and dry BMI to 18.3 \u00b1 3.5 kg/m\u00b2. Child Pugh index was relevant for both mortality and rehospitalization. Nevertheless, similar matches for mortality were documented with ascitis volume and dry BMI, and arm circumference below the 5th percentile was highly significantly associated with rehospitalization.\"\nQuestion:\n\"Should ascitis volume and anthropometric measurements be estimated in hospitalized alcoholic cirrotics?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "21889895": {
                "source": [
                    "\"The aim of this study was to determine if educating residents about the potential effects of radiation exposure from computed tomographic (CT) imaging alters ordering patterns. This study also explored whether referring physicians are interested in radiation education and was an initial effort to address their CT ordering behavior.\nTwo to four months after a radiologist's lecture on the potential effects of radiation exposure related to CT scans, urology and orthopedic residents were surveyed regarding the number and types of CT scans they ordered, the use of alternative imaging modalities, and whether they used the lecture information to educate patients.\nTwenty-one resident lecture attendants completed the survey. The number of CT scans ordered after the lecture stayed constant for 90% (19 of 21) and decreased for 10% (two of 21). The types of CT scans ordered changed after the lecture for 14% (three of 21). Thirty-three percent (seven of 21) reported increases in alternative imaging after the lecture, including 24% (five of 21) reporting increases in magnetic resonance imaging and 19% (four of 21) reporting increases in ultrasound. Patients directed questions about radiation exposure to 57% (12 of 21); 38% (eight of 21) used the lecture information to educate patients. Referring physicians were interested in the topic, and afterward, other physician groups requested radiation education lectures.\"\nQuestion:\n\"Will CT ordering practices change if we educate residents about the potential effects of radiation exposure?\""
                ],
                "target": "no",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "12377809": {
                "source": [
                    "\"Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter and/or the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography.\nTwenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborectalis were measured at rest, and during voluntary squeezing and straining. Statistical analysis was performed within and between the two groups.\nThe anal sphincter became paradoxically shorter and/or thicker during straining (versus the resting state) in 85% of patients but in only 35% of control subjects. Changes in sphincter length were statistically significantly different (p<0.01, chi(2) test) in patients compared with control subjects. The m. puborectalis became paradoxically shorter and/or thicker during straining in 80% of patients but in only 30% of controls. Both the changes in length and thickness of the m. puborectalis were significantly different (p<0.01, chi(2) test) in patients versus control subjects.\"\nQuestion:\n\"Is anorectal endosonography valuable in dyschesia?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            },
            "26419377": {
                "source": [
                    "\"The purpose of this study was to evaluate safe depth for suture anchor insertion during acetabular labral repair and to determine the neighbouring structures at risk during drilling and anchor insertion.\nTen human cadaveric hips (six males and four females) were obtained. Acetabular labral surface was prepared and marked for right hips as 12, 1 and 3 o'clock positions, for left hips 12, 11 and 9 o'clock positions. Those were defined as anterior, anterior-superior and superior zones, respectively. These labral positions were drilled at defined zones. After measurements, depth of the bone at 10\u00b0 and 20\u00b0 drill angles on zones was compared statistically.\nAcetabular bone widths at investigated labral insertion points did not statistically differ. A total of 14 injuries in 60 penetrations occurred (23.3\u00a0%) with free drill penetrations, and no injuries occurred with stopped drill penetrations. The bone depth was gradually decreasing from 10\u00b0 to 20\u00b0 drill angles and from anterior to superior inserting zones without significant importance. The risk of perforation to the pelvic cavity started with 20\u00a0mm drill depth, and the mean depth for all insertions was calculated as 31.7\u00a0mm (SD 2.6).\"\nQuestion:\n\"Are pelvic anatomical structures in danger during arthroscopic acetabular labral repair?\""
                ],
                "target": "yes",
                "list_label": [
                    "yes",
                    "no",
                    "maybe"
                ],
                "config": "none",
                "task": "pubmed_qa",
                "prompt": "pubmed_qa"
            }
        }
    }
}